<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-13T00:00:00Z">2023-07-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the Deceptions: An Analysis on Audio Spoofing Detection and
  Future Prospects <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Ranjan, Mayank Vatsa, Richa Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio has become an increasingly crucial biometric modality due to its
ability to provide an intuitive way for humans to interact with machines. It is
currently being used for a range of applications, including person
authentication to banking to virtual assistants. Research has shown that these
systems are also susceptible to spoofing and attacks. Therefore, protecting
audio processing systems against fraudulent activities, such as identity theft,
financial fraud, and spreading misinformation, is of paramount importance. This
paper reviews the current state-of-the-art techniques for detecting audio
spoofing and discusses the current challenges along with open research
problems. The paper further highlights the importance of considering the
ethical and privacy implications of audio spoofing detection systems. Lastly,
the work aims to accentuate the need for building more robust and generalizable
methods, the integration of automatic speaker verification and countermeasure
systems, and better evaluation protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Metric of Informational Masking for Perceptual Audio Quality
  Measurement <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo M. Delgado, Jürgen Herre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual audio quality measurement systems algorithmically analyze the
output of audio processing systems to estimate possible perceived quality
degradation using perceptual models of human audition. In this manner, they
save the time and resources associated with the design and execution of
listening tests (LTs). Models of disturbance audibility predicting peripheral
auditory masking in quality measurement systems have considerably increased
subjective quality prediction performance of signals processed by perceptual
audio codecs. Additionally, cognitive effects have also been known to regulate
perceived distortion severity by influencing their salience. However, the
performance gains due to cognitive effect models in quality measurement systems
were inconsistent so far, particularly for music signals. Firstly, this paper
presents an improved model of informational masking (IM) -- an important
cognitive effect in quality perception -- that considers disturbance
information complexity around the masking threshold. Secondly, we incorporate
the proposed IM metric into a quality measurement systems using a novel
interaction analysis procedure between cognitive effects and distortion
metrics. The procedure establishes interactions between cognitive effects and
distortion metrics using LT data. The proposed IM metric is shown to outperform
previously proposed IM metrics in a validation task against subjective quality
scores from large and diverse LT databases. Particularly, the proposed system
showed an increased quality prediction of music signals coded with bandwidth
extension techniques, where other models frequently fail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Publication for WASPAA 2023 - IEEE Workshop on Applications
  of Signal Processing to Audio and Acoustics, Mohonk Mountain House, New
  Paltz, NY, USA, Oct 22-25, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Integration of Large Language Models into Automatic Speech
  Recognition Systems: An Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeping Min, Jinbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the integration of Large Language Models (LLMs) into
Automatic Speech Recognition (ASR) systems to improve transcription accuracy.
The increasing sophistication of LLMs, with their in-context learning
capabilities and instruction-following behavior, has drawn significant
attention in the field of Natural Language Processing (NLP). Our primary focus
is to investigate the potential of using an LLM's in-context learning
capabilities to enhance the performance of ASR systems, which currently face
challenges such as ambient noise, speaker accents, and complex linguistic
contexts. We designed a study using the Aishell-1 and LibriSpeech datasets,
with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.
Unfortunately, our initial experiments did not yield promising results,
indicating the complexity of leveraging LLM's in-context learning for ASR
applications. Despite further exploration with varied settings and models, the
corrected sentences from the LLMs frequently resulted in higher Word Error
Rates (WER), demonstrating the limitations of LLMs in speech applications. This
paper provides a detailed overview of these experiments, their results, and
implications, establishing that using LLMs' in-context learning capabilities to
correct potential errors in speech recognition transcriptions is still a
challenging task at the current stage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio
  <span class="highlight-title">Pretrain</span>ing for Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07848v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07848v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yanni Hu, Yuguang Yang, Jixun Yao, Wen Fei, Lei Ma, Heng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning based pretraining methods have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) model for speech emotion recognition. To be specific, we
first build an effective emotion CLAP model Emo-CLAP for emotion recognition,
utilizing various self-supervised learning based pre-trained models. Then,
considering the importance of the gender attribute in speech emotion modeling,
two GEmo-CLAP approaches are further proposed to integrate the emotion and
gender information of speech signals, forming more reasonable objectives.
Extensive experiments on the IEMOCAP corpus demonstrate that our proposed two
GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving superior recognition
performance compared with other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Singing Voice Synthesis Using Differentiable LPC and
  Glottal-Flow-Inspired Wavetables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chin-Yun Yu, György Fazekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for
singing voice synthesis (SVS) that exploits the physical characteristics of the
human voice using differentiable digital signal processing. GOLF employs a
glottal model as the harmonic source and IIR filters to simulate the vocal
tract, resulting in an interpretable and efficient approach. We show it is
competitive with state-of-the-art singing voice vocoders, requiring fewer
synthesis parameters and less memory to train, and runs an order of magnitude
faster for inference. Additionally, we demonstrate that GOLF can model the
phase components of the human voice, which has immense potential for rendering
and analysing singing voices in a differentiable manner. Our results highlight
the effectiveness of incorporating the physical properties of the human voice
mechanism into SVS and underscore the advantages of signal-processing-based
approaches, which offer greater interpretability and efficiency in synthesis.
Audio samples are available at https://yoyololicon.github.io/golf-demo/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures. Accepted at ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalization for <span class="highlight-title">BERT</span>-based Discriminative Speech Recognition
  Rescoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jari Kolehmainen, Yile Gu, Aditya Gourav, Prashanth Gurunath Shivakumar, Ankur Gandhe, Ariya Rastrow, Ivan Bulyko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognition of personalized content remains a challenge in end-to-end speech
recognition. We explore three novel approaches that use personalized content in
a neural rescoring step to improve recognition: gazetteers, prompting, and a
cross-attention based encoder-decoder model. We use internal de-identified
en-US data from interactions with a virtual voice assistant supplemented with
personalized named entities to compare these approaches. On a test set with
personalized named entities, we show that each of these approaches improves
word error rate by over 10%, against a neural rescoring baseline. We also show
that on this test set, natural language prompts can improve word error rate by
7% without any training and with a marginal loss in generalization. Overall,
gazetteers were found to perform the best with a 10% improvement in word error
rate (WER), while also improving WER on a general test set by 1%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the Deceptions: An Analysis on Audio Spoofing Detection and
  Future Prospects <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Ranjan, Mayank Vatsa, Richa Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio has become an increasingly crucial biometric modality due to its
ability to provide an intuitive way for humans to interact with machines. It is
currently being used for a range of applications, including person
authentication to banking to virtual assistants. Research has shown that these
systems are also susceptible to spoofing and attacks. Therefore, protecting
audio processing systems against fraudulent activities, such as identity theft,
financial fraud, and spreading misinformation, is of paramount importance. This
paper reviews the current state-of-the-art techniques for detecting audio
spoofing and discusses the current challenges along with open research
problems. The paper further highlights the importance of considering the
ethical and privacy implications of audio spoofing detection systems. Lastly,
the work aims to accentuate the need for building more robust and generalizable
methods, the integration of automatic speaker verification and countermeasure
systems, and better evaluation protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Metric of Informational Masking for Perceptual Audio Quality
  Measurement <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo M. Delgado, Jürgen Herre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual audio quality measurement systems algorithmically analyze the
output of audio processing systems to estimate possible perceived quality
degradation using perceptual models of human audition. In this manner, they
save the time and resources associated with the design and execution of
listening tests (LTs). Models of disturbance audibility predicting peripheral
auditory masking in quality measurement systems have considerably increased
subjective quality prediction performance of signals processed by perceptual
audio codecs. Additionally, cognitive effects have also been known to regulate
perceived distortion severity by influencing their salience. However, the
performance gains due to cognitive effect models in quality measurement systems
were inconsistent so far, particularly for music signals. Firstly, this paper
presents an improved model of informational masking (IM) -- an important
cognitive effect in quality perception -- that considers disturbance
information complexity around the masking threshold. Secondly, we incorporate
the proposed IM metric into a quality measurement systems using a novel
interaction analysis procedure between cognitive effects and distortion
metrics. The procedure establishes interactions between cognitive effects and
distortion metrics using LT data. The proposed IM metric is shown to outperform
previously proposed IM metrics in a validation task against subjective quality
scores from large and diverse LT databases. Particularly, the proposed system
showed an increased quality prediction of music signals coded with bandwidth
extension techniques, where other models frequently fail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Publication for WASPAA 2023 - IEEE Workshop on Applications
  of Signal Processing to Audio and Acoustics, Mohonk Mountain House, New
  Paltz, NY, USA, Oct 22-25, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LACE: A light-weight, causal model for enhancing coded speech through
  adaptive convolutions <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Büthe, Jean-Marc Valin, Ahmed Mustafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical speech coding uses low-complexity postfilters with zero lookahead
to enhance the quality of coded speech, but their effectiveness is limited by
their simplicity. Deep Neural Networks (DNNs) can be much more effective, but
require high complexity and model size, or added delay. We propose a DNN model
that generates classical filter kernels on a per-frame basis with a model of
just 300~K parameters and 100~MFLOPS complexity, which is a practical
complexity for desktop or mobile device CPUs. The lack of added delay allows it
to be integrated into the Opus codec, and we demonstrate that it enables
effective wideband encoding for bitrates down to 6 kb/s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted at WASPAA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Integration of Large Language Models into Automatic Speech
  Recognition Systems: An Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeping Min, Jinbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the integration of Large Language Models (LLMs) into
Automatic Speech Recognition (ASR) systems to improve transcription accuracy.
The increasing sophistication of LLMs, with their in-context learning
capabilities and instruction-following behavior, has drawn significant
attention in the field of Natural Language Processing (NLP). Our primary focus
is to investigate the potential of using an LLM's in-context learning
capabilities to enhance the performance of ASR systems, which currently face
challenges such as ambient noise, speaker accents, and complex linguistic
contexts. We designed a study using the Aishell-1 and LibriSpeech datasets,
with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.
Unfortunately, our initial experiments did not yield promising results,
indicating the complexity of leveraging LLM's in-context learning for ASR
applications. Despite further exploration with varied settings and models, the
corrected sentences from the LLMs frequently resulted in higher Word Error
Rates (WER), demonstrating the limitations of LLMs in speech applications. This
paper provides a detailed overview of these experiments, their results, and
implications, establishing that using LLMs' in-context learning capabilities to
correct potential errors in speech recognition transcriptions is still a
challenging task at the current stage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio
  <span class="highlight-title">Pretrain</span>ing for Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07848v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07848v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yanni Hu, Yuguang Yang, Jixun Yao, Wen Fei, Lei Ma, Heng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning based pretraining methods have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) model for speech emotion recognition. To be specific, we
first build an effective emotion CLAP model Emo-CLAP for emotion recognition,
utilizing various self-supervised learning based pre-trained models. Then,
considering the importance of the gender attribute in speech emotion modeling,
two GEmo-CLAP approaches are further proposed to integrate the emotion and
gender information of speech signals, forming more reasonable objectives.
Extensive experiments on the IEMOCAP corpus demonstrate that our proposed two
GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving superior recognition
performance compared with other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Singing Voice Synthesis Using Differentiable LPC and
  Glottal-Flow-Inspired Wavetables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chin-Yun Yu, György Fazekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for
singing voice synthesis (SVS) that exploits the physical characteristics of the
human voice using differentiable digital signal processing. GOLF employs a
glottal model as the harmonic source and IIR filters to simulate the vocal
tract, resulting in an interpretable and efficient approach. We show it is
competitive with state-of-the-art singing voice vocoders, requiring fewer
synthesis parameters and less memory to train, and runs an order of magnitude
faster for inference. Additionally, we demonstrate that GOLF can model the
phase components of the human voice, which has immense potential for rendering
and analysing singing voices in a differentiable manner. Our results highlight
the effectiveness of incorporating the physical properties of the human voice
mechanism into SVS and underscore the advantages of signal-processing-based
approaches, which offer greater interpretability and efficiency in synthesis.
Audio samples are available at https://yoyololicon.github.io/golf-demo/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures. Accepted at ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building a digital twin of EDFA: a grey-box modeling approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Liu, Xiaomin Liu, Yihao Zhang, Meng Cai, Mengfan Fu, Xueying Zhong, Lilin Yi, Weisheng Hu, Qunbi Zhuge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enable intelligent and self-driving optical networks, high-accuracy
physical layer models are required. The dynamic wavelength-dependent gain
effects of non-constant-pump erbium-doped fiber amplifiers (EDFAs) remain a
crucial problem in terms of modeling, as it determines optical-to-signal noise
ratio as well as the magnitude of fiber nonlinearities. Black-box data-driven
models have been widely studied, but it requires a large size of data for
training and suffers from poor generalizability. In this paper, we derive the
gain spectra of EDFAs as a simple univariable linear function, and then based
on it we propose a grey-box EDFA gain modeling scheme. Experimental results
show that for both automatic gain control (AGC) and automatic power control
(APC) EDFAs, our model built with 8 data samples can achieve better performance
than the neural network (NN) based model built with 900 data samples, which
means the required data size for modeling can be reduced by at least two orders
of magnitude. Moreover, in the experiment the proposed model demonstrates
superior generalizability to unseen scenarios since it is based on the
underlying physics of EDFAs. The results indicate that building a customized
digital twin of each EDFA in optical networks become feasible, which is
essential especially for next generation multi-band network operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defeating Proactive Jammers Using Deep Reinforcement Learning for
  Resource-Constrained IoT Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abubakar Sani Ali, Shimaa Naser, Sami Muhaidat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional anti-jamming techniques like spread spectrum, adaptive power/rate
control, and cognitive radio, have demonstrated effectiveness in mitigating
jamming attacks. However, their robustness against the growing complexity of
internet-of-thing (IoT) networks and diverse jamming attacks is still limited.
To address these challenges, machine learning (ML)-based techniques have
emerged as promising solutions. By offering adaptive and intelligent
anti-jamming capabilities, ML-based approaches can effectively adapt to dynamic
attack scenarios and overcome the limitations of traditional methods. In this
paper, we propose a deep reinforcement learning (DRL)-based approach that
utilizes state input from realistic wireless network interface cards. We train
five different variants of deep Q-network (DQN) agents to mitigate the effects
of jamming with the aim of identifying the most sample-efficient, lightweight,
robust, and least complex agent that is tailored for power-constrained devices.
The simulation results demonstrate the effectiveness of the proposed DRL-based
anti-jamming approach against proactive jammers, regardless of their jamming
strategy which eliminates the need for a pattern recognition or jamming
strategy detection step. Our findings present a promising solution for securing
IoT networks against jamming attacks and highlights substantial opportunities
for continued investigation and advancement within this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Demonstration of 3D Reflected Beamforming at sub6GHz thanks
  to Varactor Based Reconfigurable Intelligent Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Ratajczak, Eric Séguenot, Dinh-Thuy Phan-Huy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconfigurable intelligent surface (RIS) is a promising solution to boost
coverage sustainably by reflecting waves from a transmitter to a receiver and
acting as a low-power and passive relay. In this paper, for the first time, we
demonstrate experimentally that a reconfigurable intelligent surface designed
for sub6GHz, and using varactor technology, can perform three-dimensional
reflective beamforming. This result is achieved with a RIS prototype of 984
unit-cells, thanks to a compact control circuit individually addressing and
configuring the voltage of each unit-cell, with a distinct voltage. To our
knowledge, this prototype configures 17 to 70 times more distinct voltages than
in the state-of-the-art. The experimental results in an indoor environment show
a 10 dB gain. They also show, for the first time, that producing such a new
prototype is feasible with minimal energy footprint and environmental impact,
thanks to refurbishing. Indeed, a reflectarray antenna originally designed for
three-dimensional beamforming has been turned into a RIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotic SEP Analysis and Optimization of Linear-Quantized Precoding
  in Massive MIMO Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyu Wu, Junjie Ma, Ya-Feng Liu, A. Lee Swindlehurst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A promising approach to deal with the high hardware cost and energy
consumption of massive MIMO transmitters is to use low-resolution
digital-to-analog converters (DACs) at each antenna element. This leads to a
transmission scheme where the transmitted signals are restricted to a finite
set of voltage levels. This paper is concerned with the analysis and
optimization of a low-cost quantized precoding strategy, referred to as
linear-quantized precoding, for a downlink massive MIMO system under Rayleigh
fading. In linear-quantized precoding, the signals are first processed by a
linear precoding matrix and subsequently quantized component-wise by the DAC.
In this paper, we analyze both the signal-to-interference-plus-noise ratio
(SINR) and the symbol error probability (SEP) performances of such
linear-quantized precoding schemes in an asymptotic framework where the number
of transmit antennas and the number of users grow large with a fixed ratio. Our
results provide a rigorous justification for the heuristic arguments based on
the Bussgang decomposition that are commonly used in prior works. Based on the
asymptotic analysis, we further derive the optimal precoder within a class of
linear-quantized precoders that includes several popular precoders as special
cases. Our numerical results demonstrate the excellent accuracy of the
asymptotic analysis for finite systems and the optimality of the derived
precoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 8 figures, submitted for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Downlink Precoding for Cell-free FBMC/OQAM Systems With Asynchronous
  Reception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Qi, Jian Dang, Zaichen Zhang, Liang Wu, Yongpeng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, an efficient precoding design scheme is proposed for downlink
cell-free distributed massive multiple-input multiple-output (DM-MIMO) filter
bank multi-carrier (FBMC) systems with asynchronous reception and highly
frequency selectivity. The proposed scheme includes a multiple interpolation
structure to eliminate the impact of response difference we recently
discovered, which has better performance in highly frequency-selective
channels. Besides, we also consider the phase shift in asynchronous reception
and introduce a phase compensation in the design process. The phase
compensation also benefits from the multiple interpolation structure and better
adapts to asynchronous reception. Based on the proposed scheme, we
theoretically analyze its ergodic achievable rate performance and derive a
closed-form expression. Simulation results show that the derived expression can
accurately characterize the rate performance, and FBMC with the proposed scheme
outperforms orthogonal frequency-division multiplexing (OFDM) in the
asynchronous scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Metric of Informational Masking for Perceptual Audio Quality
  Measurement <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo M. Delgado, Jürgen Herre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual audio quality measurement systems algorithmically analyze the
output of audio processing systems to estimate possible perceived quality
degradation using perceptual models of human audition. In this manner, they
save the time and resources associated with the design and execution of
listening tests (LTs). Models of disturbance audibility predicting peripheral
auditory masking in quality measurement systems have considerably increased
subjective quality prediction performance of signals processed by perceptual
audio codecs. Additionally, cognitive effects have also been known to regulate
perceived distortion severity by influencing their salience. However, the
performance gains due to cognitive effect models in quality measurement systems
were inconsistent so far, particularly for music signals. Firstly, this paper
presents an improved model of informational masking (IM) -- an important
cognitive effect in quality perception -- that considers disturbance
information complexity around the masking threshold. Secondly, we incorporate
the proposed IM metric into a quality measurement systems using a novel
interaction analysis procedure between cognitive effects and distortion
metrics. The procedure establishes interactions between cognitive effects and
distortion metrics using LT data. The proposed IM metric is shown to outperform
previously proposed IM metrics in a validation task against subjective quality
scores from large and diverse LT databases. Particularly, the proposed system
showed an increased quality prediction of music signals coded with bandwidth
extension techniques, where other models frequently fail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Publication for WASPAA 2023 - IEEE Workshop on Applications
  of Signal Processing to Audio and Acoustics, Mohonk Mountain House, New
  Paltz, NY, USA, Oct 22-25, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coherent Compensation based ISAC Signal Processing for Long-range
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Wang, Zhiqing Wei, Liyan Su, Zhiyong Feng, Huici Wu, Dongsheng Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communication (ISAC) will greatly enhance the
efficiency of physical resource utilization. The design of ISAC signal based on
the orthogonal frequency division multiplex (OFDM) signal is the mainstream.
However, when detecting the long-range target, the delay of echo signal exceeds
CP duration, which will result in inter-symbol interference (ISI) and
inter-carrier interference (ICI), limiting the sensing range. Facing the above
problem, we propose to increase useful signal power through coherent
compensation and improve the signal to interference plus noise power ratio
(SINR) of each OFDM block. Compared with the traditional 2D-FFT algorithm, the
improvement of SINR of range-doppler map (RDM) is verified by simulation, which
will expand the sensing range.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intelligent Omni Surfaces assisted Integrated Multi Target Sensing and
  Multi User MIMO Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Zhang, Wen Chen, Qingqing Wu, Zhendong Li, Xusheng Zhu, Jinhong Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drawing inspiration from the advantages of intelligent reflecting surfaces
(IRS) in wireless networks,this paper presents a novel design for intelligent
omni surface (IOS) enabled integrated sensing and communications (ISAC). By
harnessing the power of multi antennas and a multitude of elements, the
dual-function base station (BS) and IOS collaborate to realize joint active and
passive beamforming, enabling seamless 360-degree ISAC coverage. The objective
is to maximize the minimum signal-tointerference-plus-noise ratio (SINR) of
multi-target sensing, while ensuring the multi-user multi-stream
communications. To achieve this, a comprehensive optimization approach is
employed, encompassing the design of radar receive vector, transmit beamforming
matrix, and IOS transmissive and reflective coefficients. Due to the non-convex
nature of the formulated problem, an auxiliary variable is introduced to
transform it into a more tractable form. Consequently, the problem is
decomposed into three subproblems based on the block coordinate descent
algorithm. Semidefinite relaxation and successive convex approximation methods
are leveraged to convert the sub-problem into a convex problem, while the
iterative rank minimization algorithm and penalty function method ensure the
equivalence. Furthermore,the scenario is extended to mode switching and time
switching protocols. Simulation results validate the convergence and superior
performance of the proposed algorithm compared to other benchmark algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Biophysics as Network Weights: Conditional Generative Models for
  Dynamic Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihan Ma, Alexander Kenneth Clarke, Kostiantyn Maksymenko, Samuel Deslauriers-Gauthier, Xinjun Sheng, Xiangyang Zhu, Dario Farina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulations of biophysical systems are fundamental for studying physiological
mechanisms and developing human machine interfaces. Whilst advanced numerical
methods, such as finite element models, can excel in this task, they are
extremely computationally expensive to use when generating a large number of
simulations or simulating dynamic events with continuously changing structural
parameters. We propose an architecture that uses a conditional generative model
to interpolate between the numerical model states, dramatically lowering the
modeling time while maintaining a high generation accuracy. As a demonstration
of this concept, we present BioMime, a hybrid-structured generative model that
enables an accurate, ultra-fast, and arbitrarily high temporal-resolution
simulation of a specific biophysical system during dynamic changes. This
methodology has wide applications in physiological and clinical research as
well as in supporting data augmentation strategies for signal analysis,
representing a computationally efficient and highly accurate model for
biophysical simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Broadband Channel Estimation for Intelligent Reflecting Surface Aided
  mmWave Massive MIMO Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.01629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.01629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Wan, Zhen Gao, Mohamed-Slim Alouini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the broadband channel estimation (CE) for intelligent
reflecting surface (IRS)-aided millimeter-wave (mmWave) massive MIMO systems.
The CE for such systems is a challenging task due to the large dimension of
both the active massive MIMO at the base station (BS) and passive IRS. To
address this problem, this paper proposes a compressive sensing (CS)-based CE
solution for IRS-aided mmWave massive MIMO systems, whereby the angular channel
sparsity of large-scale array at mmWave is exploited for improved CE with
reduced pilot overhead. Specifically, we first propose a downlink pilot
transmission framework. By designing the pilot signals based on the prior
knowledge that the line-of-sight dominated BS-to-IRS channel is known, the
high-dimensional channels for BS-to-user and IRS-to-user can be jointly
estimated based on CS theory. Moreover, to efficiently estimate broadband
channels, a distributed orthogonal matching pursuit algorithm is exploited,
where the common sparsity shared by the channels at different subcarriers is
utilized. Additionally, the redundant dictionary to combat the power leakage is
also designed for the enhanced CE performance. Simulation results demonstrate
the effectiveness of the proposed scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures. Accepted by IEEE International Conference on
  Communications (ICC) 2020, Dublin, Ireland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint User and Data Detection in Grant-Free NOMA with Attention-based
  BiLSTM Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saud Khan, Salman Durrani, Muhammad Basit Shahab, Sarah J. Johnson, Seyit Camtepe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the multi-user detection (MUD) problem in uplink grant-free
non-orthogonal multiple access (NOMA), where the access point has to identify
the total number and correct identity of the active Internet of Things (IoT)
devices and decode their transmitted data. We assume that IoT devices use
complex spreading sequences and transmit information in a random-access manner
following the burst-sparsity model, where some IoT devices transmit their data
in multiple adjacent time slots with a high probability, while others transmit
only once during a frame. Exploiting the temporal correlation, we propose an
attention-based bidirectional long short-term memory (BiLSTM) network to solve
the MUD problem. The BiLSTM network creates a pattern of the device activation
history using forward and reverse pass LSTMs, whereas the attention mechanism
provides essential context to the device activation points. By doing so, a
hierarchical pathway is followed for detecting active devices in a grant-free
scenario. Then, by utilising the complex spreading sequences, blind data
detection for the estimated active devices is performed. The proposed framework
does not require prior knowledge of device sparsity levels and channels for
performing MUD. The results show that the proposed network achieves better
performance compared to existing benchmark schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonic-Copuled Riccati Equations and its Applications in Distributed
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Qian, Peihu Duan, Zhisheng Duan, Ling shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The coupled Riccati equations are cosisted of multiple Riccati-like equations
with solutions coupled with each other, which can be applied to depict the
properties of more complex systems such as markovian systems or multi-agent
systems. This paper manages to formulate and investigate a new kind of coupled
Riccati equations, called harmonic-coupled Riccati equations (HCRE), from the
matrix iterative law of the consensus on information-based distributed
filtering (CIDF) algortihm proposed in [1], where the solutions of the
equations are coupled with harmonic means. Firstly, mild conditions of the
existence and uniqueness of the solution to HCRE are induced with collective
observability and primitiviness of weighting matrix. Then, it is proved that
the matrix iterative law of CIDF will converge to the unique solution of the
corresponding HCRE, hence can be used to obtain the solution to HCRE. Moreover,
through applying the novel theory of HCRE, it is pointed out that the real
estimation error covariance of CIDF will also become steady-state and the
convergent value is simplified as the solution to a discrete time Lyapunov
equation (DLE). Altogether, these new results develop the theory of the coupled
Riccati equations, and provide a novel perspective on the performance analysis
of CIDF algorithm, which sufficiently reduces the conservativeness of the
evaluation techniques in the literature. Finally, the theoretical results are
verified with numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAXiot: A Decentralized Authentication and Authorization Scheme for
  Dynamic IoT Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artur Philipp, Axel Küpper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated and decentralized networks supporting frequently changing system
participants are a requirement for future Internet of Things (IoT) use cases.
IoT devices and networks often lack adequate authentication and authorization
mechanisms, resulting in insufficient privacy for entities in such systems. In
this work we address both issues by designing a privacy preserving
challenge-response style authentication and authorization scheme based on
Decentralized Identifiers and Verifiable Credentials. Our solution allows a
decentralized permission management of frequently changing network participants
and supports authenticated encryption for data confidentiality. We demonstrate
our solution in an MQTT 5.0 scenario and evaluate its security, privacy
guarantees, and performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 3 listings, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory-Aware Rate Adaptation for Flying Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruben Queiros, Jose Ruela, Helder Fontes, Rui Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the trend towards ubiquitous wireless connectivity, there are
scenarios where the communications infrastructure is damaged and wireless
coverage is insufficient or does not exist, such as in natural disasters and
temporary crowded events. Flying networks, composed of Unmanned Aerial Vehicles
(UAV), have emerged as a flexible and cost-effective solution to provide
on-demand wireless connectivity in these scenarios. UAVs have the capability to
operate virtually everywhere, and the growing payload capacity makes them
suitable platforms to carry wireless communications hardware. The state of the
art in the field of flying networks is mainly focused on the optimal
positioning of the flying nodes, while the wireless link parameters are
configured with default values. On the other hand, current link adaptation
algorithms are mainly targeting fixed or low mobility scenarios.
  We propose a novel rate adaptation approach for flying networks, named
Trajectory Aware Rate Adaptation (TARA), which leverages the knowledge of
flying nodes' movement to predict future channel conditions and perform rate
adaptation accordingly. Simulation results of 100 different trajectories show
that our solution increases throughput by up to 53% and achieves an average
improvement of 14%, when compared with conventional rate adaptation algorithms
such as Minstrel-HT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target Acquired? Evaluating Target Generation Algorithms for IPv6 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lion Steger, Liming Kuang, Johannes Zirngibl, Georg Carle, Oliver Gasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet measurements are a crucial foundation of IPv6-related research. Due
to the infeasibility of full address space scans for IPv6 however, those
measurements rely on collections of reliably responsive, unbiased addresses, as
provided e.g., by the IPv6 Hitlist service. Although used for various use
cases, the hitlist provides an unfiltered list of responsive addresses, the
hosts behind which can come from a range of different networks and devices,
such as web servers, customer-premises equipment (CPE) devices, and Internet
infrastructure. In this paper, we demonstrate the importance of tailoring
hitlists in accordance with the research goal in question. By using PeeringDB
we classify hitlist addresses into six different network categories, uncovering
that 42% of hitlist addresses are in ISP networks. Moreover, we show the
different behavior of those addresses depending on their respective category,
e.g., ISP addresses exhibiting a relatively low lifetime. Furthermore, we
analyze different Target Generation Algorithms (TGAs), which are used to
increase the coverage of IPv6 measurements by generating new responsive targets
for scans. We evaluate their performance under various conditions and find
generated addresses to show vastly differing responsiveness levels for
different TGAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the Network Traffic Measurement and Analysis
  Conference (TMA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Capacity Enhancement using Air Computing: An Earthquake Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baris Yamansavascilar, Atay Ozgovde, Cem Ersoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earthquakes are one of the most destructive natural disasters harming life
and the infrastructure of cities. After an earthquake, functioning
communication and computational capacity are crucial for rescue teams and
healthcare of victims. Therefore, an earthquake can be investigated for dynamic
capacity enhancement in which additional resources are deployed since the
surviving portion of the infrastructure may not meet the demand of the users.
In this study, we propose a new computation paradigm, air computing, which is
the air vehicle assisted next generation edge computing through different air
platforms, in order to enhance the capacity of the areas affected by an
earthquake. To this end, we put forward a novel paradigm that presents a
dynamic, responsive, and high-resolution computation environment by explaining
its corresponding components, air layers, and essential advantages. Moreover,
we focus on the unmanned aerial vehicle (UAV) deployment problem and apply
three different methods including the emergency method, the load balancing
method, and the location selection index (LSI) method in which we take the
delay requirements of applications into account. To test and compare their
performance in terms of the task success rate, we developed an earthquake
scenario in which three towns are affected with different severity. The
experimental results showed that each method can be beneficial considering the
circumstances, and goal of the rescue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Li, Billy Lau, Xin Yuan, Wei Ni, Mohsen Guizani, Chau Yuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multivariate Time Series characterization and forecasting of VoIP
  traffic in real mobile networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Di Mauro, Giovanni Galatro, Fabio Postiglione, Wei Song, Antonio Liotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the behavior of real-time traffic (e.g., VoIP) in mobility
scenarios could help the operators to better plan their network infrastructures
and to optimize the allocation of resources. Accordingly, in this work the
authors propose a forecasting analysis of crucial QoS/QoE descriptors (some of
which neglected in the technical literature) of VoIP traffic in a real mobile
environment. The problem is formulated in terms of a multivariate time series
analysis. Such a formalization allows to discover and model the temporal
relationships among various descriptors and to forecast their behaviors for
future periods. Techniques such as Vector Autoregressive models and machine
learning (deep-based and tree-based) approaches are employed and compared in
terms of performance and time complexity, by reframing the multivariate time
series problem into a supervised learning one. Moreover, a series of auxiliary
analyses (stationarity, orthogonal impulse responses, etc.) are performed to
discover the analytical structure of the time series and to provide deep
insights about their relationships. The whole theoretical analysis has an
experimental counterpart since a set of trials across a real-world LTE-Advanced
environment has been performed to collect, post-process and analyze about
600,000 voice packets, organized per flow and differentiated per codec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a 6G embedding sustainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esteban Selva, Azeddine Gati, Marie- Hélène Hamon, Giorgio Calochira, Giuseppe Avino, Bahare Masood Khorsandi, Stefan Wunderer, Stefan Wänstedt, Pernilla Bergmark, Serge Bories, Tommy Svensson, Marja Matinmikko-Blue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From its conception, 6G is being designed with a particular focus on
sustainability. The general philosophy of the H2020 Hexa-X project work on
sustainability in 6G is based on two principles: to reduce direct negative life
cycle impacts of 6G systems as much as possible (Sustainable 6G) and to analyze
use cases that maximize positive environmental, social, and economic effects in
other sectors of society (6G for Sustainability or its enablement effect). To
apply this philosophy, Hexa-X is designing 6G with three sustainability
objectives in mind: to enable the reduction of emissions in 6G-powered sectors
of society, to reduce the total cost of ownership and to improve energy
efficiency. This paper describes these objectives, their associated KPIs and
quantitative targets, and the levers to reach them. Furthermore, to maximize
the positive effects of 6G through the enablement effect, a link between 6G and
the United Nations' Sustainable Development Goals (UN SDGs) framework is
proposed and illustrated by Hexa-X use case families.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE ICC 2023 Second International Workshop on Green and Sustainable
  Networking (GreenNet), May 2023, Rome, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLOS Dies Twice: Challenges and Solutions of V2X for Cooperative
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lantao Li, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent multi-lidar sensor fusion between connected vehicles for
cooperative perception has recently been recognized as the best technique for
minimizing the blind zone of individual vehicular perception systems and
further enhancing the overall safety of autonomous driving systems. This
technique relies heavily on the reliability and availability of
vehicle-to-everything (V2X) communication. In practical sensor fusion
application scenarios, the non-line-of-sight (NLOS) issue causes blind zones
for not only the perception system but also V2X direct communication. To
counteract underlying communication issues, we introduce an abstract perception
matrix matching method for quick sensor fusion matching procedures and
mobility-height hybrid relay determination procedures, proactively improving
the efficiency and performance of V2X communication to serve the upper layer
application fusion requirements. To demonstrate the effectiveness of our
solution, we design a new simulation framework to consider autonomous driving,
sensor fusion and V2X communication in general, paving the way for end-to-end
performance evaluation and further solution derivation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission to IEEE Vehicular Technology Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud
  Computing Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Zhang, Xuling Zhang, Guangzhi Zhu, Yuyang Wang, Pan Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to
empower various areas as a bridge between physical objects and the digital
world. Through virtualization and simulation techniques, multiple functions can
be achieved by leveraging computing resources. In this process, Mobile Cloud
Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key
factors to achieve real-time feedback. However, current works only considered
edge servers or cloud servers in the DT system models. Besides, The models
ignore the DT with not only one data resource. In this paper, we propose a new
DT system model considering a heterogeneous MEC/MCC environment. Each DT in the
model is maintained in one of the servers via multiple data collection devices.
The offloading decision-making problem is also considered and a new offloading
scheme is proposed based on Distributed Deep Learning (DDL). Simulation results
demonstrate that our proposed algorithm can effectively and efficiently
decrease the system's average latency and energy consumption. Significant
improvement is achieved compared with the baselines under the dynamic
environment of DTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint User and Data Detection in Grant-Free NOMA with Attention-based
  BiLSTM Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saud Khan, Salman Durrani, Muhammad Basit Shahab, Sarah J. Johnson, Seyit Camtepe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the multi-user detection (MUD) problem in uplink grant-free
non-orthogonal multiple access (NOMA), where the access point has to identify
the total number and correct identity of the active Internet of Things (IoT)
devices and decode their transmitted data. We assume that IoT devices use
complex spreading sequences and transmit information in a random-access manner
following the burst-sparsity model, where some IoT devices transmit their data
in multiple adjacent time slots with a high probability, while others transmit
only once during a frame. Exploiting the temporal correlation, we propose an
attention-based bidirectional long short-term memory (BiLSTM) network to solve
the MUD problem. The BiLSTM network creates a pattern of the device activation
history using forward and reverse pass LSTMs, whereas the attention mechanism
provides essential context to the device activation points. By doing so, a
hierarchical pathway is followed for detecting active devices in a grant-free
scenario. Then, by utilising the complex spreading sequences, blind data
detection for the estimated active devices is performed. The proposed framework
does not require prior knowledge of device sparsity levels and channels for
performing MUD. The results show that the proposed network achieves better
performance compared to existing benchmark schemes.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Connection between Game-Theoretic Feature Attributions and
  Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Albini, Shubham Sharma, Saumitra Mishra, Danial Dervovic, Daniele Magazzeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) has received widespread interest in
recent years, and two of the most popular types of explanations are feature
attributions, and counterfactual explanations. These classes of approaches have
been largely studied independently and the few attempts at reconciling them
have been primarily empirical. This work establishes a clear theoretical
connection between game-theoretic feature attributions, focusing on but not
limited to SHAP, and counterfactuals explanations. After motivating operative
changes to Shapley values based feature attributions and counterfactual
explanations, we prove that, under conditions, they are in fact equivalent. We
then extend the equivalency result to game-theoretic solution concepts beyond
Shapley values. Moreover, through the analysis of the conditions of such
equivalence, we shed light on the limitations of naively using counterfactual
explanations to provide feature importances. Experiments on three datasets
quantitatively show the difference in explanations at every stage of the
connection between the two approaches and corroborate the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AIES 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual
  Language Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuijing Liu, Aamir Hasan, Kaiwen Hong, Runxuan Wang, Peixin Chang, Zachary Mizrachi, Justin Lin, D. Livingston McPherson, Wendy A. Rogers, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persons with visual impairments (PwVI) have difficulties understanding and
navigating spaces around them. Current wayfinding technologies either focus
solely on navigation or provide limited communication about the environment.
Motivated by recent advances in visual-language grounding and semantic
navigation, we propose DRAGON, a guiding robot powered by a dialogue system and
the ability to associate the environment with natural language. By
understanding the commands from the user, DRAGON is able to guide the user to
the desired landmarks on the map, describe the environment, and answer
questions from visual observations. Through effective utilization of dialogue,
the robot can ground the user's free-form descriptions to landmarks in the
environment, and give the user semantic information through spoken language. We
conduct a user study with blindfolded participants in an everyday indoor
environment. Our results demonstrate that DRAGON is able to communicate with
the user smoothly, provide a good guiding experience, and connect users with
their surrounding environment in an intuitive manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Webpage and videos are at
  https://sites.google.com/view/dragon-wayfinding/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Li, Billy Lau, Xin Yuan, Wei Ni, Mohsen Guizani, Chau Yuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming the Mental Set Effect in Programming Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agnia Sergeyuk, Sergey Titov, Yaroslav Golubev, Timofey Bryksin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper adopts a cognitive psychology perspective to investigate the
recurring mistakes in code resulting from the mental set (Einstellung) effect.
The Einstellung effect is the tendency to approach problem-solving with a
preconceived mindset, often overlooking better solutions that may be available.
This effect can significantly impact creative thinking, as the development of
patterns of thought can hinder the emergence of novel and creative ideas. Our
study aims to test the Einstellung effect and the two mechanisms of its
overcoming in the field of programming. The first intervention was the change
of the color scheme of the code editor to the less habitual one. The second
intervention was a combination of instruction to "forget the previous solutions
and tasks" and the change in the color scheme. During the experiment,
participants were given two sets of four programming tasks. Each task had two
possible solutions: one using suboptimal code dictated by the mental set, and
the other using a less familiar but more efficient and recommended methodology.
Between the sets, participants either received no treatment or one of two
interventions aimed at helping them overcome the mental set. The results of our
experiment suggest that the tested techniques were insufficient to support
overcoming the mental set, which we attribute to the specificity of the
programming domain. The study contributes to the existing literature by
providing insights into creativity support during problem-solving in software
development and offering a framework for experimental research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to PPIG'23, 15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Exactly is an Insight? A Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leilani Battle, Alvitta Ottley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Insights are often considered the ideal outcome of visual analysis sessions.
However, there is no single definition of what an insight is. Some scholars
define insights as correlations, while others define them as hypotheses or aha
moments. This lack of a clear definition can make it difficult to build
visualization tools that effectively support insight discovery. In this paper,
we contribute a comprehensive literature review that maps the landscape of
existing insight definitions. We summarize key themes regarding how insight is
defined, with the goal of helping readers identify which definitions of insight
align closely with their research and tool development goals. Based on our
review, we also suggest interesting research directions, such as synthesizing a
unified formalism for insight and connecting theories of insight to other
critical concepts in visualization research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. arXiv admin note: text overlap with
  arXiv:2206.04767</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">59</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, Kfir Aberman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalization has emerged as a prominent aspect within the field of
generative AI, enabling the synthesis of individuals in diverse contexts and
styles, while retaining high-fidelity to their identities. However, the process
of personalization presents inherent challenges in terms of time and memory
requirements. Fine-tuning each personalized model needs considerable GPU time
investment, and storing a personalized model per subject can be demanding in
terms of storage capacity. To overcome these challenges, we propose
HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of
personalized weights from a single image of a person. By composing these
weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth
can generate a person's face in various contexts and styles, with high subject
details while also preserving the model's crucial knowledge of diverse styles
and semantic modifications. Our method achieves personalization on faces in
roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual
Inversion, using as few as one reference image, with the same quality and style
diversity as DreamBooth. Also our method yields a model that is 10000x smaller
than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://hyperdreambooth.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent video recognition models utilize Transformer models for long-range
spatio-temporal context modeling. Video transformer designs are based on
self-attention that can model global context at a high computational cost. In
comparison, convolutional designs for videos offer an efficient alternative but
lack long-range dependency modeling. Towards achieving the best of both
designs, this work proposes Video-FocalNet, an effective and efficient
architecture for video recognition that models both local and global contexts.
Video-FocalNet is based on a spatio-temporal focal modulation architecture that
reverses the interaction and aggregation steps of self-attention for better
efficiency. Further, the aggregation step and the interaction step are both
implemented using efficient convolution and element-wise multiplication
operations that are computationally less expensive than their self-attention
counterparts on video representations. We extensively explore the design space
of focal modulation-based spatio-temporal context modeling and demonstrate our
parallel spatial and temporal encoding design to be the optimal choice.
Video-FocalNets perform favorably well against the state-of-the-art
transformer-based models for video recognition on three large-scale datasets
(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our
code/models are released at https://github.com/TalalWasim/Video-FocalNets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://TalalWasim.github.io/Video-FocalNets/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-context Autoencoder for Context Compression in a Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the In-context Autoencoder (ICAE) for context compression in a
large language model (LLM). The ICAE has two modules: a learnable encoder
adapted with LoRA from an LLM for compressing a long context into a limited
number of memory slots, and a fixed decoder which is the target LLM that can
condition on the memory slots for various purposes. We first pretrain the ICAE
using both autoencoding and language modeling objectives on massive text data,
enabling it to generate memory slots that accurately and comprehensively
represent the original context. Then, we fine-tune the pretrained ICAE on a
small amount of instruct data to enhance its interaction with various prompts
for producing desirable responses. Our experimental results demonstrate that
the ICAE learned with our proposed pretraining and fine-tuning paradigm can
effectively produce memory slots with $4\times$ context compression, which can
be well conditioned on by the target LLM to respond to various prompts. The
promising results demonstrate significant implications of the ICAE for its
novel approach to the long context problem and its potential to reduce
computation and memory overheads for LLM inference in practice, suggesting
further research effort in context management for an LLM. Our code and data
will be released shortly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Connection between Game-Theoretic Feature Attributions and
  Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Albini, Shubham Sharma, Saumitra Mishra, Danial Dervovic, Daniele Magazzeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) has received widespread interest in
recent years, and two of the most popular types of explanations are feature
attributions, and counterfactual explanations. These classes of approaches have
been largely studied independently and the few attempts at reconciling them
have been primarily empirical. This work establishes a clear theoretical
connection between game-theoretic feature attributions, focusing on but not
limited to SHAP, and counterfactuals explanations. After motivating operative
changes to Shapley values based feature attributions and counterfactual
explanations, we prove that, under conditions, they are in fact equivalent. We
then extend the equivalency result to game-theoretic solution concepts beyond
Shapley values. Moreover, through the analysis of the conditions of such
equivalence, we shed light on the limitations of naively using counterfactual
explanations to provide feature importances. Experiments on three datasets
quantitatively show the difference in explanations at every stage of the
connection between the two approaches and corroborate the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AIES 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual
  Language Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuijing Liu, Aamir Hasan, Kaiwen Hong, Runxuan Wang, Peixin Chang, Zachary Mizrachi, Justin Lin, D. Livingston McPherson, Wendy A. Rogers, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persons with visual impairments (PwVI) have difficulties understanding and
navigating spaces around them. Current wayfinding technologies either focus
solely on navigation or provide limited communication about the environment.
Motivated by recent advances in visual-language grounding and semantic
navigation, we propose DRAGON, a guiding robot powered by a dialogue system and
the ability to associate the environment with natural language. By
understanding the commands from the user, DRAGON is able to guide the user to
the desired landmarks on the map, describe the environment, and answer
questions from visual observations. Through effective utilization of dialogue,
the robot can ground the user's free-form descriptions to landmarks in the
environment, and give the user semantic information through spoken language. We
conduct a user study with blindfolded participants in an everyday indoor
environment. Our results demonstrate that DRAGON is able to communicate with
the user smoothly, provide a good guiding experience, and connect users with
their surrounding environment in an intuitive manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Webpage and videos are at
  https://sites.google.com/view/dragon-wayfinding/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-assisted Knowledge Graph Engineering: Experiments with Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt Junghanns, Roy Meissner, Gordian Dziwis, Kirill Bulert, Michael Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KG) provide us with a structured, flexible, transparent,
cross-system, and collaborative way of organizing our knowledge and data across
various domains in society and industrial as well as scientific disciplines.
KGs surpass any other form of representation in terms of effectiveness.
However, Knowledge Graph Engineering (KGE) requires in-depth experiences of
graph structures, web technologies, existing models and vocabularies, rule
sets, logic, as well as best practices. It also demands a significant amount of
work. Considering the advancements in large language models (LLMs) and their
interfaces and applications in recent years, we have conducted comprehensive
experiments with ChatGPT to explore its potential in supporting KGE. In this
paper, we present a selection of these experiments and their results to
demonstrate how ChatGPT can assist us in the development and management of KGs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in conference proceedings of AI-Tomorrow-23, 29.+30.6.2023
  in Leipzig, Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Unique Concept Vectors through Latent Space Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mara Graziani, Laura O' Mahony, An-Phi Nguyen, Henning Müller, Vincent Andrearczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpreting the inner workings of deep learning models is crucial for
establishing trust and ensuring model safety. Concept-based explanations have
emerged as a superior approach that is more interpretable than feature
attribution estimates such as pixel saliency. However, defining the concepts
for the interpretability analysis biases the explanations by the user's
expectations on the concepts. To address this, we propose a novel post-hoc
unsupervised method that automatically uncovers the concepts learned by deep
models during training. By decomposing the latent space of a layer in singular
vectors and refining them by unsupervised clustering, we uncover concept
vectors aligned with directions of high variance that are relevant to the model
prediction, and that point to semantically distinct concepts. Our extensive
experiments reveal that the majority of our concepts are readily understandable
to humans, exhibit coherency, and bear relevance to the task at hand. Moreover,
we showcase the practical utility of our method in dataset exploration, where
our concept vectors successfully identify outlier training samples affected by
various confounding factors. This novel exploration technique has remarkable
versatility to data types and model architectures and it will facilitate the
identification of biases and the discovery of sources of error within training
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Benchmarks for Factuality Evaluation of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, Yoav Shoham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Before deploying a language model (LM) within a given domain, it is important
to measure its tendency to generate factually incorrect information in that
domain. Existing factual generation evaluation methods focus on facts sampled
from the LM itself, and thus do not control the set of evaluated facts and
might under-represent rare and unlikely facts. We propose FACTOR: Factual
Assessment via Corpus TransfORmation, a scalable approach for evaluating LM
factuality. FACTOR automatically transforms a factual corpus of interest into a
benchmark evaluating an LM's propensity to generate true facts from the corpus
vs. similar but incorrect statements. We use our framework to create two
benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores
increase with model size and improve when the LM is augmented with retrieval;
(ii) benchmark score correlates with perplexity, but the two metrics do not
always agree on model ranking; and (iii) when perplexity and benchmark score
disagree, the latter better reflects factuality in open-ended generation, as
measured by human annotators. We make our data and code publicly available in
https://github.com/AI21Labs/factor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The complexity of non-stationary reinforcement learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Papadimitriou, Binghui Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of continual learning in the domain of reinforcement learning,
often called non-stationary reinforcement learning, has been identified as an
important challenge to the application of reinforcement learning. We prove a
worst-case complexity result, which we believe captures this challenge:
Modifying the probabilities or the reward of a single state-action pair in a
reinforcement learning problem requires an amount of time almost as large as
the number of states in order to keep the value function up to date, unless the
strong exponential time hypothesis (SETH) is false; SETH is a widely accepted
strengthening of the P $\neq$ NP conjecture. Recall that the number of states
in current applications of reinforcement learning is typically astronomical. In
contrast, we show that just $\textit{adding}$ a new state-action pair is
considerably easier to implement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied Lifelong Learning for Task and Motion Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge A. Mendez, Leslie Pack Kaelbling, Tomás Lozano-Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robot deployed in a home over long stretches of time faces a true lifelong
learning problem. As it seeks to provide assistance to its users, the robot
should leverage any accumulated experience to improve its own knowledge to
become a more proficient assistant. We formalize this setting with a novel
lifelong learning problem formulation in the context of learning for task and
motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a
generative mixture model that produces candidate continuous parameters for a
planner. Whereas most existing lifelong learning approaches determine a priori
how data is shared across task models, our approach learns shared and
non-shared models and determines which to use online during planning based on
auxiliary tasks that serve as a proxy for each model's understanding of a
state. Our method exhibits substantial improvements in planning success on
simulated 2D domains and on several problems from the BEHAVIOR benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DecompEval: Evaluating Generated Texts as Unsupervised Decomposed
  Question Answering <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Ke, Fei Huang, Fei Mi, Yasheng Wang, Qun Liu, Xiaoyan Zhu, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing evaluation metrics for natural language generation (NLG) tasks face
the challenges on generalization ability and interpretability. Specifically,
most of the well-performed metrics are required to train on evaluation datasets
of specific NLG tasks and evaluation dimensions, which may cause over-fitting
to task-specific datasets. Furthermore, existing metrics only provide an
evaluation score for each dimension without revealing the evidence to interpret
how this score is obtained. To deal with these challenges, we propose a simple
yet effective metric called DecompEval. This metric formulates NLG evaluation
as an instruction-style question answering task and utilizes instruction-tuned
pre-trained language models (PLMs) without training on evaluation datasets,
aiming to enhance the generalization ability. To make the evaluation process
more interpretable, we decompose our devised instruction-style question about
the quality of generated texts into the subquestions that measure the quality
of each sentence. The subquestions with their answers generated by PLMs are
then recomposed as evidence to obtain the evaluation result. Experimental
results show that DecompEval achieves state-of-the-art performance in untrained
metrics for evaluating text summarization and dialogue generation, which also
exhibits strong dimension-level / task-level generalization ability and
interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>s Should not be Seen as Secrets: Systematically Measuring <span class="highlight-title">Prompt</span>
  Extraction Attack Success 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Daphne Ippolito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generations of large language models are commonly controlled through
prompting techniques, where a user's query to the model is prefixed with a
prompt that aims to guide the model's behaviour on the query. The prompts used
by companies to guide their models are often treated as secrets, to be hidden
from the user making the query. They have even been treated as commodities to
be bought and sold. However, there has been anecdotal evidence showing that the
prompts can be extracted by a user even when they are kept secret. In this
paper, we present a framework for systematically measuring the success of
prompt extraction attacks. In experiments with multiple sources of prompts and
multiple underlying language models, we find that simple text-based attacks can
in fact reveal prompts with high probability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Learning for Interactive Perception of Surgical Thread
  for Autonomous Suture Tail-Shortening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Schorp, Will Panitch, Kaushik Shivakumar, Vainavi Viswanath, Justin Kerr, Yahav Avigal, Danyal M Fer, Lionel Ott, Ken Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D sensing of suturing thread is a challenging problem in automated
surgical suturing because of the high state-space complexity, thinness and
deformability of the thread, and possibility of occlusion by the grippers and
tissue. In this work we present a method for tracking surgical thread in 3D
which is robust to occlusions and complex thread configurations, and apply it
to autonomously perform the surgical suture "tail-shortening" task: pulling
thread through tissue until a desired "tail" length remains exposed. The method
utilizes a learned 2D surgical thread detection network to segment suturing
thread in RGB images. It then identifies the thread path in 2D and reconstructs
the thread in 3D as a NURBS spline by triangulating the detections from two
stereo cameras. Once a 3D thread model is initialized, the method tracks the
thread across subsequent frames. Experiments suggest the method achieves a 1.33
pixel average reprojection error on challenging single-frame 3D thread
reconstructions, and an 0.84 pixel average reprojection error on two tracking
sequences. On the tail-shortening task, it accomplishes a 90% success rate
across 20 trials. Supplemental materials are available at
https://sites.google.com/berkeley.edu/autolab-surgical-thread/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Automation Science and Engineering (CASE)
  2023, 7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Causal Framework to Unify Common Domain Generalization Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nevin L. Zhang, Kaican Li, Han Gao, Weiyan Xie, Zhi Lin, Zhenguo Li, Luning Wang, Yongxiang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) is about learning models that generalize well to
new domains that are related to, but different from, the training domain(s). It
is a fundamental problem in machine learning and has attracted much attention
in recent years. A large number of approaches have been proposed. Different
approaches are motivated from different perspectives, making it difficult to
gain an overall understanding of the area. In this paper, we propose a causal
framework for domain generalization and present an understanding of common DG
approaches in the framework. Our work sheds new lights on the following
questions: (1) What are the key ideas behind each DG method? (2) Why is it
expected to improve generalization to new domains theoretically? (3) How are
different DG methods related to each other and what are relative advantages and
limitations? By providing a unified perspective on DG, we hope to help
researchers better understand the underlying principles and develop more
effective approaches for this critical problem in machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinyMetaFed: Efficient Federated Meta-Learning for TinyML <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Ren, Xue Li, Darko Anicic, Thomas A. Runkler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Tiny Machine Learning (TinyML) has made substantial advancements
in democratizing machine learning on low-footprint devices, such as
microcontrollers. The prevalence of these miniature devices raises the question
of whether aggregating their knowledge can benefit TinyML applications.
Federated meta-learning is a promising answer to this question, as it addresses
the scarcity of labeled data and heterogeneous data distribution across devices
in the real world. However, deploying TinyML hardware faces unique resource
constraints, making existing methods impractical due to energy, privacy, and
communication limitations. We introduce TinyMetaFed, a model-agnostic
meta-learning framework suitable for TinyML. TinyMetaFed facilitates
collaborative training of a neural network initialization that can be quickly
fine-tuned on new devices. It offers communication savings and privacy
protection through partial local reconstruction and Top-P% selective
communication, computational efficiency via online learning, and robustness to
client heterogeneity through few-shot learning. The evaluations on three TinyML
use cases demonstrate that TinyMetaFed can significantly reduce energy
consumption and communication overhead, accelerate convergence, and stabilize
the training process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the ECML PKDD 2023 workshop track: Simplification,
  Compression, Efficiency, and Frugality for Artificial Intelligence (SCEFA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negated Complementary Commonsense using Large Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Rezaei, Marek Z. Reformat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Larger language models, such as GPT-3, have shown to be excellent in many
tasks. However, we demonstrate that out-of-ordinary questions can throw the
model off guard. This work focuses on finding answers to negated complementary
questions in commonsense scenarios. We illustrate how such questions adversely
affect the model responses. We propose a model-agnostic methodology to improve
the performance in negated complementary scenarios. Our method outperforms
few-shot generation from GPT-3 (by more than 11 points) and, more importantly,
highlights the significance of studying the response of large language models
in negated complementary questions. The code, data, and experiments are
available under: https://github.com/navidre/negated_complementary_commonsense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared in Natural Language Reasoning and Structured Explanations
  Workshop (NLRSE) - ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layered controller synthesis for dynamic multi-agent systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Clement, Nicolas Perrin-Gilbert, Philipp Schlehuber-Caissier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a layered approach for multi-agent control problem,
decomposed into three stages, each building upon the results of the previous
one. First, a high-level plan for a coarse abstraction of the system is
computed, relying on parametric timed automata augmented with stopwatches as
they allow to efficiently model simplified dynamics of such systems. In the
second stage, the high-level plan, based on SMT-formulation, mainly handles the
combinatorial aspects of the problem, provides a more dynamically accurate
solution. These stages are collectively referred to as the SWA-SMT solver. They
are correct by construction but lack a crucial feature: they cannot be executed
in real time. To overcome this, we use SWA-SMT solutions as the initial
training dataset for our last stage, which aims at obtaining a neural network
control policy. We use reinforcement learning to train the policy, and show
that the initial dataset is crucial for the overall success of the method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling
  Services: A Multi-Agent Hierarchical Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhua Si, Fang He, Xi Lin, Xindi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integrated development of city clusters has given rise to an increasing
demand for intercity travel. Intercity ride-pooling service exhibits
considerable potential in upgrading traditional intercity bus services by
implementing demand-responsive enhancements. Nevertheless, its online
operations suffer the inherent complexities due to the coupling of vehicle
resource allocation among cities and pooled-ride vehicle routing. To tackle
these challenges, this study proposes a two-level framework designed to
facilitate online fleet management. Specifically, a novel multi-agent feudal
reinforcement learning model is proposed at the upper level of the framework to
cooperatively assign idle vehicles to different intercity lines, while the
lower level updates the routes of vehicles using an adaptive large neighborhood
search heuristic. Numerical studies based on the realistic dataset of Xiamen
and its surrounding cities in China show that the proposed framework
effectively mitigates the supply and demand imbalances, and achieves
significant improvement in both the average daily system profit and order
fulfillment ratio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series
  Interpretable Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlong Zhao, Xiang Ma, Xuemei Li, Caiming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting has received wide interest from existing research due
to its broad applications and inherent challenging. The research challenge lies
in identifying effective patterns in historical series and applying them to
future forecasting. Advanced models based on point-wise connected MLP and
Transformer architectures have strong fitting power, but their secondary
computational complexity limits practicality. Additionally, those structures
inherently disrupt the temporal order, reducing the information utilization and
making the forecasting process uninterpretable. To solve these problems, this
paper proposes a forecasting model, MPR-Net. It first adaptively decomposes
multi-scale historical series patterns using convolution operation, then
constructs a pattern extension forecasting method based on the prior knowledge
of pattern reproduction, and finally reconstructs future patterns into future
series using deconvolution operation. By leveraging the temporal dependencies
present in the time series, MPR-Net not only achieves linear time complexity,
but also makes the forecasting process interpretable. By carrying out
sufficient experiments on more than ten real data sets of both short and long
term forecasting tasks, MPR-Net achieves the state of the art forecasting
performance, as well as good generalization and robustness performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAN is superior to GraphRNN: node orderings, kernel- and graph
  embeddings-based metrics for graph generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ousmane Touat, Julian Stier, Pierre-Edouard Portier, Michael Granitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A wide variety of generative models for graphs have been proposed. They are
used in drug discovery, road networks, neural architecture search, and program
synthesis. Generating graphs has theoretical challenges, such as isomorphic
representations -- evaluating how well a generative model performs is
difficult. Which model to choose depending on the application domain?
  We extensively study kernel-based metrics on distributions of graph
invariants and manifold-based and kernel-based metrics in graph embedding
space. Manifold-based metrics outperform kernel-based metrics in embedding
space. We use these metrics to compare GraphRNN and GRAN, two well-known
generative models for graphs, and unveil the influence of node orderings. It
shows the superiority of GRAN over GraphRNN - further, our proposed adaptation
of GraphRNN with a depth-first search ordering is effective for small-sized
graphs.
  A guideline on good practices regarding dataset selection and node feature
initialization is provided. Our work is accompanied by open-source code and
reproducible experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint for The 9th International Conference on machine Learning,
  Optimization and Data science - LOD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized
  Variational Autoencoder for Video Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Adiban, Kalin Stefanov, Sabato Marco Siniscalchi, Giampiero Salvi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the video prediction task by putting forth a novel model that
combines (i) our recently proposed hierarchical residual vector quantized
variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN
(ST-PixelCNN). We refer to this approach as a sequential hierarchical residual
learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging
the intrinsic capabilities of HR-VQVAE at modeling still images with a
parsimonious representation, combined with the ST-PixelCNN's ability at
handling spatiotemporal information, S-HR-VQVAE can better deal with chief
challenges in video prediction. These include learning spatiotemporal
information, handling high dimensional data, combating blurry prediction, and
implicit modeling of physical characteristics. Extensive experimental results
on the KTH Human Action and Moving-MNIST tasks demonstrate that our model
compares favorably against top video prediction techniques both in quantitative
and qualitative evaluations despite a much smaller model size. Finally, we
boost S-HR-VQVAE by proposing a novel training method to jointly estimate the
HR-VQVAE and ST-PixelCNN parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 3 tables. Submitted to IEEE Transactions on
  Pattern Analysis and Machine Intelligence on 2023-07-12</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IntelliGraphs: <span class="highlight-title">Dataset</span>s for Benchmarking Knowledge Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thiviyan Thanapalasingam, Emile van Krieken, Peter Bloem, Paul Groth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Embedding (KGE) models are used to learn continuous
representations of entities and relations. A key task in the literature is
predicting missing links between entities. However, Knowledge Graphs are not
just sets of links but also have semantics underlying their structure.
Semantics is crucial in several downstream tasks, such as query answering or
reasoning. We introduce the subgraph inference task, where a model has to
generate likely and semantically valid subgraphs. We propose IntelliGraphs, a
set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain
subgraphs with semantics expressed in logical rules for evaluating subgraph
inference. We also present the dataset generator that produced the synthetic
datasets. We designed four novel baseline models, which include three models
based on traditional KGEs. We evaluate their expressiveness and show that these
models cannot capture the semantics. We believe this benchmark will encourage
the development of machine learning models that emphasize semantic
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Li, Billy Lau, Xin Yuan, Wei Ni, Mohsen Guizani, Chau Yuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Artificial Intelligence driven mask design for
  <span class="highlight-title">self-supervised</span> seismic denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire Birnie, Matteo Ravasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The presence of coherent noise in seismic data leads to errors and
uncertainties, and as such it is paramount to suppress noise as early and
efficiently as possible. Self-supervised denoising circumvents the common
requirement of deep learning procedures of having noisy-clean training pairs.
However, self-supervised coherent noise suppression methods require extensive
knowledge of the noise statistics. We propose the use of explainable artificial
intelligence approaches to see inside the black box that is the denoising
network and use the gained knowledge to replace the need for any prior
knowledge of the noise itself. This is achieved in practice by leveraging
bias-free networks and the direct linear link between input and output provided
by the associated Jacobian matrix; we show that a simple averaging of the
Jacobian contributions over a number of randomly selected input pixels,
provides an indication of the most effective mask to suppress noise present in
the data. The proposed method therefore becomes a fully automated denoising
procedure requiring no clean training labels or prior knowledge. Realistic
synthetic examples with noise signals of varying complexities, ranging from
simple time-correlated noise to complex pseudo rig noise propagating at the
velocity of the ocean, are used to validate the proposed approach. Its
automated nature is highlighted further by an application to two field
datasets. Without any substantial pre-processing or any knowledge of the
acquisition environment, the automatically identified blind-masks are shown to
perform well in suppressing both trace-wise noise in common shot gathers from
the Volve marine dataset and colored noise in post stack seismic images from a
land seismic survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepIPCv2: LiDAR-powered Robust Environmental Perception and
  Navigational Control for Autonomous Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oskar Natan, Jun Miura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DeepIPCv2, an autonomous driving model that perceives the
environment using a LiDAR sensor for more robust drivability, especially when
driving under poor illumination conditions. DeepIPCv2 takes a set of LiDAR
point clouds for its main perception input. As point clouds are not affected by
illumination changes, they can provide a clear observation of the surroundings
no matter what the condition is. This results in a better scene understanding
and stable features provided by the perception module to support the controller
module in estimating navigational control properly. To evaluate its
performance, we conduct several tests by deploying the model to predict a set
of driving records and perform real automated driving under three different
conditions. We also conduct ablation and comparative studies with some recent
models to justify its performance. Based on the experimental results, DeepIPCv2
shows a robust performance by achieving the best drivability in all conditions.
Codes are available at https://github.com/oskarnatan/DeepIPCv2
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Transformation Sequence Retrieval with General Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrique Mas-Candela, Antonio Ríos-Vila, Jorge Calvo-Zaragoza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, the novel Image Transformation Sequence Retrieval (ITSR) task
is presented, in which a model must retrieve the sequence of transformations
between two given images that act as source and target, respectively. Given
certain characteristics of the challenge such as the multiplicity of a correct
sequence or the correlation between consecutive steps of the process, we
propose a solution to ITSR using a general model-based Reinforcement Learning
such as Monte Carlo Tree Search (MCTS), which is combined with a deep neural
network. Our experiments provide a benchmark in both synthetic and real
domains, where the proposed approach is compared with supervised training. The
results report that a model trained with MCTS is able to outperform its
supervised counterpart in both the simplest and the most complex cases. Our
work draws interesting conclusions about the nature of ITSR and its associated
challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecureFalcon: The Next Cyber Reasoning System for Cyber Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Amine Ferrag, Ammar Battah, Norbert Tihanyi, Merouane Debbah, Thierry Lestable, Lucas C. Cordeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software vulnerabilities leading to various detriments such as crashes, data
loss, and security breaches, significantly hinder the quality, affecting the
market adoption of software applications and systems. Although traditional
methods such as automated software testing, fault localization, and repair have
been intensively studied, static analysis tools are most commonly used and have
an inherent false positives rate, posing a solid challenge to developer
productivity. Large Language Models (LLMs) offer a promising solution to these
persistent issues. Among these, FalconLLM has shown substantial potential in
identifying intricate patterns and complex vulnerabilities, hence crucial in
software vulnerability detection. In this paper, for the first time, FalconLLM
is being fine-tuned for cybersecurity applications, thus introducing
SecureFalcon, an innovative model architecture built upon FalconLLM.
SecureFalcon is trained to differentiate between vulnerable and non-vulnerable
C code samples. We build a new training dataset, FormAI, constructed thanks to
Generative Artificial Intelligence (AI) and formal verification to evaluate its
performance. SecureFalcon achieved an impressive 94% accuracy rate in detecting
software vulnerabilities, emphasizing its significant potential to redefine
software vulnerability detection methods in cybersecurity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Foundation Models as Surrogate Models: Advancing Towards
  More Practical Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhang, Jitao Sang, Qi Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the no-box adversarial attack, in which the attacker lacks access
to the model's architecture, weights, and training data, become the most
practical and challenging attack setup. However, there is an unawareness of the
potential and flexibility inherent in the surrogate model selection process on
no-box setting. Inspired by the burgeoning interest in utilizing foundational
models to address downstream tasks, this paper adopts an innovative idea that
1) recasting adversarial attack as a downstream task. Specifically, image noise
generation to meet the emerging trend and 2) introducing foundational models as
surrogate models. Harnessing the concept of non-robust features, we elaborate
on two guiding principles for surrogate model selection to explain why the
foundational model is an optimal choice for this role. However, paradoxically,
we observe that these foundational models underperform. Analyzing this
unexpected behavior within the feature space, we attribute the lackluster
performance of foundational models (e.g., CLIP) to their significant
representational capacity and, conversely, their lack of discriminative
prowess. To mitigate this issue, we propose the use of a margin-based loss
strategy for the fine-tuning of foundational models on target images. The
experimental results verify that our approach, which employs the basic Fast
Gradient Sign Method (FGSM) attack algorithm, outstrips the performance of
other, more convoluted algorithms. We conclude by advocating for the research
community to consider surrogate models as crucial determinants in the
effectiveness of adversarial attacks in no-box settings. The implications of
our work bear relevance for improving the efficacy of such adversarial attacks
and the overall robustness of AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RVD: A Handheld Device-Based Fundus Video <span class="highlight-title">Dataset</span> for Retinal Vessel
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MD Wahiduzzaman Khan, Hongwei Sheng, Hu Zhang, Heming Du, Sen Wang, Minas Theodore Coroneo, Farshid Hajati, Sahar Shariflou, Michael Kalloniatis, Jack Phu, Ashish Agar, Zi Huang, Mojtaba Golzan, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retinal vessel segmentation is generally grounded in image-based datasets
collected with bench-top devices. The static images naturally lose the dynamic
characteristics of retina fluctuation, resulting in diminished dataset
richness, and the usage of bench-top devices further restricts dataset
scalability due to its limited accessibility. Considering these limitations, we
introduce the first video-based retinal dataset by employing handheld devices
for data acquisition. The dataset comprises 635 smartphone-based fundus videos
collected from four different clinics, involving 415 patients from 50 to 75
years old. It delivers comprehensive and precise annotations of retinal
structures in both spatial and temporal dimensions, aiming to advance the
landscape of vasculature segmentation. Specifically, the dataset provides three
levels of spatial annotations: binary vessel masks for overall retinal
structure delineation, general vein-artery masks for distinguishing the vein
and artery, and fine-grained vein-artery masks for further characterizing the
granularities of each artery and vein. In addition, the dataset offers temporal
annotations that capture the vessel pulsation characteristics, assisting in
detecting ocular diseases that require fine-grained recognition of hemodynamic
fluctuation. In application, our dataset exhibits a significant domain shift
with respect to data captured by bench-top devices, thus posing great
challenges to existing methods. In the experiments, we provide evaluation
metrics and benchmark results on our dataset, reflecting both the potential and
challenges it offers for vessel segmentation tasks. We hope this challenging
dataset would significantly contribute to the development of eye disease
diagnosis and early prevention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regression-Oriented Knowledge Distillation for Lightweight Ship
  Orientation Angle Prediction with Optical Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Shi, Xin Ding, Peng Ding, Chun Yang, Ru Huang, Xiaoxuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ship orientation angle prediction (SOAP) with optical remote sensing images
is an important image processing task, which often relies on deep convolutional
neural networks (CNNs) to make accurate predictions. This paper proposes a
novel framework to reduce the model sizes and computational costs of SOAP
models without harming prediction accuracy. First, a new SOAP model called
Mobile-SOAP is designed based on MobileNetV2, achieving state-of-the-art
prediction accuracy. Four tiny SOAP models are also created by replacing the
convolutional blocks in Mobile-SOAP with four small-scale networks,
respectively. Then, to transfer knowledge from Mobile-SOAP to four lightweight
models, we propose a novel knowledge distillation (KD) framework termed SOAP-KD
consisting of a novel feature-based guidance loss and an optimized synthetic
samples-based knowledge transfer mechanism. Lastly, extensive experiments on
the FGSC-23 dataset confirm the superiority of Mobile-SOAP over existing models
and also demonstrate the effectiveness of SOAP-KD in improving the prediction
performance of four specially designed tiny models. Notably, by using SOAP-KD,
the test mean absolute error of the ShuffleNetV2x1.0-based model is only 8%
higher than that of Mobile-SOAP, but its number of parameters and
multiply-accumulate operations (MACs) are respectively 61.6% and 60.8% less.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prescriptive Process Monitoring Under Resource Constraints: A
  Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Shoush, Marlon Dumas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prescriptive process monitoring methods seek to optimize the performance of
business processes by triggering interventions at runtime, thereby increasing
the probability of positive case outcomes. These interventions are triggered
according to an intervention policy. Reinforcement learning has been put
forward as an approach to learning intervention policies through trial and
error. Existing approaches in this space assume that the number of resources
available to perform interventions in a process is unlimited, an unrealistic
assumption in practice. This paper argues that, in the presence of resource
constraints, a key dilemma in the field of prescriptive process monitoring is
to trigger interventions based not only on predictions of their necessity,
timeliness, or effect but also on the uncertainty of these predictions and the
level of resource utilization. Indeed, committing scarce resources to an
intervention when the necessity or effects of this intervention are highly
uncertain may intuitively lead to suboptimal intervention effects. Accordingly,
the paper proposes a reinforcement learning approach for prescriptive process
monitoring that leverages conformal prediction techniques to consider the
uncertainty of the predictions upon which an intervention decision is based. An
evaluation using real-life datasets demonstrates that explicitly modeling
uncertainty using conformal predictions helps reinforcement learning agents
converge towards policies with higher net intervention gain
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Effective Horizon of Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Xu, Finale Doshi-Velez, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse reinforcement learning (IRL) algorithms often rely on (forward)
reinforcement learning or planning over a given time horizon to compute an
approximately optimal policy for a hypothesized reward function and then match
this policy with expert demonstrations. The time horizon plays a critical role
in determining both the accuracy of reward estimate and the computational
efficiency of IRL algorithms. Interestingly, an effective time horizon shorter
than the ground-truth value often produces better results faster. This work
formally analyzes this phenomenon and provides an explanation: the time horizon
controls the complexity of an induced policy class and mitigates overfitting
with limited data. This analysis leads to a principled choice of the effective
horizon for IRL. It also prompts us to reexamine the classic IRL formulation:
it is more natural to learn jointly the reward and the effective horizon
together rather than the reward alone with a given horizon. Our experimental
results confirm the theoretical analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence for Drug Discovery: Are We There Yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catrin Hasselgren, Tudor I. Oprea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug discovery is adapting to novel technologies such as data science,
informatics, and artificial intelligence (AI) to accelerate effective treatment
development while reducing costs and animal experiments. AI is transforming
drug discovery, as indicated by increasing interest from investors, industrial
and academic scientists, and legislators. Successful drug discovery requires
optimizing properties related to pharmacodynamics, pharmacokinetics, and
clinical outcomes. This review discusses the use of AI in the three pillars of
drug discovery: diseases, targets, and therapeutic modalities, with a focus on
small molecule drugs. AI technologies, such as generative chemistry, machine
learning, and multi-property optimization, have enabled several compounds to
enter clinical trials. The scientific community must carefully vet known
information to address the reproducibility crisis. The full potential of AI in
drug discovery can only be realized with sufficient ground truth and
appropriate human intervention at later pipeline stages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 4 figures, 184 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Contextual Counterfactuals Toward Belief Calibration <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Qiuyi,  Zhang, Michael S. Lee, Sherol Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beliefs and values are increasingly being incorporated into our AI systems
through alignment processes, such as carefully curating data collection
principles or regularizing the loss function used for training. However, the
meta-alignment problem is that these human beliefs are diverse and not aligned
across populations; furthermore, the implicit strength of each belief may not
be well calibrated even among humans, especially when trying to generalize
across contexts. Specifically, in high regret situations, we observe that
contextual counterfactuals and recourse costs are particularly important in
updating a decision maker's beliefs and the strengths to which such beliefs are
held. Therefore, we argue that including counterfactuals is key to an accurate
calibration of beliefs during alignment. To do this, we first segment belief
diversity into two categories: subjectivity (across individuals within a
population) and epistemic uncertainty (within an individual across different
contexts). By leveraging our notion of epistemic uncertainty, we introduce `the
belief calibration cycle' framework to more holistically calibrate this
diversity of beliefs with context-driven counterfactual reasoning by using a
multi-objective optimization. We empirically apply our framework for finding a
Pareto frontier of clustered optimal belief strengths that generalize across
different contexts, demonstrating its efficacy on a toy dataset for credit
decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML (International Conference on Machine Learning) Workshop on
  Counterfactuals in Minds and Machines, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Nonalcoholic Fatty Liver Disease Classification Performance
  With Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Hardy, Cornelia Ilin, Joe Klepich, Ryan Mitchell, Steve Hall, Jericho Villareal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating deep learning with clinical expertise holds great potential for
addressing healthcare challenges and empowering medical professionals with
improved diagnostic tools. However, the need for annotated medical images is
often an obstacle to leveraging the full power of machine learning models. Our
research demonstrates that by combining synthetic images, generated using
diffusion models, with real images, we can enhance nonalcoholic fatty liver
disease (NAFLD) classification performance. We evaluate the quality of the
synthetic images by comparing two metrics: Inception Score (IS) and Fr\'{e}chet
Inception Distance (FID), computed on diffusion-generated images and generative
adversarial networks (GANs)-generated images. Our results show superior
performance for the diffusion-generated images, with a maximum IS score of
$1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ compared
to $99.53$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNet
v1), our synthetic augmentation method achieves a maximum image-level ROC AUC
of $0.904$ on a NAFLD prediction task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Control Policy for Artificial Pancreas via Ensemble Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhou Lv, Tianyu Wu, Luolin Xiong, Liang Wu, Jian Zhou, Yang Tang, Feng Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: The artificial pancreas (AP) has shown promising potential in
achieving closed-loop glucose control for individuals with type 1 diabetes
mellitus (T1DM). However, designing an effective control policy for the AP
remains challenging due to the complex physiological processes, delayed insulin
response, and inaccurate glucose measurements. While model predictive control
(MPC) offers safety and stability through the dynamic model and safety
constraints, it lacks individualization and is adversely affected by
unannounced meals. Conversely, deep reinforcement learning (DRL) provides
personalized and adaptive strategies but faces challenges with distribution
shifts and substantial data requirements. Methods: We propose a hybrid control
policy for the artificial pancreas (HyCPAP) to address the above challenges.
HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the
strengths of both policies while compensating for their respective limitations.
To facilitate faster deployment of AP systems in real-world settings, we
further incorporate meta-learning techniques into HyCPAP, leveraging previous
experience and patient-shared knowledge to enable fast adaptation to new
patients with limited available data. Results: We conduct extensive experiments
using the FDA-accepted UVA/Padova T1DM simulator across three scenarios. Our
approaches achieve the highest percentage of time spent in the desired
euglycemic range and the lowest occurrences of hypoglycemia. Conclusion: The
results clearly demonstrate the superiority of our methods for closed-loop
glucose management in individuals with T1DM. Significance: The study presents
novel control policies for AP systems, affirming the great potential of
proposed methods for efficient closed-loop glucose control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Microbial Genetic Algorithm-based Black-box Attack against Interpretable
  Deep Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are susceptible to adversarial samples in white and
black-box environments. Although previous studies have shown high attack
success rates, coupling DNN models with interpretation models could offer a
sense of security when a human expert is involved, who can identify whether a
given sample is benign or malicious. However, in white-box environments,
interpretable deep learning systems (IDLSes) have been shown to be vulnerable
to malicious manipulations. In black-box settings, as access to the components
of IDLSes is limited, it becomes more challenging for the adversary to fool the
system. In this work, we propose a Query-efficient Score-based black-box attack
against IDLSes, QuScore, which requires no knowledge of the target model and
its coupled interpretation model. QuScore is based on transfer-based and
score-based methods by employing an effective microbial genetic algorithm. Our
method is designed to reduce the number of queries necessary to carry out
successful attacks, resulting in a more efficient process. By continuously
refining the adversarial samples created based on feedback scores from the
IDLS, our approach effectively navigates the search space to identify
perturbations that can fool the system. We evaluate the attack's effectiveness
on four CNN models (Inception, ResNet, VGG, DenseNet) and two interpretation
models (CAM, Grad), using both ImageNet and CIFAR datasets. Our results show
that the proposed approach is query-efficient with a high attack success rate
that can reach between 95% and 100% and transferability with an average success
rate of 69% in the ImageNet and CIFAR datasets. Our attack method generates
adversarial examples with attribution maps that resemble benign samples. We
have also demonstrated that our attack is resilient against various
preprocessing defense techniques and can easily be transferred to different DNN
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Declarative Mechanism Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1912.13122v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1912.13122v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrés García-Camino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulation of Multi-Agent Systems (MAS) and Declarative Electronic
Institutions (DEIs) was a multidisciplinary research topic of the past decade
involving (Physical and Software) Agents and Law since the beginning, but
recently evolved towards News-claimed Robot Lawyer since 2016. One of these
first proposals of restricting the behaviour of Software Agentswas Electronic
Institutions.However, with the recent reformulation of Artificial Neural
Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal
issues regarding the use of DL has raised concerns in the Artificial
Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly
addressed, we propose the Regulation of Artificial Neural Networks as
Agent-based Training of a special type of regulated Artificial Neural Network
that we call Institutional Neural Network (INN).The main purpose of this paper
is to bring attention to Artificial Teaching (AT) and to give a tentative
answer showing a proof-of-concept implementation of Regulated Deep Learning
(RDL). This paper introduces the former concept and provide sI, a language
previously used to model declaratively and extend Electronic Institutions, as a
means to regulate the execution of Artificial Neural Networks and their
interactions with Artificial Teachers (ATs)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Supply Chain Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, Ishai Menache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supply chain operations traditionally involve a variety of complex decision
making problems. Over the last few decades, supply chains greatly benefited
from advances in computation, which allowed the transition from manual
processing to automation and cost-effective optimization. Nonetheless, business
operators still need to spend substantial efforts in explaining and
interpreting the optimization outcomes to stakeholders. Motivated by the recent
advances in Large Language Models (LLMs), we study how this disruptive
technology can help bridge the gap between supply chain automation and human
comprehension and trust thereof. We design OptiGuide -- a framework that
accepts as input queries in plain text, and outputs insights about the
underlying optimization outcomes. Our framework does not forgo the
state-of-the-art combinatorial optimization technology, but rather leverages it
to quantitatively answer what-if scenarios (e.g., how would the cost change if
we used supplier B instead of supplier A for a given demand?). Importantly, our
design does not require sending proprietary data over to LLMs, which can be a
privacy concern in some circumstances. We demonstrate the effectiveness of our
framework on a real server placement scenario within Microsoft's cloud supply
chain. Along the way, we develop a general evaluation benchmark, which can be
used to evaluate the accuracy of the LLM output in other scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Navigation of Underactuated Bipedal Robots in
  Height-Constrained Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.05714v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.05714v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyu Li, Jun Zeng, Shuxiao Chen, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating a large-scaled robot in unknown and cluttered height-constrained
environments is challenging. Not only is a fast and reliable planning algorithm
required to go around obstacles, the robot should also be able to change its
intrinsic dimension by crouching in order to travel underneath
height-constrained regions. There are few mobile robots that are capable of
handling such a challenge, and bipedal robots provide a solution. However, as
bipedal robots have nonlinear and hybrid dynamics, trajectory planning while
ensuring dynamic feasibility and safety on these robots is challenging. This
paper presents an end-to-end autonomous navigation framework which leverages
three layers of planners and a variable walking height controller to enable
bipedal robots to safely explore height-constrained environments. A
vertically-actuated Spring-Loaded Inverted Pendulum (vSLIP) model is introduced
to capture the robot's coupled dynamics of planar walking and vertical walking
height. This reduced-order model is utilized to optimize for long-term and
short-term safe trajectory plans. A variable walking height controller is
leveraged to enable the bipedal robot to maintain stable periodic walking gaits
while following the planned trajectory. The entire framework is tested and
experimentally validated using a bipedal robot Cassie. This demonstrates
reliable autonomy to drive the robot to safely avoid obstacles while walking to
the goal location in various kinds of height-constrained cluttered
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in International Journal of Robotics Research (IJRR) 2023.
  This is the author's version and will no longer be updated as the copyright
  may get transferred at anytime</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology
  Reporting <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Matthias Keicher, Ege Özsoy, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. We will make all annotations and
our code for annotation generation, model evaluation, and training publicly
available upon acceptance. Our dataset and code is available at
https://github.com/ChantalMP/Rad-ReStruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Formal Controller Synthesis for Markov Jump Linear Systems with
  Uncertain Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00679v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00679v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Rickard, Thom Badings, Licio Romao, Alessandro Abate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated synthesis of provably correct controllers for cyber-physical
systems is crucial for deployment in safety-critical scenarios. However, hybrid
features and stochastic or unknown behaviours make this problem challenging. We
propose a method for synthesising controllers for Markov jump linear systems
(MJLSs), a class of discrete-time models for cyber-physical systems, so that
they certifiably satisfy probabilistic computation tree logic (PCTL) formulae.
An MJLS consists of a finite set of stochastic linear dynamics and discrete
jumps between these dynamics that are governed by a Markov decision process
(MDP). We consider the cases where the transition probabilities of this MDP are
either known up to an interval or completely unknown. Our approach is based on
a finite-state abstraction that captures both the discrete (mode-jumping) and
continuous (stochastic linear) behaviour of the MJLS. We formalise this
abstraction as an interval MDP (iMDP) for which we compute intervals of
transition probabilities using sampling techniques from the so-called 'scenario
approach', resulting in a probabilistically sound approximation. We apply our
method to multiple realistic benchmark problems, in particular, a temperature
control and an aerial vehicle delivery problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, accepted to QEST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reading Radiology Imaging Like The Radiologist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are data writing errors in the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep
  Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Sharifi, Mustafa Yildirim, Saber Fallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamic nature of driving environments and the presence of diverse road
users pose significant challenges for decision-making in autonomous driving.
Deep reinforcement learning (DRL) has emerged as a popular approach to tackle
this problem. However, the application of existing DRL solutions is mainly
confined to simulated environments due to safety concerns, impeding their
deployment in real-world. To overcome this limitation, this paper introduces a
novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics
(DRLSL) that combines the strengths of DRL (learning from experience) and
symbolic first-order logics (knowledge-driven reasoning) to enable safe
learning in real-time interactions of autonomous driving within real
environments. This innovative approach provides a means to learn autonomous
driving policies by actively engaging with the physical environment while
ensuring safety. We have implemented the DRLSL framework in autonomous driving
using the highD dataset and demonstrated that our method successfully avoids
unsafe actions during both the training and testing phases. Furthermore, our
results indicate that DRLSL achieves faster convergence during training and
exhibits better generalizability to new driving scenarios compared to
traditional DRL methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, 1 table, 1 algorithm. Under review as a journal
  paper at IEEE transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification and Generation of real-world data with an Associative
  Memory Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04827v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04827v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Simas, Luis Sa-Couto, Andreas Wichert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drawing from memory the face of a friend you have not seen in years is a
difficult task. However, if you happen to cross paths, you would easily
recognize each other. The biological memory is equipped with an impressive
compression algorithm that can store the essential, and then infer the details
to match perception. The Willshaw Memory is a simple abstract model for
cortical computations which implements mechanisms of biological memories. Using
our recently proposed sparse coding prescription for visual patterns, this
model can store and retrieve an impressive amount of real-world data in a
fault-tolerant manner. In this paper, we extend the capabilities of the basic
Associative Memory Model by using a Multiple-Modality framework. In this
setting, the memory stores several modalities (e.g., visual, or textual) of
each pattern simultaneously. After training, the memory can be used to infer
missing modalities when just a subset is perceived. Using a simple
encoder-memory-decoder architecture, and a newly proposed iterative retrieval
algorithm for the Willshaw Model, we perform experiments on the MNIST dataset.
By storing both the images and labels as modalities, a single Memory can be
used not only to retrieve and complete patterns but also to classify and
generate new ones. We further discuss how this model could be used for other
learning tasks, thus serving as a biologically-inspired framework for learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures. Neurocomputing, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Discrete Soft Actor-Critic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibin Zhou, Zichuan Lin, Junyou Li, Qiang Fu, Wei Yang, Deheng Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the adaption of soft actor-critic (SAC) from continuous action space
to discrete action space. We revisit vanilla SAC and provide an in-depth
understanding of its Q value underestimation and performance instability issues
when applied to discrete settings. We thereby propose entropy-penalty and
double average Q-learning with Q-clip to address these issues. Extensive
experiments on typical benchmarks with discrete action space, including Atari
games and a large-scale MOBA game, show the efficacy of our proposed method.
Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pattern<span class="highlight-title">GPT</span> :A Pattern-Driven Framework for Large Language Model Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00470v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00470v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Xiao, Xin Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models(LLMS) have shown excellent text generation
capabilities,capable of generating fluent responses for many downstream tasks.
However,applying large language models to real-world critical tasks remains
challenging due to their susceptibility to hallucinations and inability to
directly use external knowledge. To address the above challenges,this paper
proposes PatternGPT, a pattern-driven text generation framework for large
language models. First,the framework utilizes the extraction capabilities of
large language models to generate rich and diverse patterns and later draws on
the idea of federated learning. Using multiple agents to achieve sharing to
obtain more diverse patterns. Finally, it searches for high-quality patterns
using judgment criteria and optimization algorithms and uses the searched
patterns to guide the model for generation. This framework has the advantages
of generating diversified patterns, protecting data privacy,combining external
knowledge, and improving the quality of generation, which provides an effective
method to optimize the text generation capability of large language models,and
make it better applied to the field of intelligent dialogue and content
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03109v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03109v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages; more work is at: https://llm-eval.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prospective Learning: Principled Extrapolation to the Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin De Silva, Rahul Ramesh, Lyle Ungar, Marshall Hussain Shuler, Noah J. Cowan, Michael Platt, Chen Li, Leyla Isik, Seung-Eon Roh, Adam Charles, Archana Venkataraman, Brian Caffo, Javier J. How, Justus M Kebschull, John W. Krakauer, Maxim Bichuch, Kaleab Alemayehu Kinfu, Eva Yezerets, Dinesh Jayaraman, Jong M. Shin, Soledad Villar, Ian Phillips, Carey E. Priebe, Thomas Hartung, Michael I. Miller, Jayanta Dey,  Ningyuan,  Huang, Eric Eaton, Ralph Etienne-Cummings, Elizabeth L. Ogburn, Randal Burns, Onyema Osuagwu, Brett Mensh, Alysson R. Muotri, Julia Brown, Chris White, Weiwei Yang, Andrei A. Rusu, Timothy Verstynen, Konrad P. Kording, Pratik Chaudhari, Joshua T. Vogelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning is a process which can update decision rules, based on past
experience, such that future performance improves. Traditionally, machine
learning is often evaluated under the assumption that the future will be
identical to the past in distribution or change adversarially. But these
assumptions can be either too optimistic or pessimistic for many problems in
the real world. Real world scenarios evolve over multiple spatiotemporal scales
with partially predictable dynamics. Here we reformulate the learning problem
to one that centers around this idea of dynamic futures that are partially
learnable. We conjecture that certain sequences of tasks are not
retrospectively learnable (in which the data distribution is fixed), but are
prospectively learnable (in which distributions may be dynamic), suggesting
that prospective learning is more difficult in kind than retrospective
learning. We argue that prospective learning more accurately characterizes many
real world problems that (1) currently stymie existing artificial intelligence
solutions and/or (2) lack adequate explanations for how natural intelligences
solve them. Thus, studying prospective learning will lead to deeper insights
and solutions to currently vexing challenges in both natural and artificial
intelligences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2nd Conference on Lifelong Learning Agents (CoLLAs),
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block shuffling learning for Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitong Liu, Zhichao Lian, Siqi Gu, Liang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake detection methods based on convolutional neural networks (CNN) have
demonstrated high accuracy. \textcolor{black}{However, these methods often
suffer from decreased performance when faced with unknown forgery methods and
common transformations such as resizing and blurring, resulting in deviations
between training and testing domains.} This phenomenon, known as overfitting,
poses a significant challenge. To address this issue, we propose a novel block
shuffling regularization method. Firstly, our approach involves dividing the
images into blocks and applying both intra-block and inter-block shuffling
techniques. This process indirectly achieves weight-sharing across different
dimensions. Secondly, we introduce an adversarial loss algorithm to mitigate
the overfitting problem induced by the shuffling noise. Finally, we restore the
spatial layout of the blocks to capture the semantic associations among them.
Extensive experiments validate the effectiveness of our proposed method, which
surpasses existing approaches in forgery face detection. Notably, our method
exhibits excellent generalization capabilities, demonstrating robustness
against cross-dataset evaluations and common image transformations. Especially
our method can be easily integrated with various CNN models. Source code is
available at
\href{https://github.com/NoWindButRain/BlockShuffleLearning}{Github}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Most Equitable Voting Rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lirong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In social choice theory, anonymity (all agents being treated equally) and
neutrality (all alternatives being treated equally) are widely regarded as
``minimal demands'' and ``uncontroversial'' axioms of equity and fairness.
However, the ANR impossibility -- there is no voting rule that satisfies
anonymity, neutrality, and resolvability (always choosing one winner) -- holds
even in the simple setting of two alternatives and two agents. How to design
voting rules that optimally satisfy anonymity, neutrality, and resolvability
remains an open question.
  We address the optimal design question for a wide range of preferences and
decisions that include ranked lists and committees. Our conceptual contribution
is a novel and strong notion of most equitable refinements that optimally
preserves anonymity and neutrality for any irresolute rule that satisfies the
two axioms. Our technical contributions are twofold. First, we characterize the
conditions for the ANR impossibility to hold under general settings, especially
when the number of agents is large. Second, we propose the
most-favorable-permutation (MFP) tie-breaking to compute a most equitable
refinement and design a polynomial-time algorithm to compute MFP when agents'
preferences are full rankings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on <span class="highlight-title">Transformer</span>s in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, Deheng Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has been considered the dominating neural architecture in NLP and
CV, mostly under supervised settings. Recently, a similar surge of using
Transformers has appeared in the domain of reinforcement learning (RL), but it
is faced with unique design choices and challenges brought by the nature of RL.
However, the evolution of Transformers in RL has not yet been well unraveled.
In this paper, we seek to systematically review motivations and progress on
using Transformers in RL, provide a taxonomy on existing works, discuss each
sub-field, and summarize future prospects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Control <span class="highlight-title">Transformer</span>: Robot Navigation in Unknown Environments through
  PRM-Guided Return-Conditioned Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06407v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06407v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Lawson, Ahmed H. Qureshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning long-horizon tasks such as navigation has presented difficult
challenges for successfully applying reinforcement learning to robotics. From
another perspective, under known environments, sampling-based planning can
robustly find collision-free paths in environments without learning. In this
work, we propose Control Transformer that models return-conditioned sequences
from low-level policies guided by a sampling-based Probabilistic Roadmap (PRM)
planner. We demonstrate that our framework can solve long-horizon navigation
tasks using only local information. We evaluate our approach on
partially-observed maze navigation with MuJoCo robots, including Ant, Point,
and Humanoid. We show that Control Transformer can successfully navigate
through mazes and transfer to unknown environments. Additionally, we apply our
method to a differential drive robot (Turtlebot3) and show zero-shot sim2real
transfer under noisy observations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrustGuard: GNN-based Robust and Explainable Trust Evaluation with
  Dynamicity Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wang, Zheng Yan, Jiahe Lan, Elisa Bertino, Witold Pedrycz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trust evaluation assesses trust relationships between entities and
facilitates decision-making. Machine Learning (ML) shows great potential for
trust evaluation owing to its learning capabilities. In recent years, Graph
Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in
dealing with graph data. This has motivated researchers to explore their use in
trust evaluation, as trust relationships among entities can be modeled as a
graph. However, current trust evaluation methods that employ GNNs fail to fully
satisfy the dynamic nature of trust, overlook the adverse effects of attacks on
trust evaluation, and cannot provide convincing explanations on evaluation
results. To address these problems, we propose TrustGuard, a GNN-based accurate
trust evaluation model that supports trust dynamicity, is robust against
typical attacks, and provides explanations through visualization. Specifically,
TrustGuard is designed with a layered architecture that contains a snapshot
input layer, a spatial aggregation layer, a temporal aggregation layer, and a
prediction layer. Among them, the spatial aggregation layer adopts a defense
mechanism to robustly aggregate local trust, and the temporal aggregation layer
applies an attention mechanism for effective learning of temporal patterns.
Extensive experiments on two real-world datasets show that TrustGuard
outperforms state-of-the-art GNN-based trust evaluation models with respect to
trust prediction across single-timeslot and multi-timeslot, even in the
presence of attacks. In addition, TrustGuard can explain its evaluation results
by visualizing both spatial and temporal views.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Policies Beat Superhuman Go AIs <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00241v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00241v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony T. Wang, Adam Gleave, Tom Tseng, Kellin Pelrine, Nora Belrose, Joseph Miller, Michael D. Dennis, Yawen Duan, Viktor Pogrebniak, Sergey Levine, Stuart Russell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We attack the state-of-the-art Go-playing AI system KataGo by training
adversarial policies against it, achieving a >97% win rate against KataGo
running at superhuman settings. Our adversaries do not win by playing Go well.
Instead, they trick KataGo into making serious blunders. Our attack transfers
zero-shot to other superhuman Go-playing AIs, and is comprehensible to the
extent that human experts can implement it without algorithmic assistance to
consistently beat superhuman AIs. The core vulnerability uncovered by our
attack persists even in KataGo agents adversarially trained to defend against
our attack. Our results demonstrate that even superhuman AI systems may harbor
surprising failure modes. Example games are available https://goattack.far.ai/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2023, see paper for changelog</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojuan Tang, Song-Chun Zhu, Yitao Liang, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph (KG) reasoning is an important problem for knowledge graphs.
In this paper, we propose a novel and principled framework called \textbf{RulE}
(stands for {Rul}e {E}mbedding) to effectively leverage logical rules to
enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE
learns rule embeddings from existing triplets and first-order {rules} by
jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical
rules} in a unified embedding space. Based on the learned rule embeddings, a
confidence score can be calculated for each rule, reflecting its consistency
with the observed triplets. This allows us to perform logical rule inference in
a soft way, thus alleviating the brittleness of logic. On the other hand, RulE
injects prior logical rule information into the embedding space, enriching and
regularizing the entity/relation embeddings. This makes KGE alone perform
better too. RulE is conceptually simple and empirically effective. We conduct
extensive experiments to verify each component of RulE. Results on multiple
benchmarks reveal that our model outperforms the majority of existing
embedding-based and rule-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent Neural Network Mechanisms for Generalization to Objects in
  Novel Orientations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.13445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.13445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avi Cooper, Xavier Boix, Daniel Harari, Spandan Madan, Hanspeter Pfister, Tomotake Sasaki, Pawan Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability of Deep Neural Networks (DNNs) to recognize objects in
orientations outside the distribution of the training data is not well
understood. We present evidence that DNNs are capable of generalizing to
objects in novel orientations by disseminating orientation-invariance obtained
from familiar objects seen from many viewpoints. This capability strengthens
when training the DNN with an increasing number of familiar objects, but only
in orientations that involve 2D rotations of familiar orientations. We show
that this dissemination is achieved via neurons tuned to common features
between familiar and unfamiliar objects. These results implicate brain-like
neural mechanisms for generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision
  for Unsupervised Anomaly Detection is Creating the Illusion of Success 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07734v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07734v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Yoo, Tiancheng Zhao, Leman Akoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as a promising alternative to
create supervisory signals to real-world problems, avoiding the extensive cost
of manual labeling. SSL is particularly attractive for unsupervised tasks such
as anomaly detection (AD), where labeled anomalies are rare or often
nonexistent. A large catalog of augmentation functions has been used for
SSL-based AD (SSAD) on image data, and recent works have reported that the type
of augmentation has a significant impact on accuracy. Motivated by those, this
work sets out to put image-based SSAD under a larger lens and investigate the
role of data augmentation in SSAD. Through extensive experiments on 3 different
detector models and across 420 AD tasks, we provide comprehensive numerical and
visual evidences that the alignment between data augmentation and
anomaly-generating mechanism is the key to the success of SSAD, and in the lack
thereof, SSL may even impair accuracy. To the best of our knowledge, this is
the first meta-analysis on the role of data augmentation in SSAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A multilevel framework for AI governance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyesun Choung, Prabu David, John S. Seberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To realize the potential benefits and mitigate potential risks of AI, it is
necessary to develop a framework of governance that conforms to ethics and
fundamental human values. Although several organizations have issued guidelines
and ethical frameworks for trustworthy AI, without a mediating governance
structure, these ethical principles will not translate into practice. In this
paper, we propose a multilevel governance approach that involves three groups
of interdependent stakeholders: governments, corporations, and citizens. We
examine their interrelationships through dimensions of trust, such as
competence, integrity, and benevolence. The levels of governance combined with
the dimensions of trust in AI provide practical insights that can be used to
further enhance user experiences and inform public policy related to AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication and is forthcoming in
  The Global and Digital Governance Handbook. Cite as: Choung, H., David, P., &
  Seberger, J.S. (2023). A multilevel framework for AI governance. The Global
  and Digital Governance Handbook. Routledge, Taylor & Francis Group</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-12T00:00:00Z">2023-07-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Embeddings from Large-Scale Acoustic Bird Classifiers Enable
  Few-Shot Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burooj Ghani, Tom Denton, Stefan Kahl, Holger Klinck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated bioacoustic analysis aids understanding and protection of both
marine and terrestrial animals and their habitats across extensive
spatiotemporal scales, and typically involves analyzing vast collections of
acoustic data. With the advent of deep learning models, classification of
important signals from these datasets has markedly improved. These models power
critical data analyses for research and decision-making in biodiversity
monitoring, animal behaviour studies, and natural resource management. However,
deep learning models are often data-hungry and require a significant amount of
labeled training data to perform well. While sufficient training data is
available for certain taxonomic groups (e.g., common bird species), many
classes (such as rare and endangered species, many non-bird taxa, and
call-type), lack enough data to train a robust model from scratch. This study
investigates the utility of feature embeddings extracted from large-scale audio
classification models to identify bioacoustic classes other than the ones these
models were originally trained on. We evaluate models on diverse datasets,
including different bird calls and dialect types, bat calls, marine mammals
calls, and amphibians calls. The embeddings extracted from the models trained
on bird vocalization data consistently allowed higher quality classification
than the embeddings trained on general audio datasets. The results of this
study indicate that high-quality feature embeddings from large-scale acoustic
bird classifiers can be harnessed for few-shot transfer learning, enabling the
learning of new classes from a limited quantity of training data. Our findings
reveal the potential for efficient analyses of novel bioacoustic tasks, even in
scenarios where available training data is limited to a few samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ B-CLEAN-SC: CLEAN-SC for broadband sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Goudarzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents B-CLEAN-SC, a variation of CLEAN-SC for broadband
sources. Opposed to CLEAN-SC, which ``deconvolves'' the beamforming map for
each frequency individually, B-CLEAN-SC processes frequency intervals. Instead
of performing a deconvolution iteration at the location of the maximum level,
B-CLEAN-SC performs it at the location of the over-frequency-averaged maximum
to improve the location estimation. The method is validated and compared to
standard CLEAN-SC on synthetic cases, and real-world experiments, for broad-
and narrowband sources. It improves the source reconstruction at low and high
frequencies and suppresses noise, while it only increases the need for memory
but not computational effort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Aid in Annotating Speech Emotional Data?
  Uncovering New Frontiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddique Latif, Muhammad Usama, Mohammad Ibrahim Malik, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in speech emotion recognition (SER) models,
state-of-the-art deep learning (DL) approaches face the challenge of the
limited availability of annotated data. Large language models (LLMs) have
revolutionised our understanding of natural language, introducing emergent
properties that broaden comprehension in language, speech, and vision. This
paper examines the potential of LLMs to annotate abundant speech data, aiming
to enhance the state-of-the-art in SER. We evaluate this capability across
various settings using publicly available speech emotion classification
datasets. Leveraging ChatGPT, we experimentally demonstrate the promising role
of LLMs in speech emotion data annotation. Our evaluation encompasses
single-shot and few-shots scenarios, revealing performance variability in SER.
Notably, we achieve improved results through data augmentation, incorporating
ChatGPT-annotated samples into existing datasets. Our work uncovers new
frontiers in speech emotion classification, highlighting the increasing
significance of LLMs in this field moving forward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rhythm Modeling for Voice Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin van Niekerk, Marc-André Carbonneau, Herman Kamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice conversion aims to transform source speech into a different target
voice. However, typical voice conversion systems do not account for rhythm,
which is an important factor in the perception of speaker identity. To bridge
this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion
that does not require parallel data or text transcriptions. Using
self-supervised representations, we first divide source audio into segments
approximating sonorants, obstruents, and silences. Then we model rhythm by
estimating speaking rate or the duration distribution of each segment type.
Finally, we match the target speaking rate or rhythm by time-stretching the
speech segments. Experiments show that Urhythmic outperforms existing
unsupervised methods in terms of quality and prosody. Code and checkpoints:
https://github.com/bshall/urhythmic. Audio demo page:
https://ubisoft-laforge.github.io/speech/urhythmic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 4 tables, submitted to IEEE Signal Processing
  Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Routing Mixture of Experts for Multilingual and Code-Switching
  Speech Recognition <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Guodong Ma, Yuke Li, Binbin Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual speech recognition for both monolingual and code-switching
speech is a challenging task. Recently, based on the Mixture of Experts (MoE),
many works have made good progress in multilingual and code-switching ASR, but
present huge computational complexity with the increase of supported languages.
In this work, we propose a computation-efficient network named Language-Routing
Mixture of Experts (LR-MoE) for multilingual and code-switching ASR. LR-MoE
extracts language-specific representations through the Mixture of Language
Experts (MLE), which is guided to learn by a frame-wise language routing
mechanism. The weight-shared frame-level language identification (LID) network
is jointly trained as the shared pre-router of each MoE layer. Experiments show
that the proposed method significantly improves multilingual and code-switching
speech recognition performances over baseline with comparable computational
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proc. INTERSPEECH 2023, August 20-24, 2023, Dublin,
  Ireland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalyan Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Event Localization (AVEL) is the task of temporally localizing
and classifying \emph{audio-visual events}, i.e., events simultaneously visible
and audible in a video. In this paper, we solve AVEL in a weakly-supervised
setting, where only video-level event labels (their presence/absence, but not
their locations in time) are available as supervision for training. Our idea is
to use a base model to estimate labels on the training data at a finer temporal
resolution than at the video level and re-train the model with these labels.
I.e., we determine the subset of labels for each \emph{slice} of frames in a
training video by (i) replacing the frames outside the slice with those from a
second video having no overlap in video-level labels, and (ii) feeding this
synthetic video into the base model to extract labels for just the slice in
question. To handle the out-of-distribution nature of our synthetic videos, we
propose an auxiliary objective for the base model that induces more reliable
predictions of the localized event labels as desired. Our three-stage pipeline
outperforms several existing AVEL methods with no architectural changes and
improves performance on a related weakly-supervised task as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextualized End-to-End Speech Recognition with Contextual Phrase
  Prediction Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12493v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12493v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixun Huang, Ao Zhang, Zhanheng Yang, Pengcheng Guo, Bingshen Mu, Tianyi Xu, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual information plays a crucial role in speech recognition
technologies and incorporating it into the end-to-end speech recognition models
has drawn immense interest recently. However, previous deep bias methods lacked
explicit supervision for bias tasks. In this study, we introduce a contextual
phrase prediction network for an attention-based deep bias method. This network
predicts context phrases in utterances using contextual embeddings and
calculates bias loss to assist in the training of the contextualized model. Our
method achieved a significant word error rate (WER) reduction across various
end-to-end speech recognition models. Experiments on the LibriSpeech corpus
show that our proposed model obtains a 12.1% relative WER improvement over the
baseline model, and the WER of the context phrases decreases relatively by
40.5%. Moreover, by applying a context phrase filtering strategy, we also
effectively eliminate the WER degradation when using a larger biasing list.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by interspeech2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VampNet: Music Generation via Masked Acoustic Token Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VampNet, a masked acoustic token modeling approach to music
synthesis, compression, inpainting, and variation. We use a variable masking
schedule during training which allows us to sample coherent music from the
model by applying a variety of masking approaches (called prompts) during
inference. VampNet is non-autoregressive, leveraging a bidirectional
transformer architecture that attends to all tokens in a forward pass. With
just 36 sampling passes, VampNet can generate coherent high-fidelity musical
waveforms. We show that by prompting VampNet in various ways, we can apply it
to tasks like music compression, inpainting, outpainting, continuation, and
looping with variation (vamping). Appropriately prompted, VampNet is capable of
maintaining style, genre, instrumentation, and other high-level aspects of the
music. This flexible prompting capability makes VampNet a powerful music
co-creation tool. Code and audio samples are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARBLE: Music Audio Representation Benchmark for Universal Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of extensive intersection between art and Artificial Intelligence
(AI), such as image generation and fiction co-creation, AI for music remains
relatively nascent, particularly in music understanding. This is evident in the
limited work on deep music representations, the scarcity of large-scale
datasets, and the absence of a universal and community-driven benchmark. To
address this issue, we introduce the Music Audio Representation Benchmark for
universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various
Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy
with four hierarchy levels, including acoustic, performance, score, and
high-level description. We then establish a unified protocol based on 14 tasks
on 8 public-available datasets, providing a fair and standard assessment of
representations of all open-sourced pre-trained models developed on music
recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and
reproducible suite for the community, with a clear statement on copyright
issues on datasets. Results suggest recently proposed large-scale pre-trained
musical language models perform the best in most tasks, with room for further
improvement. The leaderboard and toolkit repository are published at
https://marble-bm.shef.ac.uk to promote future music AI research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMTNet: Hierarchical cavitation intensity recognition based on sub-main
  transfer network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01429v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01429v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Sha, Johannes Faber, Shuiping Gou, Bo Liu, Wei Li, Stefan Schramm, Horst Stoecker, Thomas Steckenreiter, Domagoj Vnucec, Nadine Wetzstein, Andreas Widl, Kai Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of smart manufacturing, data-driven machinery
health management has been of growing attention. In situations where some
classes are more difficult to be distinguished compared to others and where
classes might be organised in a hierarchy of categories, current DL methods can
not work well. In this study, a novel hierarchical cavitation intensity
recognition framework using Sub-Main Transfer Network, termed SMTNet, is
proposed to classify acoustic signals of valve cavitation. SMTNet model outputs
multiple predictions ordered from coarse to fine along a network corresponding
to a hierarchy of target cavitation states. Firstly, a data augmentation method
based on Sliding Window with Fast Fourier Transform (Swin-FFT) is developed to
solve few-shot problem. Secondly, a 1-D double hierarchical residual block (1-D
DHRB) is presented to capture sensitive features of the frequency domain valve
acoustic signals. Thirdly, hierarchical multi-label tree is proposed to assist
the embedding of the semantic structure of target cavitation states into
SMTNet. Fourthly, experience filtering mechanism is proposed to fully learn a
prior knowledge of cavitation detection model. Finally, SMTNet has been
evaluated on two cavitation datasets without noise (Dataset 1 and Dataset 2),
and one cavitation dataset with real noise (Dataset 3) provided by SAMSON AG
(Frankfurt). The prediction accurcies of SMTNet for cavitation intensity
recognition are as high as 95.32%, 97.16% and 100%, respectively. At the same
time, the testing accuracies of SMTNet for cavitation detection are as high as
97.02%, 97.64% and 100%. In addition, SMTNet has also been tested for different
frequencies of samples and has achieved excellent results of the highest
frequency of samples of mobile phones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>we need update this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpeechBlender: Speech Augmentation Framework for Mispronunciation Data
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali, Hamdy Mubarak, Shazia Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of labeled second language (L2) speech data is a major challenge in
designing mispronunciation detection models. We introduce SpeechBlender - a
fine-grained data augmentation pipeline for generating mispronunciation errors
to overcome such data scarcity. The SpeechBlender utilizes varieties of masks
to target different regions of phonetic units, and use the mixing factors to
linearly interpolate raw speech signals while augmenting pronunciation. The
masks facilitate smooth blending of the signals, generating more effective
samples than the `Cut/Paste' method. Our proposed technique achieves
state-of-the-art results, with Speechocean762, on ASR dependent
mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson
Correlation Coefficient (PCC) compared to the previous state-of-the-art [1].
Additionally, we demonstrate a 5.0% improvement at the phoneme level compared
to our baseline. We also observed a 4.6% increase in F1-score with Arabic
AraVoiceL2 testset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Roman Numeral Analysis with Graph Neural Networks: Onset-wise
  Predictions from Note-wise Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil Karystinaios, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Roman Numeral analysis is the important task of identifying chords and their
functional context in pieces of tonal music. This paper presents a new approach
to automatic Roman Numeral analysis in symbolic music. While existing
techniques rely on an intermediate lossy representation of the score, we
propose a new method based on Graph Neural Networks (GNNs) that enable the
direct description and processing of each individual note in the score. The
proposed architecture can leverage notewise features and interdependencies
between notes but yield onset-wise representation by virtue of our novel edge
contraction algorithm. Our results demonstrate that ChordGNN outperforms
existing state-of-the-art models, achieving higher accuracy in Roman Numeral
analysis on the reference datasets. In addition, we investigate variants of our
model using proposed techniques such as NADE, and post-processing of the chord
predictions. The full source code for this work is available at
https://github.com/manoskary/chordgnn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 24th Conference of the International Society
  for Music Information Retrieval (ISMIR 2023), Milan, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KIT's Multilingual Speech Translation System for IWSLT 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which evaluates translation quality
on scientific conference talks. The test condition features accented input
speech and terminology-dense contents. The task requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Embeddings from Large-Scale Acoustic Bird Classifiers Enable
  Few-Shot Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burooj Ghani, Tom Denton, Stefan Kahl, Holger Klinck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated bioacoustic analysis aids understanding and protection of both
marine and terrestrial animals and their habitats across extensive
spatiotemporal scales, and typically involves analyzing vast collections of
acoustic data. With the advent of deep learning models, classification of
important signals from these datasets has markedly improved. These models power
critical data analyses for research and decision-making in biodiversity
monitoring, animal behaviour studies, and natural resource management. However,
deep learning models are often data-hungry and require a significant amount of
labeled training data to perform well. While sufficient training data is
available for certain taxonomic groups (e.g., common bird species), many
classes (such as rare and endangered species, many non-bird taxa, and
call-type), lack enough data to train a robust model from scratch. This study
investigates the utility of feature embeddings extracted from large-scale audio
classification models to identify bioacoustic classes other than the ones these
models were originally trained on. We evaluate models on diverse datasets,
including different bird calls and dialect types, bat calls, marine mammals
calls, and amphibians calls. The embeddings extracted from the models trained
on bird vocalization data consistently allowed higher quality classification
than the embeddings trained on general audio datasets. The results of this
study indicate that high-quality feature embeddings from large-scale acoustic
bird classifiers can be harnessed for few-shot transfer learning, enabling the
learning of new classes from a limited quantity of training data. Our findings
reveal the potential for efficient analyses of novel bioacoustic tasks, even in
scenarios where available training data is limited to a few samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ B-CLEAN-SC: CLEAN-SC for broadband sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Goudarzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents B-CLEAN-SC, a variation of CLEAN-SC for broadband
sources. Opposed to CLEAN-SC, which ``deconvolves'' the beamforming map for
each frequency individually, B-CLEAN-SC processes frequency intervals. Instead
of performing a deconvolution iteration at the location of the maximum level,
B-CLEAN-SC performs it at the location of the over-frequency-averaged maximum
to improve the location estimation. The method is validated and compared to
standard CLEAN-SC on synthetic cases, and real-world experiments, for broad-
and narrowband sources. It improves the source reconstruction at low and high
frequencies and suppresses noise, while it only increases the need for memory
but not computational effort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Aid in Annotating Speech Emotional Data?
  Uncovering New Frontiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddique Latif, Muhammad Usama, Mohammad Ibrahim Malik, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in speech emotion recognition (SER) models,
state-of-the-art deep learning (DL) approaches face the challenge of the
limited availability of annotated data. Large language models (LLMs) have
revolutionised our understanding of natural language, introducing emergent
properties that broaden comprehension in language, speech, and vision. This
paper examines the potential of LLMs to annotate abundant speech data, aiming
to enhance the state-of-the-art in SER. We evaluate this capability across
various settings using publicly available speech emotion classification
datasets. Leveraging ChatGPT, we experimentally demonstrate the promising role
of LLMs in speech emotion data annotation. Our evaluation encompasses
single-shot and few-shots scenarios, revealing performance variability in SER.
Notably, we achieve improved results through data augmentation, incorporating
ChatGPT-annotated samples into existing datasets. Our work uncovers new
frontiers in speech emotion classification, highlighting the increasing
significance of LLMs in this field moving forward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rhythm Modeling for Voice Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin van Niekerk, Marc-André Carbonneau, Herman Kamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice conversion aims to transform source speech into a different target
voice. However, typical voice conversion systems do not account for rhythm,
which is an important factor in the perception of speaker identity. To bridge
this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion
that does not require parallel data or text transcriptions. Using
self-supervised representations, we first divide source audio into segments
approximating sonorants, obstruents, and silences. Then we model rhythm by
estimating speaking rate or the duration distribution of each segment type.
Finally, we match the target speaking rate or rhythm by time-stretching the
speech segments. Experiments show that Urhythmic outperforms existing
unsupervised methods in terms of quality and prosody. Code and checkpoints:
https://github.com/bshall/urhythmic. Audio demo page:
https://ubisoft-laforge.github.io/speech/urhythmic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 4 tables, submitted to IEEE Signal Processing
  Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Routing Mixture of Experts for Multilingual and Code-Switching
  Speech Recognition <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Guodong Ma, Yuke Li, Binbin Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual speech recognition for both monolingual and code-switching
speech is a challenging task. Recently, based on the Mixture of Experts (MoE),
many works have made good progress in multilingual and code-switching ASR, but
present huge computational complexity with the increase of supported languages.
In this work, we propose a computation-efficient network named Language-Routing
Mixture of Experts (LR-MoE) for multilingual and code-switching ASR. LR-MoE
extracts language-specific representations through the Mixture of Language
Experts (MLE), which is guided to learn by a frame-wise language routing
mechanism. The weight-shared frame-level language identification (LID) network
is jointly trained as the shared pre-router of each MoE layer. Experiments show
that the proposed method significantly improves multilingual and code-switching
speech recognition performances over baseline with comparable computational
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proc. INTERSPEECH 2023, August 20-24, 2023, Dublin,
  Ireland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalyan Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Event Localization (AVEL) is the task of temporally localizing
and classifying \emph{audio-visual events}, i.e., events simultaneously visible
and audible in a video. In this paper, we solve AVEL in a weakly-supervised
setting, where only video-level event labels (their presence/absence, but not
their locations in time) are available as supervision for training. Our idea is
to use a base model to estimate labels on the training data at a finer temporal
resolution than at the video level and re-train the model with these labels.
I.e., we determine the subset of labels for each \emph{slice} of frames in a
training video by (i) replacing the frames outside the slice with those from a
second video having no overlap in video-level labels, and (ii) feeding this
synthetic video into the base model to extract labels for just the slice in
question. To handle the out-of-distribution nature of our synthetic videos, we
propose an auxiliary objective for the base model that induces more reliable
predictions of the localized event labels as desired. Our three-stage pipeline
outperforms several existing AVEL methods with no architectural changes and
improves performance on a related weakly-supervised task as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextualized End-to-End Speech Recognition with Contextual Phrase
  Prediction Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12493v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12493v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixun Huang, Ao Zhang, Zhanheng Yang, Pengcheng Guo, Bingshen Mu, Tianyi Xu, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual information plays a crucial role in speech recognition
technologies and incorporating it into the end-to-end speech recognition models
has drawn immense interest recently. However, previous deep bias methods lacked
explicit supervision for bias tasks. In this study, we introduce a contextual
phrase prediction network for an attention-based deep bias method. This network
predicts context phrases in utterances using contextual embeddings and
calculates bias loss to assist in the training of the contextualized model. Our
method achieved a significant word error rate (WER) reduction across various
end-to-end speech recognition models. Experiments on the LibriSpeech corpus
show that our proposed model obtains a 12.1% relative WER improvement over the
baseline model, and the WER of the context phrases decreases relatively by
40.5%. Moreover, by applying a context phrase filtering strategy, we also
effectively eliminate the WER degradation when using a larger biasing list.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by interspeech2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VampNet: Music Generation via Masked Acoustic Token Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VampNet, a masked acoustic token modeling approach to music
synthesis, compression, inpainting, and variation. We use a variable masking
schedule during training which allows us to sample coherent music from the
model by applying a variety of masking approaches (called prompts) during
inference. VampNet is non-autoregressive, leveraging a bidirectional
transformer architecture that attends to all tokens in a forward pass. With
just 36 sampling passes, VampNet can generate coherent high-fidelity musical
waveforms. We show that by prompting VampNet in various ways, we can apply it
to tasks like music compression, inpainting, outpainting, continuation, and
looping with variation (vamping). Appropriately prompted, VampNet is capable of
maintaining style, genre, instrumentation, and other high-level aspects of the
music. This flexible prompting capability makes VampNet a powerful music
co-creation tool. Code and audio samples are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Tuberculosis from Real-World Cough Audio Recordings and
  Metadata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George P. Kafentzis, Stephane Tetsing, Joe Brew, Lola Jover, Mindaugas Galvosas, Carlos Chaccour, Peter M. Small
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tuberculosis (TB) is an infectious disease caused by the bacterium
Mycobacterium tuberculosis and primarily affects the lungs, as well as other
body parts. TB is spread through the air when an infected person coughs,
sneezes, or talks. Medical doctors diagnose TB in patients via clinical
examinations and specialized tests. However, coughing is a common symptom of
respiratory diseases such as TB. Literature suggests that cough sounds coming
from different respiratory diseases can be distinguished by both medical
doctors and computer algorithms. Therefore, cough recordings associated with
patients with and without TB seems to be a reasonable avenue of investigation.
In this work, we utilize a very large dataset of TB and non-TB cough audio
recordings obtained from the south-east of Africa, India, and the south-east of
Asia using a fully automated phone-based application (Hyfe), without manual
annotation. We fit statistical classifiers based on spectral and time domain
features with and without clinical metadata. A stratified grouped
cross-validation approach shows that an average Area Under Curve (AUC) of
approximately 0.70 $\pm$ 0.05 both for a cough-level and a participant-level
classification can be achieved using cough sounds alone. The addition of
demographic and clinical factors increases performance, resulting in an average
AUC of approximately 0.81 $\pm$ 0.05. Our results suggest mobile phone-based
applications that integrate clinical symptoms and cough sound analysis could
help community health workers and, most importantly, health service programs to
improve TB case-finding efforts while reducing costs, which could substantially
improve public health.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARBLE: Music Audio Representation Benchmark for Universal Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of extensive intersection between art and Artificial Intelligence
(AI), such as image generation and fiction co-creation, AI for music remains
relatively nascent, particularly in music understanding. This is evident in the
limited work on deep music representations, the scarcity of large-scale
datasets, and the absence of a universal and community-driven benchmark. To
address this issue, we introduce the Music Audio Representation Benchmark for
universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various
Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy
with four hierarchy levels, including acoustic, performance, score, and
high-level description. We then establish a unified protocol based on 14 tasks
on 8 public-available datasets, providing a fair and standard assessment of
representations of all open-sourced pre-trained models developed on music
recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and
reproducible suite for the community, with a clear statement on copyright
issues on datasets. Results suggest recently proposed large-scale pre-trained
musical language models perform the best in most tasks, with room for further
improvement. The leaderboard and toolkit repository are published at
https://marble-bm.shef.ac.uk to promote future music AI research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMTNet: Hierarchical cavitation intensity recognition based on sub-main
  transfer network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01429v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01429v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Sha, Johannes Faber, Shuiping Gou, Bo Liu, Wei Li, Stefan Schramm, Horst Stoecker, Thomas Steckenreiter, Domagoj Vnucec, Nadine Wetzstein, Andreas Widl, Kai Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of smart manufacturing, data-driven machinery
health management has been of growing attention. In situations where some
classes are more difficult to be distinguished compared to others and where
classes might be organised in a hierarchy of categories, current DL methods can
not work well. In this study, a novel hierarchical cavitation intensity
recognition framework using Sub-Main Transfer Network, termed SMTNet, is
proposed to classify acoustic signals of valve cavitation. SMTNet model outputs
multiple predictions ordered from coarse to fine along a network corresponding
to a hierarchy of target cavitation states. Firstly, a data augmentation method
based on Sliding Window with Fast Fourier Transform (Swin-FFT) is developed to
solve few-shot problem. Secondly, a 1-D double hierarchical residual block (1-D
DHRB) is presented to capture sensitive features of the frequency domain valve
acoustic signals. Thirdly, hierarchical multi-label tree is proposed to assist
the embedding of the semantic structure of target cavitation states into
SMTNet. Fourthly, experience filtering mechanism is proposed to fully learn a
prior knowledge of cavitation detection model. Finally, SMTNet has been
evaluated on two cavitation datasets without noise (Dataset 1 and Dataset 2),
and one cavitation dataset with real noise (Dataset 3) provided by SAMSON AG
(Frankfurt). The prediction accurcies of SMTNet for cavitation intensity
recognition are as high as 95.32%, 97.16% and 100%, respectively. At the same
time, the testing accuracies of SMTNet for cavitation detection are as high as
97.02%, 97.64% and 100%. In addition, SMTNet has also been tested for different
frequencies of samples and has achieved excellent results of the highest
frequency of samples of mobile phones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>we need update this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpeechBlender: Speech Augmentation Framework for Mispronunciation Data
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali, Hamdy Mubarak, Shazia Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of labeled second language (L2) speech data is a major challenge in
designing mispronunciation detection models. We introduce SpeechBlender - a
fine-grained data augmentation pipeline for generating mispronunciation errors
to overcome such data scarcity. The SpeechBlender utilizes varieties of masks
to target different regions of phonetic units, and use the mixing factors to
linearly interpolate raw speech signals while augmenting pronunciation. The
masks facilitate smooth blending of the signals, generating more effective
samples than the `Cut/Paste' method. Our proposed technique achieves
state-of-the-art results, with Speechocean762, on ASR dependent
mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson
Correlation Coefficient (PCC) compared to the previous state-of-the-art [1].
Additionally, we demonstrate a 5.0% improvement at the phoneme level compared
to our baseline. We also observed a 4.6% increase in F1-score with Arabic
AraVoiceL2 testset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretically Private Federated Submodel Learning with
  Storage Constrained Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajani Vithana, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated submodel learning (FSL), a machine learning model is divided
into multiple submodels based on different types of data used for training.
Each user involved in the training process only downloads and updates the
submodel relevant to the user's local data, which significantly reduces the
communication cost compared to classical federated learning (FL). However, the
index of the submodel updated by the user and the values of the updates reveal
information about the user's private data. In order to guarantee
information-theoretic privacy in FSL, the model is stored at multiple
non-colluding databases, and the user sends queries and updates to each
database in such a way that no information is revealed on the updating submodel
index or the values of the updates. In this work, we consider the practical
scenario where the multiple non-colluding databases are allowed to have
arbitrary storage constraints. The goal of this work is to develop read-write
schemes and storage mechanisms for FSL that efficiently utilize the available
storage in each database to store the submodel parameters in such a way that
the total communication cost is minimized while guaranteeing
information-theoretic privacy of the updating submodel index and the values of
the updates. As the main result, we consider both heterogeneous and homogeneous
storage constrained databases, and propose private read-write and storage
schemes for the two cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2302.03670</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kriging-Based 3-D Spectrum Awareness for Radio Dynamic Zones Using
  Aerial Spectrum Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sung Joon Maeng, Ozgur Ozdemir, Ismail Guvenc, Mihail L. Sichitiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio dynamic zones (RDZs) are geographical areas within which dedicated
spectrum resources are monitored and controlled to enable the development and
testing of new spectrum technologies. Real-time spectrum awareness within an
RDZ is critical for preventing interference with nearby incumbent users of the
spectrum. In this paper, we consider a 3D RDZ scenario and propose to use
unmanned aerial vehicles (UAVs) equipped with spectrum sensors to create and
maintain a 3D radio map of received signal power from different sources within
the RDZ. In particular, we introduce a 3D Kriging interpolation technique that
uses realistic 3D correlation models of the signal power extracted from
extensive measurements carried out at the NSF AERPAW platform. Using C-Band
signal measurements by a UAV at altitudes between 30 m-110 m, we first develop
realistic propagation models on air-to-ground path loss, shadowing, spatial
correlation, and semi-variogram, while taking into account the knowledge of
antenna radiation patterns and ground reflection. Subsequently, we generate a
3D radio map of a signal source within the RDZ using the Kriging interpolation
and evaluate its sensitivity to the number of measurements used and their
spatial distribution. Our results show that the proposed 3D Kriging
interpolation technique provides significantly better radio maps when compared
with an approach that assumes perfect knowledge of path loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Models for Physiological Signals: A Systematic
  Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nour Neifar, Afef Mdhaffar, Achraf Ben-Hamadou, Mohamed Jmaiel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a systematic literature review on deep generative
models for physiological signals, particularly electrocardiogram,
electroencephalogram, photoplethysmogram and electromyogram. Compared to the
existing review papers, we present the first review that summarizes the recent
state-of-the-art deep generative models. By analysing the state-of-the-art
research related to deep generative models along with their main applications
and challenges, this review contributes to the overall understanding of these
models applied to physiological signals. Additionally, by highlighting the
employed evaluation protocol and the most used physiological databases, this
review facilitates the assessment and benchmarking of deep generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paper under review, 34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In silico Ptychography of Lithium-ion Cathode Materials from Subsampled
  4-D STEM Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex W. Robinson, Amirafshar Moshtaghpour, Jack Wells, Daniel Nicholls, Zoe Broad, Angus I. Kirkland, Beata L. Mehdi, Nigel D. Browning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High quality scanning transmission electron microscopy (STEM) data
acquisition and analysis has become increasingly important due to the
commercial demand for investigating the properties of complex materials such as
battery cathodes; however, multidimensional techniques (such as 4-D STEM) which
can improve resolution and sample information are ultimately limited by the
beam-damage properties of the materials or the signal-to-noise ratio of the
result. subsampling offers a solution to this problem by retaining high signal,
but distributing the dose across the sample such that the damage can be
reduced. It is for these reasons that we propose a method of subsampling for
4-D STEM, which can take advantage of the redundancy within said data to
recover functionally identical results to the ground truth. We apply these
ideas to a simulated 4-D STEM data set of a LiMnO2 sample and we obtained high
quality reconstruction of phase images using 12.5% subsampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Estimation for Beyond Diagonal Reconfigurable Intelligent
  Surfaces with Group-Connected Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Li, Yumeng Zhang, Bruno Clerckx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study channel estimation for a beyond diagonal reconfigurable intelligent
surface (BD-RIS) aided multiple input single output system. We first describe
the channel estimation strategy based on the least square (LS) method, derive
the mean square error (MSE) of the LS estimator, and formulate the BD-RIS
design problem that minimizes the estimation MSE with unique constraints
induced by group-connected architectures of BD-RIS. Then, we propose an
efficient BD-RIS design which theoretically guarantees to achieve the MSE lower
bound. Finally, we provide simulation results to verify the effectiveness of
the proposed channel estimation scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, submitted to conference for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 137.5 TOPS/W SRAM Compute-in-Memory Macro with 9-b Memory
  Cell-Embedded ADCs and Signal Margin Enhancement Techniques for AI Edge
  Applications <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Wang, Fengshi Tian, Xizi Chen, Jiakun Zheng, Xuejiao Liu, Fengbin Tu, Jie Yang, Mohamad Sawan,  Kwang-Ting,  Cheng, Chi-Ying Tsui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a high-precision SRAM-based CIM macro that can
perform 4x4-bit MAC operations and yield 9-bit signed output. The inherent
discharge branches of SRAM cells are utilized to apply time-modulated MAC and
9-bit ADC readout operations on two bit-line capacitors. The same principle is
used for both MAC and A-to-D conversion ensuring high linearity and thus
supporting large number of analog MAC accumulations. The memory cell-embedded
ADC eliminates the use of separate ADCs and enhances energy and area
efficiency. Additionally, two signal margin enhancement techniques, namely the
MAC-folding and boosted-clipping schemes, are proposed to further improve the
CIM computation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ASSCC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tractable Statistical Representation of IFTR Fading with Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Olyaee, Hadi Hashemi, Juan M. Romero-Jerez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently introduced independent fluctuating two-ray (IFTR) fading model,
consisting of two specular components fluctuating independently plus a diffuse
component, has proven to provide an excellent fit to different wireless
environments, including the millimeter-wave band. However, the original
formulations of the probability density function (PDF) and cumulative
distribution function (CDF) of this model are not applicable to all possible
values of its defining parameters, and are given in terms of multifold
generalized hypergeometric functions, which prevents their widespread use for
the derivation of performance metric expressions. In this paper we present a
new formulation of the IFTR model as a countable mixture of Gamma distributions
which greatly facilitates the performance evaluation for this model in terms of
the metrics already known for the much simpler and widely used Nakagami-m
fading. Additionally, a closed-form expression is presented for the generalized
moment generating function (GMGF), which permits to readily obtain all the
moments of the distribution of the model, as well as several relevant
performance metrics. Based on these new derivations, the IFTR model is
evaluated for the average channel capacity, the outage probability with and
without co-channel interference, and the bit error rate (BER), which are
verified by Monte Carlo simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was submitted to the IEEE for publication. Copyright may be
  transferred without notice, after which this version may no longer be
  accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FIS-ONE: Floor Identification System with One Label for Crowdsourced RF
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weipeng Zhuo, Ka Ho Chiu, Jierun Chen, Ziqi Zhao, S. -H. Gary Chan, Sangtae Ha, Chul-Ho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floor labels of crowdsourced RF signals are crucial for many smart-city
applications, such as multi-floor indoor localization, geofencing, and robot
surveillance. To build a prediction model to identify the floor number of a new
RF signal upon its measurement, conventional approaches using the crowdsourced
RF signals assume that at least few labeled signal samples are available on
each floor. In this work, we push the envelope further and demonstrate that it
is technically feasible to enable such floor identification with only one
floor-labeled signal sample on the bottom floor while having the rest of signal
samples unlabeled.
  We propose FIS-ONE, a novel floor identification system with only one labeled
sample. FIS-ONE consists of two steps, namely signal clustering and cluster
indexing. We first build a bipartite graph to model the RF signal samples and
obtain a latent representation of each node (each signal sample) using our
attention-based graph neural network model so that the RF signal samples can be
clustered more accurately. Then, we tackle the problem of indexing the clusters
with proper floor labels, by leveraging the observation that signals from an
access point can be detected on different floors, i.e., signal spillover.
Specifically, we formulate a cluster indexing problem as a combinatorial
optimization problem and show that it is equivalent to solving a traveling
salesman problem, whose (near-)optimal solution can be found efficiently. We
have implemented FIS-ONE and validated its effectiveness on the Microsoft
dataset and in three large shopping malls. Our results show that FIS-ONE
outperforms other baseline algorithms significantly, with up to 23% improvement
in adjusted rand index and 25% improvement in normalized mutual information
using only one floor-labeled signal sample.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ICDCS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Unrolling for Nonconvex Robust Principal Component Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Z. C. Tan, Caroline Chaux, Emmanuel Soubies, Vincent Y. F. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We design algorithms for Robust Principal Component Analysis (RPCA) which
consists in decomposing a matrix into the sum of a low rank matrix and a sparse
matrix. We propose a deep unrolled algorithm based on an accelerated
alternating projection algorithm which aims to solve RPCA in its nonconvex
form. The proposed procedure combines benefits of deep neural networks and the
interpretability of the original algorithm and it automatically learns
hyperparameters. We demonstrate the unrolled algorithm's effectiveness on
synthetic datasets and also on a face modeling problem, where it leads to both
better numerical and visual performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures; Accepted to the 2023 IEEE International Workshop
  on Machine Learning for Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deterministic Multi-sensor Measurement-adaptive Birth using Labeled
  Random Finite Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Bondarchuk, Anthony Trezza, Donald J. Bucci Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measurement-adaptive track initiation remains a critical design requirement
of many practical multi-target tracking systems. For labeled random finite sets
multi-object filters, prior work has been established to construct a labeled
multi-object birth density using measurements from multiple sensors. A
truncation procedure has also been provided that leverages a stochastic Gibbs
sampler to truncate the birth density for scalability. In this work, we
introduce a deterministic herded Gibbs sampling truncation solution for
efficient multi-sensor adaptive track initialization. Removing the stochastic
behavior of the track initialization procedure without impacting average
tracking performance enables a more robust tracking solution more suitable for
safety-critical applications. Simulation results for linear sensing scenarios
are provided to verify performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2023 Proc. IEEE 26th Int. Conf. Inf. Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Anomaly Detection in PPG Data using Representation Learning
  and Biometric Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramin Ghorbani, Marcel J. T. Reinders, David M. J. Tax
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoplethysmography (PPG) signals, typically acquired from wearable devices,
hold significant potential for continuous fitness-health monitoring. In
particular, heart conditions that manifest in rare and subtle deviating heart
patterns may be interesting. However, robust and reliable anomaly detection
within these data remains a challenge due to the scarcity of labeled data and
high inter-subject variability. This paper introduces a two-stage framework
leveraging representation learning and personalization to improve anomaly
detection performance in PPG data. The proposed framework first employs
representation learning to transform the original PPG signals into a more
discriminative and compact representation. We then apply three different
unsupervised anomaly detection methods for movement detection and biometric
identification. We validate our approach using two different datasets in both
generalized and personalized scenarios. The results show that representation
learning significantly improves anomaly detection performance while reducing
the high inter-subject variability. Personalized models further enhance anomaly
detection performance, underscoring the role of personalization in PPG-based
fitness-health monitoring systems. The results from biometric identification
show that it's easier to distinguish a new user from one intended authorized
user than from a group of users. Overall, this study provides evidence of the
effectiveness of representation learning and personalization for anomaly
detection in PPG data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dynamic Obstacle Tracking Strategy for Proactive Handoffs in
  Millimeter-wave Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00429v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00429v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rathindra Nath Dutta, Subhojit Sarkar, Sasthi C. Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stringent line-of-sight demands necessitated by the fast attenuating nature
of millimeter waves (mmWaves) through obstacles pose one of the central
problems of next generation wireless networks. These mmWave links are easily
disrupted due to obstacles, including vehicles and pedestrians, which cause
degradation in link quality and even link failure. Dynamic obstacles are
usually tracked by dedicated tracking hardware like RGB-D cameras, which
usually have small ranges, and hence lead to prohibitively increased deployment
costs to achieve complete coverage of the deployment area. In this manuscript,
we propose an altogether different approach to track multiple dynamic obstacles
in an mmWave network, solely based on short-term historical link failure
information, without resorting to any dedicated tracking hardware. After
proving that the said problem is NP-complete, we employ a greedy set-cover
based approach to solve it. Using the obtained trajectories, we perform
proactive handoffs for at-risk links. We compare our approach with an RGB-D
camera-based approach and show that our approach provides better tracking and
handoff performances when the camera coverage is low to moderate, which is
often the case in real deployment scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Network Slicing for eMBB, URLLC, and mMTC: An Uplink Rate-Splitting
  Multiple Access Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanwen Liu, Bruno Clerckx, Petar Popovski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are three generic services in 5G: enhanced mobile broadband (eMBB),
ultra-reliable low-latency communications (URLLC), and massive machine-type
communications (mMTC). To guarantee the performance of heterogeneous services,
network slicing is proposed to allocate resources to different services.
Network slicing is typically done in an orthogonal multiple access (OMA)
fashion, which means different services are allocated non-interfering
resources. However, as the number of users grows, OMA-based slicing is not
always optimal, and a non-orthogonal scheme may achieve a better performance.
This work aims to analyse the performances of different slicing schemes in
uplink, and a promising scheme based on rate-splitting multiple access (RSMA)
is studied. RSMA can provide a more flexible decoding order and theoretically
has the largest achievable rate region than OMA and non-orthogonal multiple
access (NOMA) without time-sharing. Hence, RSMA has the potential to increase
the rate of users requiring different services. In addition, it is not
necessary to decode the two split streams of one user successively, so RSMA
lets suitable users split messages and designs an appropriate decoding order
depending on the service requirements. This work shows that for network slicing
RSMA can outperform NOMA counterpart, and obtain significant gains over OMA in
some region.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised topological data analysis for MALDI mass spectrometry imaging
  applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gideon Klaila, Vladimir Vutov, Anastasios Stefanou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Matrix-assisted laser desorption/ionization mass spectrometry
imaging (MALDI MSI) displays significant potential for applications in cancer
research, especially in tumor typing and subtyping. Lung cancer is the primary
cause of tumor-related deaths, where the most lethal entities are
adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing between
these two common subtypes is crucial for therapy decisions and successful
patient management.
  Results: We propose a new algebraic topological framework, which obtains
intrinsic information from MALDI data and transforms it to reflect topological
persistence. Our framework offers two main advantages. Firstly, topological
persistence aids in distinguishing the signal from noise. Secondly, it
compresses the MALDI data, saving storage space and optimizes computational
time for subsequent classification tasks. We present an algorithm that
efficiently implements our topological framework, relying on a single tuning
parameter. Afterwards, logistic regression and random forest classifiers are
employed on the extracted persistence features, thereby accomplishing an
automated tumor (sub-)typing process. To demonstrate the competitiveness of our
proposed framework, we conduct experiments on a real-world MALDI dataset using
cross-validation. Furthermore, we showcase the effectiveness of the single
denoising parameter by evaluating its performance on synthetic MALDI images
with varying levels of noise.
  Conclusion: Our empirical experiments demonstrate that the proposed algebraic
topological framework successfully captures and leverages the intrinsic
spectral information from MALDI data, leading to competitive results in
classifying lung cancer subtypes. Moreover, the frameworks ability to be
fine-tuned for denoising highlights its versatility and potential for enhancing
data analysis in MALDI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ B-HAR: an open-source baseline framework for in depth study of human
  activity recognition <span class="highlight-title">dataset</span>s and workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.10870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.10870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florenc Demrozi, Cristian Turetta, Graziano Pravadelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR), based on machine and deep learning
algorithms is considered one of the most promising technologies to monitor
professional and daily life activities for different categories of people
(e.g., athletes, elderly, kids, employers) in order to provide a variety of
services related, for example to well-being, empowering of technical
performances, prevention of risky situation, and educational purposes. However,
the analysis of the effectiveness and the efficiency of HAR methodologies
suffers from the lack of a standard workflow, which might represent the
baseline for the estimation of the quality of the developed pattern recognition
models. This makes the comparison among different approaches a challenging
task. In addition, researchers can make mistakes that, when not detected,
definitely affect the achieved results. To mitigate such issues, this paper
proposes an open-source automatic and highly configurable framework, named
B-HAR, for the definition, standardization, and development of a baseline
framework in order to evaluate and compare HAR methodologies. It implements the
most popular data processing methods for data preparation and the most commonly
used machine and deep learning pattern recognition models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, 3 Figures, 3 Tables, Link to B-HAR Library:
  https://github.com/B-HAR-HumanActivityRecognition/B-HAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Analysis and Comparison of Non-ideal Wireless PBFT and RAFT
  Consensus Networks in 6G Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Luo, Xiangyue Yang, Hongfang Yu, Gang Sun, Bo Lei, Mohsen Guizani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to advantages in security and privacy, blockchain is considered a key
enabling technology to support 6G communications. Practical Byzantine Fault
Tolerance (PBFT) and RAFT are seen as the most applicable consensus mechanisms
(CMs) in blockchain-enabled wireless networks. However, previous studies on
PBFT and RAFT rarely consider the channel performance of the physical layer,
such as path loss and channel fading, resulting in research results that are
far from real networks. Additionally, 6G communications will widely deploy
high-frequency signals such as terahertz (THz) and millimeter wave (mmWave),
while performances of PBFT and RAFT are still unknown when these signals are
transmitted in wireless PBFT or RAFT networks. Therefore, it is urgent to study
the performance of non-ideal wireless PBFT and RAFT networks with THz and
mmWave signals, to better make PBFT and RAFT play a role in the 6G era. In this
paper, we study and compare the performance of THz and mmWave signals in
non-ideal wireless PBFT and RAFT networks, considering Rayleigh Fading (RF) and
close-in Free Space (FS) reference distance path loss. Performance is evaluated
by five metrics: consensus success rate, latency, throughput, reliability gain,
and energy consumption. Meanwhile, we find and derive that there is a maximum
distance between two nodes that can make CMs inevitably successful, and it is
named the active distance of CMs. The research results analyze the performance
of non-ideal wireless PBFT and RAFT networks, and provide important references
for the future transmission of THz and mmWave signals in PBFT and RAFT
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.15759</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Half-Duplex Two-Node Slotted ALOHA Network With Asynchronous
  Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Ali Hashemian, Farid Ashtiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the long history of research on slotted ALOHA, the exact analysis of
the average delay is still in question as the performance of each node is
coupled with the activity of other nodes. In this paper, we consider a network
comprised of two half-duplex transmitter nodes with asynchronous arrival
traffic that follow the slotted ALOHA protocol. We propose a new queueing
theoretic model based on the state-dependent queues to analyze the network. In
addition, we derive the exact values of delay and stability region for each
node. The numerical results demonstrate the accuracy of our proposed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretically Private Federated Submodel Learning with
  Storage Constrained Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajani Vithana, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated submodel learning (FSL), a machine learning model is divided
into multiple submodels based on different types of data used for training.
Each user involved in the training process only downloads and updates the
submodel relevant to the user's local data, which significantly reduces the
communication cost compared to classical federated learning (FL). However, the
index of the submodel updated by the user and the values of the updates reveal
information about the user's private data. In order to guarantee
information-theoretic privacy in FSL, the model is stored at multiple
non-colluding databases, and the user sends queries and updates to each
database in such a way that no information is revealed on the updating submodel
index or the values of the updates. In this work, we consider the practical
scenario where the multiple non-colluding databases are allowed to have
arbitrary storage constraints. The goal of this work is to develop read-write
schemes and storage mechanisms for FSL that efficiently utilize the available
storage in each database to store the submodel parameters in such a way that
the total communication cost is minimized while guaranteeing
information-theoretic privacy of the updating submodel index and the values of
the updates. As the main result, we consider both heterogeneous and homogeneous
storage constrained databases, and propose private read-write and storage
schemes for the two cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2302.03670</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corona: System Implications of Emerging Nanophotonic Technology <span class="chip">ISCA-35</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Vantrease, Robert Schreiber, Matteo Monchiero, Moray McLaren, Norman P. Jouppi, Marco Fiorentin, Al Davis, Nathan Binkert, Raymond G. Beausoleil, Jung Ho Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We expect that many-core microprocessors will push performance per chip from
the 10 gigaflop to the 10 teraflop range in the coming decade. To support this
increased performance, memory and inter-core bandwidths will also have to scale
by orders of magnitude. Pin limitations, the energy cost of electrical
signaling, and the non-scalability of chip-length global wires are significant
bandwidth impediments. Recent developments in silicon nanophotonic technology
have the potential to meet these off- and on- stack bandwidth requirements at
acceptable power levels.
  Corona is a 3D many-core architecture that uses nanophotonic communication
for both inter-core communication and off-stack communication to memory or I/O
devices. Its peak floating-point performance is 10 teraflops. Dense wavelength
division multiplexed optically connected memory modules provide 10 terabyte per
second memory bandwidth. A photonic crossbar fully interconnects its 256
low-power multithreaded cores at 20 terabyte per second bandwidth. We have
simulated a 1024 thread Corona system running synthetic benchmarks and scaled
versions of the SPLASH-2 benchmark suite. We believe that in comparison with an
electrically-connected many-core alternative that uses the same on-stack
interconnect power, Corona can provide 2 to 6 times more performance on many
memory-intensive workloads, while simultaneously reducing power.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This edition is recompiled from proceedings of ISCA-35 (the 35th
  International Symposium on Computer Architecture, June 21 - 25, 2008,
  Beijing, China) and has minor formatting differences. 13 pages; 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Architecture for Control Plane Slicing in Beyond 5G Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rashmi Yadav, Rashmi Kamran, Pranav Jha, Abhay Karandikar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To accommodate various use cases with differing characteristics, the Fifth
Generation (5G) mobile communications system intends to utilize network
slicing. Network slicing enables the creation of multiple logical networks over
a shared physical network infrastructure. While the problems such as resource
allocation for multiple slices in mobile networks have been explored in
considerable detail in the existing literature, the suitability of the existing
mobile network architecture to support network slicing has not been analysed
adequately. We think the existing 5G System (5GS) architecture suffers from
certain limitations, such as a lack of slice isolation in its control plane.
This work focuses on the future evolution of the existing 5GS architecture from
a slicing perspective, especially that of its control plane, addressing some of
the limitations of the existing 5GS architecture. We propose a new network
architecture which enables efficient slicing in beyond 5G networks. The
proposed architecture results in enhanced modularity and scalability of the
control plane in sliced mobile networks. In addition, it also brings slice
isolation to the control plane, which is not feasible in the existing 5G
system. We also present a performance evaluation that confirms the improved
performance and scalability of the proposed system viz a viz the existing 5G
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating DNS Resiliency and Responsiveness with Truncation,
  Fragmentation & DoTCP Fallback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyush Dikshit, Mike Kosek, Nils Faulhaber, Jayasree Sengupta, Vaibhav Bajpai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since its introduction in 1987, the DNS has become one of the core components
of the Internet. While it was designed to work with both TCP and UDP,
DNS-over-UDP (DoUDP) has become the default option due to its low overhead. As
new Resource Records were introduced, the sizes of DNS responses increased
considerably. This expansion of message body has led to truncation and IP
fragmentation more often in recent years where large UDP responses make DNS an
easy vector for amplifying denial-of-service attacks which can reduce the
resiliency of DNS services. This paper investigates the resiliency,
responsiveness, and usage of DoTCP and DoUDP over IPv4 and IPv6 for 10 widely
used public DNS resolvers. In these experiments, these aspects are investigated
from the edge and from the core of the Internet to represent the communication
of the resolvers with DNS clients and authoritative name servers. Overall, more
than 14M individual measurements from 2527 RIPE Atlas Probes have been
analyzed, highlighting that most resolvers show similar resiliency for both
DoTCP and DoUDP. While DNS Flag Day 2020 recommended 1232 bytes of buffer sizes
yet we find out that 3 out of 10 resolvers mainly announce very large EDNS(0)
buffer sizes both from the edge as well as from the core, which potentially
causes fragmentation. In reaction to large response sizes from authoritative
name servers, we find that resolvers do not fall back to the usage of DoTCP in
many cases, bearing the risk of fragmented responses. As the message sizes in
the DNS are expected to grow further, this problem will become more urgent in
the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact Resource Allocation over Fair Wireless Relay Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edgar Arribas, Vicent Cholvi, Vincenzo Mancuso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In relay-enabled cellular networks, the intertwined nature of network agents
calls for complex schemes to allocate wireless resources. Resources need to be
distributed among mobile users while considering how relay resources are
allocated, and constrained by the traffic rate achievable by base stations and
over backhaul links. In this work, we derive a resource allocation scheme that
achieves max-min fairness across mobile users. Furthermore, the optimal
allocation is found with linear complexity with respect to the number of mobile
users and relays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Linear Algebraic Framework for Dynamic Scheduling Over Memory-Equipped
  Quantum Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Fittipaldi, Anastasios Giovanidis, Frédéric Grosshans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Internetworking is a recent field that promises numerous interesting
applications, many of which require the distribution of entanglement between
arbitrary pairs of users. This work deals with the problem of scheduling in an
arbitrary entanglement swapping quantum network - often called first generation
quantum network - in its general topology, multicommodity, loss-aware
formulation. We introduce a linear algebraic framework that exploits quantum
memory through the creation of intermediate entangled links. The framework is
then employed to mathematically derive a natural class of quadratic scheduling
policies for quantum networks by applying Lyapunov Drift Minimization, a
standard technique in classical network science. Moreover, an additional class
of Max-Weight inspired policies is proposed and benchmarked, reducing
significantly the computation cost, at the price of a slight performance
degradation. The policies are compared in terms of information availability,
localization and overall network performance through an ad-hoc simulator that
admits user-provided network topologies and scheduling policies in order to
showcase the potential application of the provided tools to quantum network
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures. To be submitted to the journal "IEEE
  Transactions on Quantum Engineering"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Packet-Level Analysis in Programmable Data Planes to Advance
  Network Intrusion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Doriguzzi-Corin, Luis Augusto Dias Knob, Luca Mendozzi, Domenico Siracusa, Marco Savi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programmable data planes offer precise control over the low-level processing
steps applied to network packets, serving as a valuable tool for analysing
malicious flows in the field of intrusion detection. Albeit with limitations on
physical resources and capabilities, they allow for the efficient extraction of
detailed traffic information, which can then be utilised by Machine Learning
(ML) algorithms responsible for identifying security threats. In addressing
resource constraints, existing solutions in the literature rely on compressing
network data through the collection of statistical traffic features in the data
plane. While this compression saves memory resources in switches and minimises
the burden on the control channel between the data and the control plane, it
also results in a loss of information available to the Network Intrusion
Detection System (NIDS), limiting access to packet payload, categorical
features, and the semantic understanding of network communications, such as the
behaviour of packets within traffic flows. This paper proposes P4DDLe, a
framework that exploits the flexibility of P4-based programmable data planes
for packet-level feature extraction and pre-processing. P4DDLe leverages the
programmable data plane to extract raw packet features from the network
traffic, categorical features included, and to organise them in a way that the
semantics of traffic flows is preserved. To minimise memory and control channel
overheads, P4DDLe selectively processes and filters packet-level data, so that
all and only the relevant features required by the NIDS are collected. The
experimental evaluation with recent Distributed Denial of Service (DDoS) attack
data demonstrates that the proposed approach is very efficient in collecting
compact and high-quality representations of network flows, ensuring precise
detection of DDoS attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying SDN to Mobile Networks: A New Perspective for 6G Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rashmi Yadav, Rashmi Kamran, Pranav Jha, Abhay Karandikar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The upcoming Sixth Generation (6G) mobile communications system envisions
supporting a variety of use cases with differing characteristics, e.g., very
low to extremely high data rates, diverse latency needs, ultra massive
connectivity, sustainable communications, ultra-wide coverage etc. To
accommodate these diverse use cases, the 6G system architecture needs to be
scalable, modular, and flexible; both in its user plane and the control plane.
In this paper, we identify some limitations of the existing Fifth Generation
System (5GS) architecture, especially that of its control plane. Further, we
propose a novel architecture for the 6G System (6GS) employing Software Defined
Networking (SDN) technology to address these limitations of the control plane.
The control plane in existing 5GS supports two different categories of
functionalities handling end user signalling (e.g., user registration,
authentication) and control of user plane functions. We propose to move the
end-user signalling functionality out of the mobile network control plane and
treat it as user service, i.e., as payload or data. This proposal results in an
evolved service-driven architecture for mobile networks bringing increased
simplicity, modularity, scalability, flexibility and security to its control
plane. The proposed architecture can also support service specific signalling
support, if needed, making it better suited for diverse 6GS use cases. To
demonstrate the advantages of the proposed architecture, we also compare its
performance with the 5GS using a process algebra-based simulation tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FIS-ONE: Floor Identification System with One Label for Crowdsourced RF
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weipeng Zhuo, Ka Ho Chiu, Jierun Chen, Ziqi Zhao, S. -H. Gary Chan, Sangtae Ha, Chul-Ho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floor labels of crowdsourced RF signals are crucial for many smart-city
applications, such as multi-floor indoor localization, geofencing, and robot
surveillance. To build a prediction model to identify the floor number of a new
RF signal upon its measurement, conventional approaches using the crowdsourced
RF signals assume that at least few labeled signal samples are available on
each floor. In this work, we push the envelope further and demonstrate that it
is technically feasible to enable such floor identification with only one
floor-labeled signal sample on the bottom floor while having the rest of signal
samples unlabeled.
  We propose FIS-ONE, a novel floor identification system with only one labeled
sample. FIS-ONE consists of two steps, namely signal clustering and cluster
indexing. We first build a bipartite graph to model the RF signal samples and
obtain a latent representation of each node (each signal sample) using our
attention-based graph neural network model so that the RF signal samples can be
clustered more accurately. Then, we tackle the problem of indexing the clusters
with proper floor labels, by leveraging the observation that signals from an
access point can be detected on different floors, i.e., signal spillover.
Specifically, we formulate a cluster indexing problem as a combinatorial
optimization problem and show that it is equivalent to solving a traveling
salesman problem, whose (near-)optimal solution can be found efficiently. We
have implemented FIS-ONE and validated its effectiveness on the Microsoft
dataset and in three large shopping malls. Our results show that FIS-ONE
outperforms other baseline algorithms significantly, with up to 23% improvement
in adjusted rand index and 25% improvement in normalized mutual information
using only one floor-labeled signal sample.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ICDCS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud
  Computing Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Zhang, Xuling Zhang, Guangzhi Zhu, Yuyang Wang, Pan Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to
empower various areas as a bridge between physical objects and the digital
world. Through virtualization and simulation techniques, multiple functions can
be achieved by leveraging computing resources. In this process, Mobile Cloud
Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key
factors to achieve real-time feedback. However, current works only considered
edge servers or cloud servers in the DT system models. Besides, The models
ignore the DT with not only one data resource. In this paper, we propose a new
DT system model considering a heterogeneous MEC/MCC environment. Each DT in the
model is maintained in one of the servers via multiple data collection devices.
The offloading decision-making problem is also considered and a new offloading
scheme is proposed based on Distributed Deep Learning (DDL). Simulation results
demonstrate that our proposed algorithm can effectively and efficiently
decrease the system's average latency and energy consumption. Significant
improvement is achieved compared with the baselines under the dynamic
environment of DTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Characterization of Quantum Flip Stars with Quantum Network
  Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matheus Guedes de Andrade, Jake Navas, Inès Montaño, Don Towsley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The experimental realization of quantum information systems will be difficult
due to how sensitive quantum information is to noise. Overcoming this
sensitivity is central to designing quantum networks capable of transmitting
quantum information reliably over large distances. Moreover, the ability to
characterize communication noise in quantum networks is crucial in developing
network protocols capable of overcoming the effects of noise in quantum
networks. In this context, quantum network tomography refers to the
characterization of channel noise in a quantum network through end-to-end
measurements. In this work, we propose network tomography protocols for quantum
star networks formed by quantum channels characterized by a single, non-trivial
Pauli operator. Our results further the end-to-end characterization of quantum
bit-flip star networks by introducing tomography protocols where state
distribution and measurements are designed separately. We build upon previously
proposed quantum network tomography protocols, as well as provide novel methods
for the unique characterization of bit-flip probabilities in stars. We
introduce a theoretical benchmark based on the Quantum Fisher Information
matrix to compare the efficiency of quantum network protocols. We apply our
techniques to the protocols proposed, and provide an initial analysis on the
potential benefits of entanglement for Quantum Network Tomography. Furthermore,
we simulate the proposed protocols using NetSquid to assess the convergence
properties of the estimators obtained for particular parameter regimes. Our
findings show that the efficiency of protocols depend on parameter values and
motivate the search for adaptive quantum network tomography protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures. Accepted for publication in IEEE QCE23
  proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Quantum Walk Control Plane for Quantum Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matheus Guedes de Andrade, Nitish K. Panigrahy, Wenhan Dai, Saikat Guha, Don Towsley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum networks are complex systems formed by the interaction among quantum
processors through quantum channels. Analogous to classical computer networks,
quantum networks allow for the distribution of quantum operations among quantum
processors. In this work, we describe a Quantum Walk Control Protocol (QWCP) to
perform distributed quantum operations in a quantum network. We consider a
generalization of the discrete-time coined quantum walk model that accounts for
the interaction between quantum walks in the network graph with quantum
registers inside the network nodes. QWCP allows for the implementation of
networked quantum services, such as distributed quantum computing and
entanglement distribution, abstracting hardware implementation and the
transmission of quantum information through channels. Multiple interacting
quantum walks can be used to propagate entangled control signals across the
network in parallel. We demonstrate how to use QWCP to perform distributed
multi-qubit controlled gates, which shows the universality of the protocol for
distributed quantum computing. Furthermore, we apply the QWCP to the task of
entanglement distribution in a quantum network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages; 2 figures. A preliminary version of this work was presented
  at IEEE International Conference on Quantum Computing and Engineering 2021
  (QCE21). arXiv admin note: text overlap with arXiv:2106.09839</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Market Driven Multi-domain Network Service Orchestration in 5G Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouhamad Dieye, Wael Jaafar, Halima Elbiaze, Roch Glitho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of a new breed of enhanced multimedia services has put network
operators into a position where they must support innovative services while
ensuring both end-to-end Quality of Service requirements and profitability.
Recently, Network Function Virtualization (NFV) has been touted as a
cost-effective underlying technology in 5G networks to efficiently provision
novel services. These NFV-based services have been increasingly associated with
multi-domain networks. However, several orchestration issues, linked to
cross-domain interactions and emphasized by the heterogeneity of underlying
technologies and administrative authorities, present an important challenge. In
this paper, we tackle the cross-domain interaction issue by proposing an
intelligent and profitable auction-based approach to allow inter-domains
resource allocation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Control Plane Experimentation with Horse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eder Leao Fernandes, Gianni Antichi, Timm Boettger, Ignacio Castro, Steve Uhlig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation and emulation are popular approaches for experimentation in
Computer Networks. However, due to their respective inherent drawbacks,
existing solutions cannot perform both fast and realistic control plane
experiments. To close this gap, we introduce Horse. Horse is a hybrid solution
with an emulated control plane, for realism, and simulated data plane, for
speed. Our decoupling of the control and data plane allows us to speed up the
experiments without sacrificing control plane realism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fresh-CSMA: A Distributed Protocol for Minimizing Age of Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishrant Tripathi, Nicholas Jones, Eytan Modiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the design of distributed scheduling algorithms that minimize age
of information in single-hop wireless networks. The centralized max-weight
policy is known to be nearly optimal in this setting; hence, our goal is to
design a distributed CSMA scheme that can mimic its performance. To that end,
we propose a distributed protocol called Fresh-CSMA and show that in an
idealized setting, Fresh-CSMA can match the scheduling decisions of the
max-weight policy with high probability in each frame, and also match the
theoretical performance guarantees of the max-weight policy over the entire
time horizon. We then consider a more realistic setting and study the impact of
protocol parameters on the probability of collisions and the overhead caused by
the distributed nature of the protocol. We also consider the monitoring of
Markov sources and extend our approach to CSMA protocols that incorporate Age
of Incorrect Information (AoII) instead of AoI. Finally, we provide simulations
that support our theoretical results and show that the performance gap between
the ideal and realistic versions of Fresh-CSMA is small.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE INFOCOM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dynamic Obstacle Tracking Strategy for Proactive Handoffs in
  Millimeter-wave Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00429v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00429v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rathindra Nath Dutta, Subhojit Sarkar, Sasthi C. Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stringent line-of-sight demands necessitated by the fast attenuating nature
of millimeter waves (mmWaves) through obstacles pose one of the central
problems of next generation wireless networks. These mmWave links are easily
disrupted due to obstacles, including vehicles and pedestrians, which cause
degradation in link quality and even link failure. Dynamic obstacles are
usually tracked by dedicated tracking hardware like RGB-D cameras, which
usually have small ranges, and hence lead to prohibitively increased deployment
costs to achieve complete coverage of the deployment area. In this manuscript,
we propose an altogether different approach to track multiple dynamic obstacles
in an mmWave network, solely based on short-term historical link failure
information, without resorting to any dedicated tracking hardware. After
proving that the said problem is NP-complete, we employ a greedy set-cover
based approach to solve it. Using the obtained trajectories, we perform
proactive handoffs for at-risk links. We compare our approach with an RGB-D
camera-based approach and show that our approach provides better tracking and
handoff performances when the camera coverage is low to moderate, which is
often the case in real deployment scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Analysis and Comparison of Non-ideal Wireless PBFT and RAFT
  Consensus Networks in 6G Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Luo, Xiangyue Yang, Hongfang Yu, Gang Sun, Bo Lei, Mohsen Guizani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to advantages in security and privacy, blockchain is considered a key
enabling technology to support 6G communications. Practical Byzantine Fault
Tolerance (PBFT) and RAFT are seen as the most applicable consensus mechanisms
(CMs) in blockchain-enabled wireless networks. However, previous studies on
PBFT and RAFT rarely consider the channel performance of the physical layer,
such as path loss and channel fading, resulting in research results that are
far from real networks. Additionally, 6G communications will widely deploy
high-frequency signals such as terahertz (THz) and millimeter wave (mmWave),
while performances of PBFT and RAFT are still unknown when these signals are
transmitted in wireless PBFT or RAFT networks. Therefore, it is urgent to study
the performance of non-ideal wireless PBFT and RAFT networks with THz and
mmWave signals, to better make PBFT and RAFT play a role in the 6G era. In this
paper, we study and compare the performance of THz and mmWave signals in
non-ideal wireless PBFT and RAFT networks, considering Rayleigh Fading (RF) and
close-in Free Space (FS) reference distance path loss. Performance is evaluated
by five metrics: consensus success rate, latency, throughput, reliability gain,
and energy consumption. Meanwhile, we find and derive that there is a maximum
distance between two nodes that can make CMs inevitably successful, and it is
named the active distance of CMs. The research results analyze the performance
of non-ideal wireless PBFT and RAFT networks, and provide important references
for the future transmission of THz and mmWave signals in PBFT and RAFT
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.15759</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for
  Test-Time Policy Adaptation <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andi Peng, Aviv Netanyahu, Mark Ho, Tianmin Shu, Andreea Bobu, Julie Shah, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policies often fail due to distribution shift -- changes in the state and
reward that occur when a policy is deployed in new environments. Data
augmentation can increase robustness by making the model invariant to
task-irrelevant changes in the agent's observation. However, designers don't
know which concepts are irrelevant a priori, especially when different end
users have different preferences about how the task is performed. We propose an
interactive framework to leverage feedback directly from the user to identify
personalized task-irrelevant concepts. Our key idea is to generate
counterfactual demonstrations that allow users to quickly identify possible
task-relevant and irrelevant concepts. The knowledge of task-irrelevant
concepts is then used to perform data augmentation and thus obtain a policy
adapted to personalized user objectives. We present experiments validating our
framework on discrete and continuous control tasks with real human users. Our
method (1) enables users to better understand agent failure, (2) reduces the
number of demonstrations required for fine-tuning, and (3) aligns the agent to
individual user task preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 52 Weeks Later: Attitudes Towards COVID-19 Apps for Different Purposes
  Over Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Kowalewski, Christine Utz, Martin Degeling, Theodor Schnitzler, Franziska Herbert, Leonie Schaewitz, Florian M. Farke, Steffen Becker, Markus Dürmuth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has prompted countries around the world to introduce
smartphone apps to support disease control efforts. Their purposes range from
digital contact tracing to quarantine enforcement to vaccination passports, and
their effectiveness often depends on widespread adoption. While previous work
has identified factors that promote or hinder adoption, it has typically
examined data collected at a single point in time or focused exclusively on
digital contact tracing apps. In this work, we conduct the first representative
study that examines changes in people's attitudes towards COVID-19-related
smartphone apps for five different purposes over the first 1.5 years of the
pandemic. In three survey rounds conducted between Summer 2020 and Summer 2021
in the United States and Germany, with approximately 1,000 participants per
round and country, we investigate people's willingness to use such apps, their
perceived utility, and people's attitudes towards them in different stages of
the pandemic. Our results indicate that privacy is a consistent concern for
participants, even in a public health crisis, and the collection of
identity-related data significantly decreases acceptance of COVID-19 apps.
Trust in authorities is essential to increase confidence in government-backed
apps and foster citizens' willingness to contribute to crisis management. There
is a need for continuous communication with app users to emphasize the benefits
of health crisis apps both for individuals and society, thus counteracting
decreasing willingness to use them and perceived usefulness as the pandemic
evolves.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 19 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Augmented Reality Selection Techniques for Passengers in
  Moving Vehicles: A Real-World User Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Connor Schramm, Markus Sasalovici, Axel Hildebrand, Ulrich Schwanecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, cars offer many possibilities to explore the world around you by
providing location-based information displayed on a 2D-Map. However, this
information is often only available to front-seat passengers while being
restricted to in-car displays. To propose a more natural way of interacting
with the environment, we implemented an augmented reality head-mounted display
to overlay points of interest onto the real world. We aim to compare multiple
selection techniques for digital objects located outside a moving car by
investigating head gaze with dwell time, head gaze with hardware button, eye
gaze with hardware button, and hand pointing with gesture confirmation. Our
study was conducted in a moving car under real-world conditions (N=22), with
significant results indicating that hand pointing usage led to slower and less
precise content selection while eye gaze was preferred by participants and
performed on par with the other techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, to be published in 15th International Conference
  on Automotive User Interfaces and Interactive Vehicular Applications
  (AutomotiveUI '23), September 18-22, 2023, Ingolstadt, Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Millions of User Interactions with ICEBOAT: Big Data Analytics
  for Automotive User Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Ebel, Kim Julian Gülle, Christoph Lingenfelder, Andreas Vogelsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User Experience (UX) professionals need to be able to analyze large amounts
of usage data on their own to make evidence-based design decisions. However,
the design process for In-Vehicle Information Systems (IVIS) lacks data-driven
support and effective tools for visualizing and analyzing user interaction
data. Therefore, we propose ICEBOAT, an interactive visualization tool tailored
to the needs of automotive UX experts to effectively and efficiently evaluate
driver interactions with IVISs. ICEBOAT visualizes telematics data collected
from production line vehicles, allowing UX experts to perform task-specific
analyses. Following a mixed methods User-centered design (UCD) approach, we
conducted an interview study (N=4) to extract the domain specific information
and interaction needs of automotive UX experts and used a co-design approach
(N=4) to develop an interactive analysis tool. Our evaluation (N=12) shows that
ICEBOAT enables UX experts to efficiently generate knowledge that facilitates
data-driven design decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published at the 15th International Conference on Automotive
  User Interfaces and Interactive Vehicular Applications (AutomotiveUI '23),
  September 18--22, 2023, Ingolstadt, Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Persuasive Robots with Social Power Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojgan Hashemian, Marta Couto, Samuel Mascarenhas, Ana Paiva, Pedro A. Santos, Rui Prada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can social power endow social robots with the capacity to persuade? This
paper represents our recent endeavor to design persuasive social robots. We
have designed and run three different user studies to investigate the
effectiveness of different bases of social power (inspired by French and
Raven's theory) on peoples' compliance to the requests of social robots. The
results show that robotic persuaders that exert social power (specifically from
expert, reward, and coercion bases) demonstrate increased ability to influence
humans. The first study provides a positive answer and shows that under the
same circumstances, people with different personalities prefer robots using a
specific social power base. In addition, social rewards can be useful in
persuading individuals. The second study suggests that by employing social
power, social robots are capable of persuading people objectively to select a
less desirable choice among others. Finally, the third study shows that the
effect of power on persuasion does not decay over time and might strengthen
under specific circumstances. Moreover, exerting stronger social power does not
necessarily lead to higher persuasion. Overall, we argue that the results of
these studies are relevant for designing human--robot-interaction scenarios
especially the ones aiming at behavioral change.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Review</span> of Automated Data Annotation Techniques in Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florenc Demrozi, Cristian Turetta, Fadi Al Machot, Graziano Pravadelli, Philipp H. Kindt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) has become one of the leading research
topics of the last decade. As sensing technologies have matured and their
economic costs have declined, a host of novel applications, e.g., in
healthcare, industry, sports, and daily life activities have become popular.
The design of HAR systems requires different time-consuming processing steps,
such as data collection, annotation, and model training and optimization. In
particular, data annotation represents the most labor-intensive and cumbersome
step in HAR, since it requires extensive and detailed manual work from human
annotators. Therefore, different methodologies concerning the automation of the
annotation procedure in HAR have been proposed. The annotation problem occurs
in different notions and scenarios, which all require individual solutions. In
this paper, we provide the first systematic review on data annotation
techniques for HAR. By grouping existing approaches into classes and providing
a taxonomy, our goal is to support the decision on which techniques can be
beneficially used in a given scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 5 figures, 20 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Hiding and Revealing: Exploring Effects of Visibility and Form of
  Interaction on the Witness Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alarith Uhde, Tim zum Hoff, Marc Hassenzahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our interactions with technology do not just shape our individual
experiences. They also affect people around us. Although previous research has
addressed such "witness" experiences, the actual effect of interaction design
on the witness experience remains largely unknown. In an online study (n =
407), we explored how witnesses perceive mid-air gesture-based interactions
with a hearing aid, using four video vignettes. We studied witnesses'
subjective visibility of manipulations and effects (following Reeves and
colleagues' taxonomy), perceived form of interaction, subjective experience,
and relationships between these measures. Although visibility patterns matched
the intended form, they did not lead to the supposed experience (i.e.,
"suspenseful" gestures did not lead to suspenseful experiences). The paper
illustrates gaps in current research about witness experiences, demonstrates
the need to overcome basic hiding/revealing profiles, and indicates a path
forward by focusing on aesthetic forms and experiences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Sector-Specific Influence and Response of AI Tools: A
  Critical <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hitesh Mohapatra, Soumya Ranjan Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI Tool is designed to generate human-like responses in natural language
conversations. Using deep learning techniques, AI Tool has been trained on a
diverse range of internet text to understand and generate coherent responses to
a wide array of prompts and questions. It can provide information, engage in
conversations, assist with tasks, and even offer creative suggestions. The
underlying technology behind AI Tool is a transformer neural network.
Transformers excel at capturing long-range dependencies in text, making them
well-suited for language-related tasks. AI Tool, has 175 billion parameters,
making it one of the largest and most powerful language models to date. AI Tool
has been trained on a massive corpus of text from the internet, which allows it
to leverage a broad understanding of language, general knowledge, and various
domains. While AI Tool aims to provide accurate and helpful responses, it may
occasionally produce incorrect or nonsensical answers. It's essential to
critically evaluate the information it provides and verify it from reliable
sources when necessary. This work presents an overview on AI Tool. It will
helps to research community and others users to understand the uses of AI Tool
and its interaction pattern.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Useful but Distracting: Keyword Highlights and Time-Synchronization in
  Captions for Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiona Draxler, Henrike Weingärtner, Maximiliane Windl, Albrecht Schmidt, Lewis L. Chuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Captions provide language learners with a scaffold for comprehension and
vocabulary acquisition. Past work has proposed several enhancements such as
keyword highlights for increased learning gains. However, little is known about
learners' experience with enhanced captions, although this is critical for
adoption in everyday life. We conducted a survey and focus group to elicit
learner preferences and requirements and implemented a processing pipeline for
enhanced captions with keyword highlights, time-synchronized keyword
highlights, and keyword captions. A subsequent online study (n = 49) showed
that time-synchronized keyword highlights were the preferred design for
learning but were perceived as too distracting to replace standard captions in
everyday viewing scenarios. We conclude that keyword highlights and
time-synchronization are suitable for integrating learning into an entertaining
everyday-life activity, but the design should be optimized to provide a more
seamless experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Learning based Upper-Limb Rehabilitation Training System with
  Collaborative Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Hong Lim, Kaibo He, Zeji Yi, Chen Hou, Chen Zhang, Yanan Sui, Luming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rehabilitation training for patients with motor disabilities usually requires
specialized devices in rehabilitation centers. Home-based multi-purpose
training would significantly increase treatment accessibility and reduce
medical costs. While it is unlikely to equip a set of rehabilitation robots at
home, we investigate the feasibility to use the general-purpose collaborative
robot for rehabilitation therapies. In this work, we developed a new system for
multi-purpose upper-limb rehabilitation training using a generic robot arm with
human motor feedback and preference. We integrated surface electromyography,
force/torque sensors, RGB-D cameras, and robot controllers with the Robot
Operating System to enable sensing, communication, and control of the system.
Imitation learning methods were adopted to imitate expert-provided training
trajectories which could adapt to subject capabilities to facilitate in-home
training. Our rehabilitation system is able to perform gross motor function and
fine motor skill training with a gripper-based end-effector. We simulated
system control in Gazebo and training effects (muscle activation level) in
OpenSim and evaluated its real performance with human subjects. For all the
subjects enrolled, our system achieved better training outcomes compared to
specialist-assisted rehabilitation under the same conditions. Our work
demonstrates the potential of utilizing collaborative robots for in-home motor
rehabilitation training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diagrammatization: Rationalizing with diagrammatic AI explanations for
  abductive-deductive reasoning on hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Y. Lim, Joseph P. Cahaly, Chester Y. F. Sng, Adam Chew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many visualizations have been developed for explainable AI (XAI), but they
often require further reasoning by users to interpret. We argue that XAI should
support diagrammatic and abductive reasoning for the AI to perform hypothesis
generation and evaluation to reduce the interpretability gap. We propose
Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii)
follow domain conventions, and iii) explain with diagrams visually or verbally.
We implemented DiagramNet for a clinical application to predict cardiac
diagnoses from heart auscultation, and explain with shape-based murmur
diagrams. In modeling studies, we found that DiagramNet not only provides
faithful murmur shape explanations, but also has better prediction performance
than baseline models. We further demonstrate the interpretability and
trustworthiness of diagrammatic explanations in a qualitative user study with
medical students, showing that clinically-relevant, diagrammatic explanations
are preferred over technical saliency map explanations. This work contributes
insights into providing domain-conventional abductive explanations for
user-centric XAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Employing Crowdsourcing for Enriching a Music Knowledge Base in Higher
  Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vassilis Lyberatos, Spyridon Kantarelis, Eirini Kaldeli, Spyros Bekiaris, Panagiotis Tzortzis, Orfeas Menis - Mastromichalakis, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the methodology followed and the lessons learned from
employing crowdsourcing techniques as part of a homework assignment involving
higher education students of computer science. Making use of a platform that
supports crowdsourcing in the cultural heritage domain students were solicited
to enrich the metadata associated with a selection of music tracks. The results
of the campaign were further analyzed and exploited by students through the use
of semantic web technologies. In total, 98 students participated in the
campaign, contributing more than 6400 annotations concerning 854 tracks. The
process also led to the creation of an openly available annotated dataset,
which can be useful for machine learning models for music tagging. The
campaign's results and the comments gathered through an online survey enable us
to draw some useful insights about the benefits and challenges of integrating
crowdsourcing into computer science curricula and how this can enhance
students' engagement in the learning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented in The 4th International Conference on Artificial
  Intelligence in Education Technology (AIET 2023), Berlin, Germany, 31 June-2
  July 2023. For The GitHub code for the created music dataset, see
  https://github.com/vaslyb/MusicCrowd</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">80</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for
  Test-Time Policy Adaptation <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andi Peng, Aviv Netanyahu, Mark Ho, Tianmin Shu, Andreea Bobu, Julie Shah, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policies often fail due to distribution shift -- changes in the state and
reward that occur when a policy is deployed in new environments. Data
augmentation can increase robustness by making the model invariant to
task-irrelevant changes in the agent's observation. However, designers don't
know which concepts are irrelevant a priori, especially when different end
users have different preferences about how the task is performed. We propose an
interactive framework to leverage feedback directly from the user to identify
personalized task-irrelevant concepts. Our key idea is to generate
counterfactual demonstrations that allow users to quickly identify possible
task-relevant and irrelevant concepts. The knowledge of task-irrelevant
concepts is then used to perform data augmentation and thus obtain a policy
adapted to personalized user objectives. We present experiments validating our
framework on discrete and continuous control tasks with real human users. Our
method (1) enables users to better understand agent failure, (2) reduces the
number of demonstrations required for fine-tuning, and (3) aligns the agent to
individual user task preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Budgeting Counterfactual for Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Liu, Pratik Chaudhari, Rasool Fakoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The main challenge of offline reinforcement learning, where data is limited,
arises from a sequence of counterfactual reasoning dilemmas within the realm of
potential actions: What if we were to choose a different course of action?
These circumstances frequently give rise to extrapolation errors, which tend to
accumulate exponentially with the problem horizon. Hence, it becomes crucial to
acknowledge that not all decision steps are equally important to the final
outcome, and to budget the number of counterfactual decisions a policy make in
order to control the extrapolation. Contrary to existing approaches that use
regularization on either the policy or value function, we propose an approach
to explicitly bound the amount of out-of-distribution actions during training.
Specifically, our method utilizes dynamic programming to decide where to
extrapolate and where not to, with an upper bound on the decisions different
from behavior policy. It balances between the potential for improvement from
taking out-of-distribution actions and the risk of making errors due to
extrapolation. Theoretically, we justify our method by the constrained
optimality of the fixed point solution to our $Q$ updating rules. Empirically,
we show that the overall performance of our method is better than the
state-of-the-art offline RL methods on tasks in the widely-used D4RL
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch n' Pack: NaViT, a Vision <span class="highlight-title">Transformer</span> for any Aspect Ratio and
  Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Lučić, Neil Houlsby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ubiquitous and demonstrably suboptimal choice of resizing images to a
fixed resolution before processing them with computer vision models has not yet
been successfully challenged. However, models such as the Vision Transformer
(ViT) offer flexible sequence-based modeling, and hence varying input sequence
lengths. We take advantage of this with NaViT (Native Resolution ViT) which
uses sequence packing during training to process inputs of arbitrary
resolutions and aspect ratios. Alongside flexible model usage, we demonstrate
improved training efficiency for large-scale supervised and contrastive
image-text pretraining. NaViT can be efficiently transferred to standard tasks
such as image and video classification, object detection, and semantic
segmentation and leads to improved results on robustness and fairness
benchmarks. At inference time, the input resolution flexibility can be used to
smoothly navigate the test-time cost-performance trade-off. We believe that
NaViT marks a departure from the standard, CNN-designed, input and modelling
pipeline used by most computer vision models, and represents a promising
direction for ViTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruction Mining: High-Quality Instruction Data Selection for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Cao, Yanbin Kang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models typically undergo two training stages, pretraining and
finetuning. Despite that large-scale pretraining endows the model with strong
capabilities to generate natural language responses, these pretrained models
can still fail to understand human instructions at times. To enhance language
models' ability of interpreting and responding to instructions, instruction
finetuning has emerged as a critical method in this area. Recent studies found
that large language models can be finetuned to perform well even with a small
amount of high-quality instruction-following data. However, the selection of
high-quality datasets for finetuning language models still lacks clear
guidelines to follow. In this paper, we propose InstructMining, a linear rule
for evaluating instruction-following data quality. We formulate InstructMining
using specific natural language indicators. To investigate the relationship
between data quality and these indicators, we further conduct extensive
finetuning experiments. The experiment results are then applied to estimating
parameters in InstructMining. To further investigate its performance, we use
InstructMining to select high-quality data from unseen datasets. Results
demonstrate that InstructMining can help select relatively high-quality samples
from various instruction-following datasets. Compared to models finetuned on
unfiltered datasets, models finetuned on InstructMining selected datasets
perform better on 42.5% cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. 12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSSE: a drone swarm search environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Castanares, Luis F. S. Carrete, Enrico F. Damiani, Leonardo D. M. de Abreu, José Fernando B. Brancalion, Fabrício J. Barth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Drone Swarm Search project is an environment, based on PettingZoo, that
is to be used in conjunction with multi-agent (or single-agent) reinforcement
learning algorithms. It is an environment in which the agents (drones), have to
find the targets (shipwrecked people). The agents do not know the position of
the target and do not receive rewards related to their own distance to the
target(s). However, the agents receive the probabilities of the target(s) being
in a certain cell of the map. The aim of this project is to aid in the study of
reinforcement learning algorithms that require dynamic probabilities as inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing different Log Bases For Vector Model Weighting Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamel Assaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval systems retrieves relevant documents based on a query
submitted by the user. The documents are initially indexed and the words in the
documents are assigned weights using a weighting technique called TFIDF which
is the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF
represents the number of occurrences of a term in a document. IDF measures
whether the term is common or rare across all documents. It is computed by
dividing the total number of documents in the system by the number of documents
containing the term and then computing the logarithm of the quotient. By
default, we use base 10 to calculate the logarithm. In this paper, we are going
to test this weighting technique by using a range of log bases from 0.1 to
100.0 to calculate the IDF. Testing different log bases for vector model
weighting technique is to highlight the importance of understanding the
performance of the system at different weighting values. We use the documents
of MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled
explicitly for experiments in data information retrieval systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, vector model, logarithms, tfidf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathalia Nascimento, Paulo Alencar, Donald Cowan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomic computing, self-adaptation has been proposed as a fundamental
paradigm to manage the complexity of multiagent systems (MASs). This achieved
by extending a system with support to monitor and adapt itself to achieve
specific concerns of interest. Communication in these systems is key given that
in scenarios involving agent interaction, it enhances cooperation and reduces
coordination challenges by enabling direct, clear information exchange.
However, improving the expressiveness of the interaction communication with
MASs is not without challenges. In this sense, the interplay between
self-adaptive systems and effective communication is crucial for future MAS
advancements. In this paper, we propose the integration of large language
models (LLMs) such as GPT-based technologies into multiagent systems. We anchor
our methodology on the MAPE-K model, which is renowned for its robust support
in monitoring, analyzing, planning, and executing system adaptations in
response to dynamic environments. We also present a practical illustration of
the proposed approach, in which we implement and assess a basic MAS-based
application. The approach significantly advances the state-of-the-art of
self-adaptive systems by proposing a new paradigm for MAS self-adaptation of
autonomous systems based on LLM capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times
  and Location Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gengyuan Zhang, Yurui Zhang, Kerui Zhang, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) are expected to be capable of reasoning with
commonsense knowledge as human beings. One example is that humans can reason
where and when an image is taken based on their knowledge. This makes us wonder
if, based on visual cues, Vision-Language Models that are pre-trained with
large-scale image-text resources can achieve and even outperform human's
capability in reasoning times and location. To address this question, we
propose a two-stage \recognition\space and \reasoning\space probing task,
applied to discriminative and generative VLMs to uncover whether VLMs can
recognize times and location-relevant features and further reason about it. To
facilitate the investigation, we introduce WikiTiLo, a well-curated image
dataset compromising images with rich socio-cultural cues. In the extensive
experimental studies, we find that although VLMs can effectively retain
relevant features in visual encoders, they still fail to make perfect
reasoning. We will release our dataset and codes to facilitate future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Models for Physiological Signals: A Systematic
  Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nour Neifar, Afef Mdhaffar, Achraf Ben-Hamadou, Mohamed Jmaiel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a systematic literature review on deep generative
models for physiological signals, particularly electrocardiogram,
electroencephalogram, photoplethysmogram and electromyogram. Compared to the
existing review papers, we present the first review that summarizes the recent
state-of-the-art deep generative models. By analysing the state-of-the-art
research related to deep generative models along with their main applications
and challenges, this review contributes to the overall understanding of these
models applied to physiological signals. Additionally, by highlighting the
employed evaluation protocol and the most used physiological databases, this
review facilitates the assessment and benchmarking of deep generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paper under review, 34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reflective Hybrid Intelligence for Meaningful Human Control in
  Decision-Support Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catholijn M. Jonker, Luciano Cavalcante Siebert, Pradeep K. Murukannaiah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing capabilities and pervasiveness of AI systems, societies must
collectively choose between reduced human autonomy, endangered democracies and
limited human rights, and AI that is aligned to human and social values,
nurturing collaboration, resilience, knowledge and ethical behaviour. In this
chapter, we introduce the notion of self-reflective AI systems for meaningful
human control over AI systems. Focusing on decision support systems, we propose
a framework that integrates knowledge from psychology and philosophy with
formal reasoning methods and machine learning approaches to create AI systems
responsive to human values and social norms. We also propose a possible
research approach to design and develop self-reflective capability in AI
systems. Finally, we argue that self-reflective AI systems can lead to
self-reflective hybrid systems (human + AI), thus increasing meaningful human
control and empowering human moral reasoning by providing comprehensible
information and insights on possible human moral blind spots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Research Handbook on Meaningful Human
  Control of Artificial Intelligence Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maneuver Decision-Making Through Automatic Curriculum Reinforcement
  Learning Without Handcrafted Reward functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Hong-Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maneuver decision-making is the core of unmanned combat aerial vehicle for
autonomous air combat. To solve this problem, we propose an automatic
curriculum reinforcement learning method, which enables agents to learn
effective decisions in air combat from scratch. The range of initial states are
used for distinguishing curricula of different difficulty levels, thereby
maneuver decision is divided into a series of sub-tasks from easy to difficult,
and test results are used to change sub-tasks. As sub-tasks change, agents
gradually learn to complete a series of sub-tasks from easy to difficult,
enabling them to make effective maneuvering decisions to cope with various
states without the need to spend effort designing reward functions. The
ablation studied show that the automatic curriculum learning proposed in this
article is an essential component for training through reinforcement learning,
namely, agents cannot complete effective decisions without curriculum learning.
Simulation experiments show that, after training, agents are able to make
effective decisions given different states, including tracking, attacking and
escaping, which are both rational and interpretable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SayPlan: Grounding Large Language Models using 3D Scene Graphs for
  Scalable Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic
search for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an iterative replanning pipeline that
refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors, 36 rooms and
140 objects, and show that our approach is capable of grounding large-scale,
long-horizon task plans from abstract, and natural language instruction for a
mobile manipulator robot to execute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for CoRL 2023. Project page can be found here:
  https://sayplan.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Bottom-Up Interactive Constraint Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimos Tsouros, Senne Berden, Tias Guns
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constraint Acquisition (CA) systems can be used to assist in the modeling of
constraint satisfaction problems. In (inter)active CA, the system is given a
set of candidate constraints and posts queries to the user with the goal of
finding the right constraints among the candidates. Current interactive CA
algorithms suffer from at least two major bottlenecks. First, in order to
converge, they require a large number of queries to be asked to the user.
Second, they cannot handle large sets of candidate constraints, since these
lead to large waiting times for the user. For this reason, the user must have
fairly precise knowledge about what constraints the system should consider. In
this paper, we alleviate these bottlenecks by presenting two novel methods that
improve the efficiency of CA. First, we introduce a bottom-up approach named
GrowAcq that reduces the maximum waiting time for the user and allows the
system to handle much larger sets of candidate constraints. It also reduces the
total number of queries for problems in which the target constraint network is
not sparse. Second, we propose a probability-based method to guide query
generation and show that it can significantly reduce the number of queries
required to converge. We also propose a new technique that allows the use of
openly accessible CP solvers in query generation, removing the dependency of
existing methods on less well-maintained custom solvers that are not publicly
available. Experimental results show that our proposed methods outperform
state-of-the-art CA methods, reducing the number of queries by up to 60%. Our
methods work well even in cases where the set of candidate constraints is 50
times larger than the ones commonly used in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hierarchical Interactive Multi-Object Search for Mobile
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Schmalstieg, Daniel Honerkamp, Tim Welschehold, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing object-search approaches enable robots to search through free
pathways, however, robots operating in unstructured human-centered environments
frequently also have to manipulate the environment to their needs. In this
work, we introduce a novel interactive multi-object search task in which a
robot has to open doors to navigate rooms and search inside cabinets and
drawers to find target objects. These new challenges require combining
manipulation and navigation skills in unexplored environments. We present
HIMOS, a hierarchical reinforcement learning approach that learns to compose
exploration, navigation, and manipulation skills. To achieve this, we design an
abstract high-level action space around a semantic map memory and leverage the
explored environment as instance navigation points. We perform extensive
experiments in simulation and the real-world that demonstrate that HIMOS
effectively transfers to new environments in a zero-shot manner. It shows
robustness to unseen subpolicies, failures in their execution, and different
robot kinematics. These capabilities open the door to a wide range of
downstream tasks across embodied AI and real-world use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeFormer: a Semi-Supervised <span class="highlight-title">Transformer</span>-based Framework for Tree
  Counting from a Single High Resolution Image <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Amini Amirkolaee, Miaojing Shi, Mark Mulligan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic tree density estimation and counting using single aerial and
satellite images is a challenging task in photogrammetry and remote sensing,
yet has an important role in forest management. In this paper, we propose the
first semisupervised transformer-based framework for tree counting which
reduces the expensive tree annotations for remote sensing images. Our method,
termed as TreeFormer, first develops a pyramid tree representation module based
on transformer blocks to extract multi-scale features during the encoding
stage. Contextual attention-based feature fusion and tree density regressor
modules are further designed to utilize the robust features from the encoder to
estimate tree density maps in the decoder. Moreover, we propose a pyramid
learning strategy that includes local tree density consistency and local tree
count ranking losses to utilize unlabeled images into the training process.
Finally, the tree counter token is introduced to regulate the network by
computing the global tree counts for both labeled and unlabeled images. Our
model was evaluated on two benchmark tree counting datasets, Jiangsu, and
Yosemite, as well as a new dataset, KCL-London, created by ourselves. Our
TreeFormer outperforms the state of the art semi-supervised methods under the
same setting and exceeds the fully-supervised methods using the same number of
labeled images. The codes and datasets are available at
https://github.com/HAAClassic/TreeFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantitative CLTs in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Favaro, Boris Hanin, Domenico Marinucci, Ivan Nourdin, Giovanni Peccati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show, both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent
depending on the metric used to measure discrepancy. Our bounds are stronger in
terms of their dependence on network width than any previously available in the
literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VELMA: Verbalization Embodiment of LLM Agents for Vision and Language
  Navigation in Street View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visualization for Multivariate Gaussian Anomaly Detection in Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao P C Bertoldo, David Arrustico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a simplified variation of the PaDiM (Pixel-Wise Anomaly
Detection through Instance Modeling) method for anomaly detection in images,
fitting a single multivariate Gaussian (MVG) distribution to the feature
vectors extracted from a backbone convolutional neural network (CNN) and using
their Mahalanobis distance as the anomaly score. We introduce an intermediate
step in this framework by applying a whitening transformation to the feature
vectors, which enables the generation of heatmaps capable of visually
explaining the features learned by the MVG. The proposed technique is evaluated
on the MVTec-AD dataset, and the results show the importance of visual model
validation, providing insights into issues in this framework that were
otherwise invisible. The visualizations generated for this paper are publicly
available at https://doi.org/10.5281/zenodo.7937978.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures, accepted to 2023 Twelfth International Conference
  on Image Processing Theory, Tools and Applications (IPTA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An OOD Multi-Task Perspective for Link Prediction with New Relation
  Types and Nodes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhou, Beatrice Bevilacqua, Bruno Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of inductive link prediction in (discrete) attributed multigraphs
infers missing attributed links (relations) between nodes in new test
multigraphs. Traditional relational learning methods face the challenge of
limited generalization to OOD test multigraphs containing both novel nodes and
novel relation types not seen in training. Recently, under the only assumption
that all relation types share the same structural predictive patterns (single
task), Gao et al. (2023) proposed an OOD link prediction method using the
theoretical concept of double exchangeability (for nodes & relation types), in
contrast to the (single) exchangeability (only for nodes) used to design Graph
Neural Networks (GNNs). In this work we further extend the double
exchangeability concept to multi-task double exchangeability, where we define
link prediction in attributed multigraphs that can have distinct and
potentially conflicting predictive patterns for different sets of relation
types (multiple tasks). Our empirical results on real-world datasets
demonstrate that our approach can effectively generalize to entirely new
relation types in test, without access to additional information, yielding
significant performance improvements over existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Generated Imagery: A New Era for the `Readymade' 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amy Smith, Michael Cook
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the term `art' defies any concrete definition, this paper aims to
examine how digital images produced by generative AI systems, such as
Midjourney, have come to be so regularly referred to as such. The discourse
around the classification of AI-generated imagery as art is currently somewhat
homogeneous, lacking the more nuanced aspects that would apply to more
traditional modes of artistic media production. This paper aims to bring
important philosophical considerations to the surface of the discussion around
AI-generated imagery in the context of art. We employ existing philosophical
frameworks and theories of language to suggest that some AI-generated imagery,
by virtue of its visual properties within these frameworks, can be presented as
`readymades' for consideration as art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Effective and Efficient Time-aware Entity Alignment Framework via
  Two-aspect Three-view Label Propagation <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Cai, Xin Mao, Youshao Xiao, Changxu Wu, Man Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) aims to find the equivalent entity pairs between
different knowledge graphs (KGs), which is crucial to promote knowledge fusion.
With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA)
methods appear to enhance EA. Existing TEA models are based on Graph Neural
Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is
difficult to transfer them to large-scale TKGs due to the scalability issue of
GNN. In this paper, we propose an effective and efficient non-neural EA
framework between TKGs, namely LightTEA, which consists of four essential
components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity
with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative
Learning. All of these modules work together to improve the performance of EA
while reducing the time consumption of the model. Extensive experiments on
public datasets indicate that our proposed model significantly outperforms the
SOTA methods for EA between TKGs, and the time consumed by LightTEA is only
dozens of seconds at most, no more than 10% of the most efficient TEA method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s in Reinforcement Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Agarwal, Aamer Abdul Rahman, Pierre-Luc St-Charles, Simon J. D. Prince, Samira Ebrahimi Kahou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have significantly impacted domains like natural language
processing, computer vision, and robotics, where they improve performance
compared to other neural networks. This survey explores how transformers are
used in reinforcement learning (RL), where they are seen as a promising
solution for addressing challenges such as unstable training, credit
assignment, lack of interpretability, and partial observability. We begin by
providing a brief domain overview of RL, followed by a discussion on the
challenges of classical RL algorithms. Next, we delve into the properties of
the transformer and its variants and discuss the characteristics that make them
well-suited to address the challenges inherent in RL. We examine the
application of transformers to various aspects of RL, including representation
learning, transition and reward function modeling, and policy optimization. We
also discuss recent research that aims to enhance the interpretability and
efficiency of transformers in RL, using visualization techniques and efficient
training strategies. Often, the transformer architecture must be tailored to
the specific needs of a given application. We present a broad overview of how
transformers have been adapted for several applications, including robotics,
medicine, language modeling, cloud computing, and combinatorial optimization.
We conclude by discussing the limitations of using transformers in RL and
assess their potential for catalyzing future breakthroughs in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion
  Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, Juho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale image generation models, with impressive quality made possible by
the vast amount of data available on the Internet, raise social concerns that
these models may generate harmful or copyrighted content. The biases and
harmfulness arise throughout the entire training process and are hard to
completely remove, which have become significant hurdles to the safe deployment
of these models. In this paper, we propose a method called SDD to prevent
problematic content generation in text-to-image diffusion models. We
self-distill the diffusion model to guide the noise estimate conditioned on the
target removal concept to match the unconditional one. Compared to the previous
methods, our method eliminates a much greater proportion of harmful content
from the generated images without degrading the overall image quality.
Furthermore, our method allows the removal of multiple concepts at once,
whereas previous works are limited to removing a single concept at a time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures, ICML 2023 Workshop on Challenges in Deployable
  Generative AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxPoser: Composable 3D Value Maps for Robotic Manipulation with
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a visual-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Project website: https://voxposer.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Giving Robots a Hand: Learning Generalizable Manipulation with
  Eye-in-Hand Human Video Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moo Jin Kim, Jiajun Wu, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eye-in-hand cameras have shown promise in enabling greater sample efficiency
and generalization in vision-based robotic manipulation. However, for robotic
imitation, it is still expensive to have a human teleoperator collect large
amounts of expert demonstrations with a real robot. Videos of humans performing
tasks, on the other hand, are much cheaper to collect since they eliminate the
need for expertise in robotic teleoperation and can be quickly captured in a
wide range of scenarios. Therefore, human video demonstrations are a promising
data source for learning generalizable robotic manipulation policies at scale.
In this work, we augment narrow robotic imitation datasets with broad unlabeled
human video demonstrations to greatly enhance the generalization of eye-in-hand
visuomotor policies. Although a clear visual domain gap exists between human
and robot data, our framework does not need to employ any explicit domain
adaptation method, as we leverage the partial observability of eye-in-hand
cameras as well as a simple fixed image masking scheme. On a suite of eight
real-world tasks involving both 3-DoF and 6-DoF robot arm control, our method
improves the success rates of eye-in-hand manipulation policies by 58%
(absolute) on average, enabling robots to generalize to both new environment
configurations and new tasks that are unseen in the robot demonstration data.
See video results at https://giving-robots-a-hand.github.io/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures, project webpage at
  https://giving-robots-a-hand.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Reconciling the Trade-off between Prediction Accuracy and
  Earliness in Prescriptive Business Process Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Metzger, Tristan Kley, Aristide Rothweiler, Klaus Pohl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prescriptive business process monitoring provides decision support to process
managers on when and how to adapt an ongoing business process to prevent or
mitigate an undesired process outcome. We focus on the problem of automatically
reconciling the trade-off between prediction accuracy and prediction earliness
in determining when to adapt. Adaptations should happen sufficiently early to
provide enough lead time for the adaptation to become effective. However,
earlier predictions are typically less accurate than later predictions. This
means that acting on less accurate predictions may lead to unnecessary
adaptations or missed adaptations.
  Different approaches were presented in the literature to reconcile the
trade-off between prediction accuracy and earliness. So far, these approaches
were compared with different baselines, and evaluated using different data sets
or even confidential data sets. This limits the comparability and replicability
of the approaches and makes it difficult to choose a concrete approach in
practice.
  We perform a comparative evaluation of the main alternative approaches for
reconciling the trade-off between prediction accuracy and earliness. Using four
public real-world event log data sets and two types of prediction models, we
assess and compare the cost savings of these approaches. The experimental
results indicate which criteria affect the effectiveness of an approach and
help us state initial recommendations for the selection of a concrete approach
in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiRP: Learning Robot Generalized Bimanual Coordination using Relative
  Parameterization Method on Human Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjia Liu, Hengyi Sim, Chenzui Li, Fei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human bimanual manipulation can perform more complex tasks than a simple
combination of two single arms, which is credited to the spatio-temporal
coordination between the arms. However, the description of bimanual
coordination is still an open topic in robotics. This makes it difficult to
give an explainable coordination paradigm, let alone applied to robotics. In
this work, we divide the main bimanual tasks in human daily activities into two
types: leader-follower and synergistic coordination. Then we propose a relative
parameterization method to learn these types of coordination from human
demonstration. It represents coordination as Gaussian mixture models from
bimanual demonstration to describe the change in the importance of coordination
throughout the motions by probability. The learned coordinated representation
can be generalized to new task parameters while ensuring spatio-temporal
coordination. We demonstrate the method using synthetic motions and human
demonstration data and deploy it to a humanoid robot to perform a generalized
bimanual coordination motion. We believe that this easy-to-use bimanual
learning from demonstration (LfD) method has the potential to be used as a data
augmentation plugin for robot large manipulation model training. The
corresponding codes are open-sourced in https://github.com/Skylark0924/Rofunc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures. Accepted by IEEE Conference on Decision and
  Control (IEEE CDC 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New <span class="highlight-title">Dataset</span> and Comparative Study for Aphid Cluster Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Cuncong Zhong, Bo Luo, Ivan Grijalva Teran, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aphids are one of the main threats to crops, rural families, and global food
security. Chemical pest control is a necessary component of crop production for
maximizing yields, however, it is unnecessary to apply the chemical approaches
to the entire fields in consideration of the environmental pollution and the
cost. Thus, accurately localizing the aphid and estimating the infestation
level is crucial to the precise local application of pesticides. Aphid
detection is very challenging as each individual aphid is really small and all
aphids are crowded together as clusters. In this paper, we propose to estimate
the infection level by detecting aphid clusters. We have taken millions of
images in the sorghum fields, manually selected 5,447 images that contain
aphids, and annotated each aphid cluster in the image. To use these images for
machine learning models, we crop the images into patches and created a labeled
dataset with over 151,000 image patches. Then, we implement and compare the
performance of four state-of-the-art object detection models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reading Radiology Imaging Like The Radiologist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Close-up View synthesis by Interpolating Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Bai, Ze Wang, Lu Yang, Hong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The virtual viewpoint is perceived as a new technique in virtual navigation,
as yet not supported due to the lack of depth information and obscure camera
parameters. In this paper, a method for achieving close-up virtual view is
proposed and it only uses optical flow to build parallax effects to realize
pseudo 3D projection without using depth sensor. We develop a bidirectional
optical flow method to obtain any virtual viewpoint by proportional
interpolation of optical flow. Moreover, with the ingenious application of the
optical-flow-value, we achieve clear and visual-fidelity magnified results
through lens stretching in any corner, which overcomes the visual distortion
and image blur through viewpoint magnification and transition in Google Street
View system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stability Guarantees for Feature Attributions with Multiplicative
  Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Xue, Rajeev Alur, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanation methods for machine learning models tend to not provide any
formal guarantees and may not reflect the underlying decision-making process.
In this work, we analyze stability as a property for reliable feature
attribution methods. We prove that relaxed variants of stability are guaranteed
if the model is sufficiently Lipschitz with respect to the masking of features.
To achieve such a model, we develop a smoothing method called Multiplicative
Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard
smoothing techniques and can be integrated with any classifier and feature
attribution method. We evaluate MuS on vision and language models with a
variety of feature attribution methods, such as LIME and SHAP, and demonstrate
that MuS endows feature attributions with non-trivial stability guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PID-Inspired Inductive Biases for Deep Reinforcement Learning in
  Partially Observable Control Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Char, Jeff Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (RL) has shown immense potential for learning to
control systems through data alone. However, one challenge deep RL faces is
that the full state of the system is often not observable. When this is the
case, the policy needs to leverage the history of observations to infer the
current state. At the same time, differences between the training and testing
environments makes it critical for the policy not to overfit to the sequence of
observations it sees at training time. As such, there is an important balancing
act between having the history encoder be flexible enough to extract relevant
information, yet be robust to changes in the environment. To strike this
balance, we look to the PID controller for inspiration. We assert the PID
controller's success shows that only summing and differencing are needed to
accumulate information over time for many control tasks. Following this
principle, we propose two architectures for encoding history: one that directly
uses PID features and another that extends these core ideas and can be used in
arbitrary control tasks. When compared with prior approaches, our encoders
produce policies that are often more robust and achieve better performance on a
variety of tracking tasks. Going beyond tracking tasks, our policies achieve
1.7x better performance on average over previous state-of-the-art methods on a
suite of high dimensional control tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ecosystem-level Analysis of Deployed Machine Learning Reveals
  Homogeneous Outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Toups, Rishi Bommasani, Kathleen A. Creel, Sarah H. Bana, Dan Jurafsky, Percy Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is traditionally studied at the model level: researchers
measure and improve the accuracy, robustness, bias, efficiency, and other
dimensions of specific models. In practice, the societal impact of machine
learning is determined by the surrounding context of machine learning
deployments. To capture this, we introduce ecosystem-level analysis: rather
than analyzing a single model, we consider the collection of models that are
deployed in a given context. For example, ecosystem-level analysis in hiring
recognizes that a job candidate's outcomes are not only determined by a single
hiring algorithm or firm but instead by the collective decisions of all the
firms they applied to. Across three modalities (text, images, speech) and 11
datasets, we establish a clear trend: deployed machine learning is prone to
systemic failure, meaning some users are exclusively misclassified by all
models available. Even when individual models improve at the population level
over time, we find these improvements rarely reduce the prevalence of systemic
failure. Instead, the benefits of these improvements predominantly accrue to
individuals who are already correctly classified by other models. In light of
these trends, we consider medical imaging for dermatology where the costs of
systemic failure are especially high. While traditional analyses reveal racial
performance disparities for both models and humans, ecosystem-level analysis
reveals new forms of racial disparity in model predictions that do not present
in human predictions. These examples demonstrate ecosystem-level analysis has
unique strengths for characterizing the societal impact of machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>All code is available at
  https://github.com/rishibommasani/EcosystemLevelAnalysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for
  Human-in-the-Loop Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Zhao, Mojtaba Taherisadr, Salma Elmalaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving fairness in sequential-decision making systems within
Human-in-the-Loop (HITL) environments is a critical concern, especially when
multiple humans with different behavior and expectations are affected by the
same adaptation decisions in the system. This human variability factor adds
more complexity since policies deemed fair at one point in time may become
discriminatory over time due to variations in human preferences resulting from
inter- and intra-human variability. This paper addresses the fairness problem
from an equity lens, considering human behavior variability, and the changes in
human preferences over time. We propose FAIRO, a novel algorithm for
fairness-aware sequential-decision making in HITL adaptation, which
incorporates these notions into the decision-making process. In particular,
FAIRO decomposes this complex fairness task into adaptive sub-tasks based on
individual human preferences through leveraging the Options reinforcement
learning framework. We design FAIRO to generalize to three types of HITL
application setups that have the shared adaptation decision problem.
Furthermore, we recognize that fairness-aware policies can sometimes conflict
with the application's utility. To address this challenge, we provide a
fairness-utility tradeoff in FAIRO, allowing system designers to balance the
objectives of fairness and utility based on specific application requirements.
Extensive evaluations of FAIRO on the three HITL applications demonstrate its
generalizability and effectiveness in promoting fairness while accounting for
human variability. On average, FAIRO can improve fairness compared with other
methods across all three applications by 35.36%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Misclassification in Automated Content Analysis Causes Bias in
  Regression. Can We Fix It? Yes We Can! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan TeBlunthuis, Valerie Hase, Chung-Hong Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated classifiers (ACs), often built via supervised machine learning
(SML), can categorize large, statistically powerful samples of data ranging
from text to images and video, and have become widely popular measurement
devices in communication science and related fields. Despite this popularity,
even highly accurate classifiers make errors that cause misclassification bias
and misleading results in downstream analyses-unless such analyses account for
these errors. As we show in a systematic literature review of SML applications,
communication scholars largely ignore misclassification bias. In principle,
existing statistical methods can use "gold standard" validation data, such as
that created by human annotators, to correct misclassification bias and produce
consistent estimates. We introduce and test such methods, including a new
method we design and implement in the R package misclassificationmodels, via
Monte Carlo simulations designed to reveal each method's limitations, which we
also release. Based on our results, we recommend our new error correction
method as it is versatile and efficient. In sum, automated classifiers, even
those below common accuracy standards or making systematic misclassifications,
can be useful for measurement with careful study design and appropriate error
correction methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 page, 21 Figures, Top Paper Award from the 2023 Annual Meeting of
  The International Communication Association Computational Methods Division</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently-Verifiable Strong Uniquely Solvable Puzzles and Matrix
  Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Anderson, Vu Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We advance the Cohn-Umans framework for developing fast matrix multiplication
algorithms. We introduce, analyze, and search for a new subclass of strong
uniquely solvable puzzles (SUSP), which we call simplifiable SUSPs. We show
that these puzzles are efficiently verifiable, which remains an open question
for general SUSPs. We also show that individual simplifiable SUSPs can achieve
the same strength of bounds on the matrix multiplication exponent $\omega$ that
infinite families of SUSPs can. We report on the construction, by computer
search, of larger SUSPs than previously known for small width. This, combined
with our tighter analysis, strengthens the upper bound on the matrix
multiplication exponent from $2.66$ to $2.505$ obtainable via this
computational approach, and nears the results of the handcrafted constructions
of Cohn et al.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Train No Gain: Revisiting Efficient Training Algorithms For
  <span class="highlight-title">Transformer</span>-based Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, Matt J. Kusner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Large Language Models for Biomedical Knowledge Extraction: A
  Case Study on Adverse Drug Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Gu, Sheng Zhang, Naoto Usuyama, Yonas Woldesenbet, Cliff Wong, Praneeth Sanapathi, Mu Wei, Naveen Valluri, Erika Strandberg, Tristan Naumann, Hoifung Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), such as GPT-4, have demonstrated remarkable
capabilities across a wide range of tasks, including health applications. In
this paper, we study how LLMs can be used to scale biomedical knowledge
curation. We find that while LLMs already possess decent competency in
structuring biomedical text, by distillation into a task-specific student model
through self-supervised learning, substantial gains can be attained over
out-of-box LLMs, with additional advantages such as cost, efficiency, and
white-box model access.
  We conduct a case study on adverse drug event (ADE) extraction, which is an
important area for improving care. On standard ADE extraction evaluation, a
GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised
state-of-the-art models without using any labeled data. Despite being over
1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by
over 6 absolute points in F1 and GPT-4 by over 5 absolute points.
  Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT)
and ADE extraction architecture shed light on best practice for biomedical
knowledge extraction. Similar gains were attained by distillation for other
standard biomedical knowledge extraction tasks such as gene-disease
associations and protected health information, further illustrating the promise
of this approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Behavior Trees from Goal-Oriented LTLf Formulas <span class="chip">AAMAS
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aadesh Neupane, Michael A. Goodrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal logic can be used to formally specify autonomous agent goals, but
synthesizing planners that guarantee goal satisfaction can be computationally
prohibitive. This paper shows how to turn goals specified using a subset of
finite trace Linear Temporal Logic (LTL) into a behavior tree (BT) that
guarantees that successful traces satisfy the LTL goal. Useful LTL formulas for
achievement goals can be derived using achievement-oriented task mission
grammars, leading to missions made up of tasks combined using LTL operators.
Constructing BTs from LTL formulas leads to a relaxed behavior synthesis
problem in which a wide range of planners can implement the action nodes in the
BT. Importantly, any successful trace induced by the planners satisfies the
corresponding LTL formula. The usefulness of the approach is demonstrated in
two ways: a) exploring the alignment between two planners and LTL goals, and b)
solving a sequential key-door problem for a Fetch robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as "Most Visionary Paper" in Autonomous Robots and
  Multirobot Systems (ARMS) 2023 workshop affiliated with the 22nd
  International Conference on Autonomous Agents and Multiagent Systems (AAMAS
  2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Answer Set Programming Templates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Alviano, Giovambattista Ianni, Francesco Pacenza, Jessica Zangari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In imperative programming, the Domain-Driven Design methodology helps in
coping with the complexity of software development by materializing in code the
invariants of a domain of interest. Code is cleaner and more secure because any
implicit assumption is removed in favor of invariants, thus enabling a fail
fast mindset and the immediate reporting of unexpected conditions. This article
introduces a notion of template for Answer Set Programming that, in addition to
the don't repeat yourself principle, enforces locality of some predicates by
means of a simple naming convention. Local predicates are mapped to the usual
global namespace adopted by mainstream engines, using universally unique
identifiers to avoid name clashes. This way, local predicates can be used to
enforce invariants on the expected outcome of a template in a possibly empty
context of application, independently by other rules that can be added to such
a context. Template applications transpiled this way can be processed by
mainstream engines and safely shared with other knowledge designers, even when
they have zero knowledge of templates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FDAPT: Federated Domain-adaptive <span class="highlight-title">Pre-train</span>ing for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lekang Jiang, Filip Svoboda, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL)
can enhance model adaptation by leveraging more sensitive and distributed data
while preserving data privacy. However, few studies have focused on this
method. Therefore, we conduct the first comprehensive empirical study to
evaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). We
demonstrate that FDAPT can maintain competitive downstream task performance to
the centralized baseline in both IID and non-IID situations. Furthermore, we
propose a novel algorithm, Frozen Federated Domain-adaptive Pre-training
(FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average and
exhibits similar downstream task performance to standard FDAPT, with general
performance fluctuations remaining less than 1%. Finally, through a critical
evaluation of our work, we identify promising future research directions for
this new research area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLAIMED -- the open source framework for building coarse-grained
  operators for accelerated discovery in science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romeo Kienzler, Rafflesia Khan, Jerome Nilmeier, Ivan Nesic, Ibrahim Haddad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern data-driven science, reproducibility and reusability are key
challenges. Scientists are well skilled in the process from data to
publication. Although some publication channels require source code and data to
be made accessible, rerunning and verifying experiments is usually hard due to
a lack of standards. Therefore, reusing existing scientific data processing
code from state-of-the-art research is hard as well. This is why we introduce
CLAIMED, which has a proven track record in scientific research for addressing
the repeatability and reusability issues in modern data-driven science. CLAIMED
is a framework to build reusable operators and scalable scientific workflows by
supporting the scientist to draw from previous work by re-composing workflows
from existing libraries of coarse-grained scientific operators. Although
various implementations exist, CLAIMED is programming language, scientific
library, and execution environment agnostic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Received IEEE OSS Award 2023 -
  https://conferences.computer.org/services/2023/symposia/oss.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of the suitability of degradation models for the planning of
  CCTV inspections of sewer pipes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fidae El Morer, Stefan Wittek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The degradation of sewer pipes poses significant economical, environmental
and health concerns. The maintenance of such assets requires structured plans
to perform inspections, which are more efficient when structural and
environmental features are considered along with the results of previous
inspection reports. The development of such plans requires degradation models
that can be based on statistical and machine learning methods. This work
proposes a methodology to assess their suitability to plan inspections
considering three dimensions: accuracy metrics, ability to produce long-term
degradation curves and explainability. Results suggest that although ensemble
models yield the highest accuracy, they are unable to infer the long-term
degradation of the pipes, whereas the Logistic Regression offers a slightly
less accurate model that is able to produce consistent degradation curves with
a high explainability. A use case is presented to demonstrate this methodology
and the efficiency of model-based planning compared to the current inspection
plan.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VampNet: Music Generation via Masked Acoustic Token Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VampNet, a masked acoustic token modeling approach to music
synthesis, compression, inpainting, and variation. We use a variable masking
schedule during training which allows us to sample coherent music from the
model by applying a variety of masking approaches (called prompts) during
inference. VampNet is non-autoregressive, leveraging a bidirectional
transformer architecture that attends to all tokens in a forward pass. With
just 36 sampling passes, VampNet can generate coherent high-fidelity musical
waveforms. We show that by prompting VampNet in various ways, we can apply it
to tasks like music compression, inpainting, outpainting, continuation, and
looping with variation (vamping). Appropriately prompted, VampNet is capable of
maintaining style, genre, instrumentation, and other high-level aspects of the
music. This flexible prompting capability makes VampNet a powerful music
co-creation tool. Code and audio samples are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertain Machine Ethical Decisions Using Hypothetical Retrospection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Kolker, Louise Dennis, Ramon Fraga Pereira, Mengwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the use of the hypothetical retrospection argumentation procedure,
developed by Sven Ove Hansson to improve existing approaches to machine ethical
reasoning by accounting for probability and uncertainty from a position of
Philosophy that resonates with humans. Actions are represented with a branching
set of potential outcomes, each with a state, utility, and either a numeric or
poetic probability estimate. Actions are chosen based on comparisons between
sets of arguments favouring actions from the perspective of their branches,
even those branches that led to an undesirable outcome. This use of arguments
allows a variety of philosophical theories for ethical reasoning to be used,
potentially in flexible combination with each other. We implement the
procedure, applying consequentialist and deontological ethical theories,
independently and concurrently, to an autonomous library system use case. We
introduce a preliminary framework that seems to meet the varied requirements of
a machine ethics system: versatility under multiple theories and a resonance
with humans that enables transparency and explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in
  Confounded Environments <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots operating in real-world environments must reason about possible
outcomes of stochastic actions and make decisions based on partial observations
of the true world state. A major challenge for making accurate and robust
action predictions is the problem of confounding, which if left untreated can
lead to prediction errors. The partially observable Markov decision process
(POMDP) is a widely-used framework to model these stochastic and
partially-observable decision-making problems. However, due to a lack of
explicit causal semantics, POMDP planning methods are prone to confounding bias
and thus in the presence of unobserved confounders may produce underperforming
policies. This paper presents a novel causally-informed extension of "anytime
regularized determinized sparse partially observable tree" (AR-DESPOT), a
modern anytime online POMDP planner, using causal modelling and inference to
eliminate errors caused by unmeasured confounder variables. We further propose
a method to learn offline the partial parameterisation of the causal model for
planning, from ground truth model data. We evaluate our methods on a toy
problem with an unobserved confounder and show that the learned causal model is
highly accurate, while our planning method is more robust to confounding and
produces overall higher performing policies than AR-DESPOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, accepted to 2023 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Graph Attention for Enhanced Spatial Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Singh, Yash Bhambhu, Himanshu Buckchash, Deepak K. Gupta, Dilip K. Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Dynamics Modeling in Interactive Environments with Koopman
  Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Kumar Mondal, Siba Smarak Panigrahi, Sai Rajeswar, Kaleem Siddiqi, Siamak Ravanbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate modeling of dynamics in interactive environments is critical for
successful long-range prediction. Such a capability could advance Reinforcement
Learning (RL) and Planning algorithms, but achieving it is challenging.
Inaccuracies in model estimates can compound, resulting in increased errors
over long horizons. We approach this problem from the lens of Koopman theory,
where the nonlinear dynamics of the environment can be linearized in a
high-dimensional latent space. This allows us to efficiently parallelize the
sequential problem of long-range prediction using convolution, while accounting
for the agent's action at every time step. Our approach also enables stability
analysis and better control over gradients through time. Taken together, these
advantages result in significant improvement over the existing approaches, both
in the efficiency and the accuracy of modeling dynamics over extended horizons.
We also report promising experimental results in dynamics modeling for the
scenarios of both model-based planning and model-free RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03109v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03109v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; code is at https://github.com/MLGroupJLU/LLM-eval-survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARBLE: Music Audio Representation Benchmark for Universal Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of extensive intersection between art and Artificial Intelligence
(AI), such as image generation and fiction co-creation, AI for music remains
relatively nascent, particularly in music understanding. This is evident in the
limited work on deep music representations, the scarcity of large-scale
datasets, and the absence of a universal and community-driven benchmark. To
address this issue, we introduce the Music Audio Representation Benchmark for
universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various
Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy
with four hierarchy levels, including acoustic, performance, score, and
high-level description. We then establish a unified protocol based on 14 tasks
on 8 public-available datasets, providing a fair and standard assessment of
representations of all open-sourced pre-trained models developed on music
recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and
reproducible suite for the community, with a clear statement on copyright
issues on datasets. Results suggest recently proposed large-scale pre-trained
musical language models perform the best in most tasks, with room for further
improvement. The leaderboard and toolkit repository are published at
https://marble-bm.shef.ac.uk to promote future music AI research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-supervised positional contrastive learning: application to
  cirrhosis classification <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large medical imaging datasets can be cheaply and quickly annotated with
low-confidence, weak labels (e.g., radiological scores). Access to
high-confidence labels, such as histology-based diagnoses, is rare and costly.
Pretraining strategies, like contrastive learning (CL) methods, can leverage
unlabeled or weakly-annotated datasets. These methods typically require large
batch sizes, which poses a difficulty in the case of large 3D images at full
resolution, due to limited GPU memory. Nevertheless, volumetric positional
information about the spatial context of each 2D slice can be very important
for some medical applications. In this work, we propose an efficient
weakly-supervised positional (WSP) contrastive learning strategy where we
integrate both the spatial context of each 2D slice and a weak label via a
generic kernel-based loss function. We illustrate our method on cirrhosis
prediction using a large volume of weakly-labeled images, namely radiological
low-confidence annotations, and small strongly-labeled (i.e., high-confidence)
datasets. The proposed model improves the classification AUC by 5% with respect
to a baseline model on our internal dataset, and by 26% on the public LIHC
dataset from the Cancer Genome Atlas. The code is available at:
https://github.com/Guerbet-AI/wsp-contrastive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-multigraph Search: Rethinking Meta-structure on Heterogeneous
  Information Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Li, Hao Xu, Kun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-structures are widely used to define which subset of neighbors to
aggregate information in heterogeneous information networks (HINs). In this
work, we investigate existing meta-structures, including meta-path and
meta-graph, and observe that they are initially designed manually with fixed
patterns and hence are insufficient to encode various rich semantic information
on diverse HINs. Through reflection on their limitation, we define a new
concept called meta-multigraph as a more expressive and flexible generalization
of meta-graph, and propose a stable differentiable search method to
automatically optimize the meta-multigraph for specific HINs and tasks. As the
flexibility of meta-multigraphs may propagate redundant messages, we further
introduce a complex-to-concise (C2C) meta-multigraph that propagates messages
from complex to concise along the depth of meta-multigraph. Moreover, we
observe that the differentiable search typically suffers from unstable search
and a significant gap between the meta-structures in search and evaluation. To
this end, we propose a progressive search algorithm by implicitly narrowing the
search space to improve search stability and reduce inconsistency. Extensive
experiments are conducted on six medium-scale benchmark datasets and one
large-scale benchmark dataset over two representative tasks, i.e., node
classification and recommendation. Empirical results demonstrate that our
search methods can automatically find expressive meta-multigraphs and C2C
meta-multigraphs, enabling our model to outperform state-of-the-art
heterogeneous graph neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:2211.14752</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One <span class="highlight-title">Transformer</span> for All Time Series: Representing and Training with
  Time-Dependent Heterogeneous Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Luetto, Fabrizio Garuti, Enver Sangineto, Lorenzo Forni, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a recent growing interest in applying Deep Learning techniques to
tabular data, in order to replicate the success of other Artificial
Intelligence areas in this structured domain. Specifically interesting is the
case in which tabular data have a time dependence, such as, for instance
financial transactions. However, the heterogeneity of the tabular values, in
which categorical elements are mixed with numerical items, makes this
adaptation difficult. In this paper we propose a Transformer architecture to
represent heterogeneous time-dependent tabular data, in which numerical
features are represented using a set of frequency functions and the whole
network is uniformly trained with a unique loss function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOPO-LSI: A User Guide 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Zheng, Kumar Neelotpal Shukla, Jasmine Xu,  David,  Wang, Michael O'Leary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for
Sustainable Investments. This document provides a user guide for MOPO-LSI
version 1.0, including problem setup, workflow and the hyper-parameters in
configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAMiT: Reciprocal Attention Mixing <span class="highlight-title">Transformer</span> for Lightweight Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haram Choi, Cheolwoong Na, Jihyeon Oh, Seungjae Lee, Jinseop Kim, Subeen Choe, Jeongmin Lee, Taehoon Kim, Jihoon Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although many recent works have made advancements in the image restoration
(IR) field, they often suffer from an excessive number of parameters. Another
issue is that most Transformer-based IR methods focus only on either local or
global features, leading to limited receptive fields or deficient parameter
issues. To address these problems, we propose a lightweight IR network,
Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed
dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which
compute bi-dimensional (spatial and channel) self-attentions in parallel with
different numbers of multi-heads. The bi-dimensional attentions help each other
to complement their counterpart's drawbacks and are then mixed. Additionally,
we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that
compensates for pixel-level information losses and utilizes semantic
information while maintaining an efficient hierarchical structure. Furthermore,
we revisit and modify MobileNet V1 and V2 to attach efficient convolutions to
our proposed components. The experimental results demonstrate that RAMiT
achieves state-of-the-art performance on multiple lightweight IR tasks,
including super-resolution, color denoising, grayscale denoising, low-light
enhancement, and deraining. Codes are available at
https://github.com/rami0205/RAMiT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. 9 pages for main contents + 14 pages for appendix +
  6 pages for references. Codes are available at
  https://github.com/rami0205/RAMiT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangled Contrastive Collaborative Filtering <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02759v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02759v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that graph neural networks (GNNs) are prevalent to model
high-order relationships for collaborative filtering (CF). Towards this
research line, graph contrastive learning (GCL) has exhibited powerful
performance in addressing the supervision label shortage issue by learning
augmented user and item representations. While many of them show their
effectiveness, two key questions still remain unexplored: i) Most existing
GCL-based CF models are still limited by ignoring the fact that user-item
interaction behaviors are often driven by diverse latent intent factors (e.g.,
shopping for family party, preferred color or brand of products); ii) Their
introduced non-adaptive augmentation techniques are vulnerable to noisy
information, which raises concerns about the model's robustness and the risk of
incorporating misleading self-supervised signals. In light of these
limitations, we propose a Disentangled Contrastive Collaborative Filtering
framework (DCCF) to realize intent disentanglement with self-supervised
augmentation in an adaptive fashion. With the learned disentangled
representations with global context, our DCCF is able to not only distill
finer-grained latent factors from the entangled self-supervision signals but
also alleviate the augmentation-induced noise. Finally, the cross-view
contrastive learning task is introduced to enable adaptive augmentation with
our parameterized interaction mask generator. Experiments on various public
datasets demonstrate the superiority of our method compared to existing
solutions. Our model implementation is released at the link
https://github.com/HKUDS/DCCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a SIGIR'23 full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REFLECT: Summarizing Robot Experiences for Failure Explanation and
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liu, Arpit Bahety, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect and analyze failed executions automatically is crucial
for an explainable and robust robotic system. Recently, Large Language Models
(LLMs) have demonstrated strong reasoning abilities on textual inputs. To
leverage the power of LLM for robot failure explanation, we introduce a
framework REFLECT, which queries LLM to identify and explain robot failures
given a hierarchical summary of robot past experiences generated from
multi-sensory data. Conditioned on the explanation, a task planner will
generate an executable plan for the robot to correct the failure and complete
the task. To systematically evaluate the framework, we create the RoboFail
dataset with a variety of tasks and failure scenarios. We demonstrate that the
LLM-based framework is able to generate informative failure explanations that
assist successful correction planning. Videos and code available at:
https://roboreflect.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diagrammatization: Rationalizing with diagrammatic AI explanations for
  abductive-deductive reasoning on hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Y. Lim, Joseph P. Cahaly, Chester Y. F. Sng, Adam Chew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many visualizations have been developed for explainable AI (XAI), but they
often require further reasoning by users to interpret. We argue that XAI should
support diagrammatic and abductive reasoning for the AI to perform hypothesis
generation and evaluation to reduce the interpretability gap. We propose
Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii)
follow domain conventions, and iii) explain with diagrams visually or verbally.
We implemented DiagramNet for a clinical application to predict cardiac
diagnoses from heart auscultation, and explain with shape-based murmur
diagrams. In modeling studies, we found that DiagramNet not only provides
faithful murmur shape explanations, but also has better prediction performance
than baseline models. We further demonstrate the interpretability and
trustworthiness of diagrammatic explanations in a qualitative user study with
medical students, showing that clinically-relevant, diagrammatic explanations
are preferred over technical saliency map explanations. This work contributes
insights into providing domain-conventional abductive explanations for
user-centric XAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Description Logics with Abstraction and Refinement <span class="chip">KR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carsten Lutz, Lukas Schulze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontologies often require knowledge representation on multiple levels of
abstraction, but description logics (DLs) are not well-equipped for supporting
this. We propose an extension of DLs in which abstraction levels are
first-class citizens and which provides explicit operators for the abstraction
and refinement of concepts and roles across multiple abstraction levels, based
on conjunctive queries. We prove that reasoning in the resulting family of DLs
is decidable while several seemingly harmless variations turn out to be
undecidable. We also pinpoint the precise complexity of our logics and several
relevant fragments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, Long version of paper accepted at KR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning-based Multi-objective Path Planning on the
  Off-road Terrain Environment for Ground Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqiao Huang, Xiru Wu, Guoming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the vastly different energy consumption between up-slope and
down-slope, a path with the shortest length on a complex off-road terrain
environment (2.5D map) is not always the path with the least energy
consumption. For any energy-sensitive vehicle, realizing a good trade-off
between distance and energy consumption in 2.5D path planning is significantly
meaningful. In this paper, we propose a deep reinforcement learning-based 2.5D
multi-objective path planning method (DMOP). The DMOP can efficiently find the
desired path in three steps: (1) Transform the high-resolution 2.5D map into a
small-size map. (2) Use a trained deep Q network (DQN) to find the desired path
on the small-size map. (3) Build the planned path to the original
high-resolution map using a path-enhanced method. In addition, the hybrid
exploration strategy and reward shaping theory are applied to train the DQN.
The reward function is constructed with the information of terrain, distance,
and border. Simulation results show that the proposed method can finish the
multi-objective 2.5D path planning task with significantly high efficiency.
With similar planned paths, the speed of the proposed method is more than 100
times faster than that of the A* method and 30 times faster than that of H3DM
method. Also, simulation proves that the method has powerful reasoning
capability that enables it to perform arbitrary untrained planning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrit Diggavi Seshadri, Alessandra Russo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, following the intuition that adverbs describing scene-sequences
are best identified by reasoning over high-level concepts of object-behavior,
we propose the design of a new framework that reasons over object-behaviours
extracted from raw-video-clips to recognize the clip's corresponding
adverb-types. Importantly, while previous works for general scene
adverb-recognition assume knowledge of the clips underlying action-types, our
method is directly applicable in the more general problem setting where the
action-type of a video-clip is unknown. Specifically, we propose a novel
pipeline that extracts human-interpretable object-behaviour-facts from raw
video clips and propose novel symbolic and transformer based reasoning methods
that operate over these extracted facts to identify adverb-types. Experiment
results demonstrate that our proposed methods perform favourably against the
previous state-of-the-art. Additionally, to support efforts in symbolic
video-processing, we release two new datasets of object-behaviour-facts
extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning based Uncertainty Decomposition for Real-time Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.02613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.02613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Das, Jonas Umlauft, Armin Lederer, Thomas Beckers, Sandra Hirche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven control in unknown environments requires a clear understanding of
the involved uncertainties for ensuring safety and efficient exploration. While
aleatoric uncertainty that arises from measurement noise can often be
explicitly modeled given a parametric description, it can be harder to model
epistemic uncertainty, which describes the presence or absence of training
data. The latter can be particularly useful for implementing exploratory
control strategies when system dynamics are unknown. We propose a novel method
for detecting the absence of training data using deep learning, which gives a
continuous valued scalar output between $0$ (indicating low uncertainty) and
$1$ (indicating high uncertainty). We utilize this detector as a proxy for
epistemic uncertainty and show its advantages over existing approaches on
synthetic and real-world datasets. Our approach can be directly combined with
aleatoric uncertainty estimates and allows for uncertainty estimation in
real-time as the inference is sample-free unlike existing approaches for
uncertainty modeling. We further demonstrate the practicality of this
uncertainty estimate in deploying online data-efficient control on a simulated
quadcopter acted upon by an unknown disturbance model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IFAC World Congress 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Rates for the Regret of Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.00479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.00479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichun Hu, Nathan Kallus, Masatoshi Uehara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the regret of reinforcement learning from offline data generated by
a fixed behavior policy in an infinite-horizon discounted Markov decision
process (MDP). While existing analyses of common approaches, such as fitted
$Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret,
empirical behavior exhibits \emph{much} faster convergence. In this paper, we
present a finer regret analysis that exactly characterizes this phenomenon by
providing fast rates for the regret convergence. First, we show that given any
estimate for the optimal quality function $Q^*$, the regret of the policy it
defines converges at a rate given by the exponentiation of the $Q^*$-estimate's
pointwise convergence rate, thus speeding it up. The level of exponentiation
depends on the level of noise in the \emph{decision-making} problem, rather
than the estimation problem. We establish such noise levels for linear and
tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman
residual minimization to establish the correct pointwise convergence
guarantees. As specific cases, our results imply $O(1/n)$ regret rates in
linear cases and $\exp(-\Omega(n))$ regret rates in tabular cases. We extend
our findings to general function approximation by extending our results to
regret guarantees based on $L_p$-convergence rates for estimating $Q^*$ rather
than pointwise rates, where $L_2$ guarantees for nonparametric $Q^*$-estimation
can be ensured under mild conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Value Functions are Control Barrier Functions: Verification of Safe
  Policies using Control Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04026v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04026v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel C. H. Tan, Fernando Acero, Robert McCarthy, Dimitrios Kanoulas, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guaranteeing safe behaviour of reinforcement learning (RL) policies poses
significant challenges for safety-critical applications, despite RL's
generality and scalability. To address this, we propose a new approach to apply
verification methods from control theory to learned value functions. By
analyzing task structures for safety preservation, we formalize original
theorems that establish links between value functions and control barrier
functions. Further, we propose novel metrics for verifying value functions in
safe control tasks and practical implementation details to improve learning.
Our work presents a novel method for certificate learning, which unlocks a
diversity of verification techniques from control theory for RL policies, and
marks a significant step towards a formal framework for the general, scalable,
and verifiable design of RL-based control systems. Code and videos are
available at this https url: https://rl-cbf.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Casadio, Luca Arnaboldi, Matthew L. Daggitt, Omri Isac, Tanvi Dinkar, Daniel Kienitz, Verena Rieser, Ekaterina Komendantskaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verification of machine learning models used in Natural Language Processing
(NLP) is known to be a hard problem. In particular, many known neural network
verification methods that work for computer vision and other numeric datasets
do not work for NLP. Here, we study technical reasons that underlie this
problem. Based on this analysis, we propose practical methods and heuristics
for preparing NLP datasets and models in a way that renders them amenable to
known verification methods based on abstract interpretation. We implement these
methods as a Python library called ANTONIO that links to the neural network
verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP
dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP
applications. We hope that, thanks to its general applicability, this work will
open novel possibilities for including NLP verification problems into neural
network verification competitions, and will popularise NLP problems within this
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in proceedings of 6th Workshop on Formal Methods for
  ML-Enabled Autonomous Systems (Affiliated with CAV 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic
  Flow Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangrok Lee, Ha Young Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic forecasting is a highly challenging task owing to the dynamical
spatio-temporal dependencies of traffic flows. To handle this, we focus on
modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze
Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple
regions. ESGCN consists of two modules: W-module and ES module. W-module is a
fully node-wise convolutional network. It encodes the time-series of each
traffic region separately and decomposes the time-series at various scales to
capture fine and coarse features. The ES module models the spatio-temporal
dynamics using Graph Convolutional Network (GCN) and generates an Adaptive
Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM,
we introduce three key concepts. 1) Using edge features to directly capture the
spatiotemporal flow representation among regions. 2) Applying an edge attention
mechanism to GCN to extract the AAM from the edge features. Here, the attention
mechanism can effectively determine important spatio-temporal adjacency
relations. 3) Proposing a novel node contrastive loss to suppress obstructed
connections and emphasize related connections. Experimental results show that
ESGCN achieves state-of-the-art performance by a large margin on four
real-world datasets (PEMS03, 04, 07, and 08) with a low computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Specific Representation of Emotion-Concept Knowledge Causally
  Supports Emotion Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09582v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09582v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Zhiyuan Liu, Dan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how language supports emotion inference remains a topic of
debate in emotion science. The present study investigated whether
language-derived emotion-concept knowledge would causally support emotion
inference by manipulating the language-specific knowledge representations in
large language models. Using the prompt technique, 14 attributes of emotion
concepts were found to be represented by distinct artificial neuron
populations. By manipulating these attribute-related neurons, the majority of
the emotion inference tasks showed performance deterioration compared to random
manipulations. The attribute-specific performance deterioration was related to
the importance of different attributes in human mental space. Our findings
provide causal evidence in support of a language-based mechanism for emotion
inference and highlight the contributions of emotion-concept knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 13 figures, 2 tables, major revisions over previous
  versions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Employing Crowdsourcing for Enriching a Music Knowledge Base in Higher
  Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vassilis Lyberatos, Spyridon Kantarelis, Eirini Kaldeli, Spyros Bekiaris, Panagiotis Tzortzis, Orfeas Menis - Mastromichalakis, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the methodology followed and the lessons learned from
employing crowdsourcing techniques as part of a homework assignment involving
higher education students of computer science. Making use of a platform that
supports crowdsourcing in the cultural heritage domain students were solicited
to enrich the metadata associated with a selection of music tracks. The results
of the campaign were further analyzed and exploited by students through the use
of semantic web technologies. In total, 98 students participated in the
campaign, contributing more than 6400 annotations concerning 854 tracks. The
process also led to the creation of an openly available annotated dataset,
which can be useful for machine learning models for music tagging. The
campaign's results and the comments gathered through an online survey enable us
to draw some useful insights about the benefits and challenges of integrating
crowdsourcing into computer science curricula and how this can enhance
students' engagement in the learning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented in The 4th International Conference on Artificial
  Intelligence in Education Technology (AIET 2023), Berlin, Germany, 31 June-2
  July 2023. For The GitHub code for the created music dataset, see
  https://github.com/vaslyb/MusicCrowd</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reduce Computational Complexity for Convolutional Layers by Skipping
  Zeros 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyi Zhang, Pengfei Zhang, Zhuopin Xu, Qi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks rely on parallel processors for acceleration. To design
operators for them, it requires not only good algorithm to reduce complexity,
but also sufficient utilization of hardwares. Convolutional layers mainly
contain 3 kinds of operators: convolution in forward propagation, deconvolution
and dilated-convolution in backward propagation. When executing these
operators, 0s are always added to tensors, causing redundant calculations. This
paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these
0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors
to dense tensors, to avoid inserted 0s in deconvolution and
dilated-convolution. In contrast to regular convolution, deconvolution is hard
to accelerate due to its complicacy. This paper provides high-performance GPU
implementations of C-K-S, and verifies their effectiveness with comparison to
PyTorch. According to the experiments, C-K-S has advantages over PyTorch in
certain cases, especially in deconvolution on small feature-maps. Further
enhancement of C-K-S can be done by making full optimizations oriented at
specific GPU architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To download the code of Dragon-Alpha and experimental datas, please
  go to https://github.com/GilgameshXYZ123/Dragon-Alpha</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ B-HAR: an open-source baseline framework for in depth study of human
  activity recognition <span class="highlight-title">dataset</span>s and workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.10870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.10870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florenc Demrozi, Cristian Turetta, Graziano Pravadelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR), based on machine and deep learning
algorithms is considered one of the most promising technologies to monitor
professional and daily life activities for different categories of people
(e.g., athletes, elderly, kids, employers) in order to provide a variety of
services related, for example to well-being, empowering of technical
performances, prevention of risky situation, and educational purposes. However,
the analysis of the effectiveness and the efficiency of HAR methodologies
suffers from the lack of a standard workflow, which might represent the
baseline for the estimation of the quality of the developed pattern recognition
models. This makes the comparison among different approaches a challenging
task. In addition, researchers can make mistakes that, when not detected,
definitely affect the achieved results. To mitigate such issues, this paper
proposes an open-source automatic and highly configurable framework, named
B-HAR, for the definition, standardization, and development of a baseline
framework in order to evaluate and compare HAR methodologies. It implements the
most popular data processing methods for data preparation and the most commonly
used machine and deep learning pattern recognition models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, 3 Figures, 3 Tables, Link to B-HAR Library:
  https://github.com/B-HAR-HumanActivityRecognition/B-HAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bidirectional Generation of Structure and Properties Through a Single
  Molecular Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10590v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10590v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinho Chang, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of large foundation models in artificial intelligence has
prompted the emergence of chemical pre-trained models. Despite the growing
interest in large molecular pre-trained models that provide informative
representations for downstream tasks, attempts for multimodal pre-training
approaches on the molecule domain were limited. To address this, we present a
novel multimodal molecular pre-trained model that incorporates the modalities
of structure and biochemical properties, drawing inspiration from recent
advances in multimodal learning techniques. Our proposed model pipeline of data
handling and training objectives aligns the structure/property features in a
common embedding space, which enables the model to regard bidirectional
information between the molecules' structure and properties. These
contributions emerge synergistic knowledge, allowing us to tackle both
multimodal and unimodal downstream tasks through a single model. Through
extensive experiments, we demonstrate that our model shows remarkable
capabilities in solving various meaningful chemical challenges, including
conditional molecule generation, property prediction, molecule classification,
and reaction prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundamental Limits for Sensor-Based Robot Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00129v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00129v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudha Majumdar, Zhiting Mei, Vincent Pacelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our goal is to develop theory and algorithms for establishing fundamental
limits on performance imposed by a robot's sensors for a given task. In order
to achieve this, we define a quantity that captures the amount of task-relevant
information provided by a sensor. Using a novel version of the generalized Fano
inequality from information theory, we demonstrate that this quantity provides
an upper bound on the highest achievable expected reward for one-step decision
making tasks. We then extend this bound to multi-step problems via a dynamic
programming approach. We present algorithms for numerically computing the
resulting bounds, and demonstrate our approach on three examples: (i) the lava
problem from the literature on partially observable Markov decision processes,
(ii) an example with continuous state and observation spaces corresponding to a
robot catching a freely-falling object, and (iii) obstacle avoidance using a
depth sensor with non-Gaussian noise. We demonstrate the ability of our
approach to establish strong limits on achievable performance for these
problems by comparing our upper bounds with achievable lower bounds (computed
by synthesizing or learning concrete control policies).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of paper presented at the 2022 Robotics: Science and
  Systems (RSS) conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pattern<span class="highlight-title">GPT</span> :A Pattern-Driven Framework for Large Language Model Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Xiao, Xin Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models(LLMS) have shown excellent text generation
capabilities,capable of generating fluent responses for many downstream tasks.
However,applying large language models to real-world critical tasks remains
challenging due to their susceptibility to hallucinations and inability to
directly use external knowledge. To address the above challenges,this paper
proposes PatternGPT, a pattern-driven text generation framework for large
language models. First,the framework utilizes the extraction capabilities of
large language models to generate rich and diverse patterns and later draws on
the idea of federated learning. Using multiple agents to achieve sharing to
obtain more diverse patterns. Finally, it searches for high-quality patterns
using judgment criteria and optimization algorithms and uses the searched
patterns to guide the model for generation. This framework has the advantages
of generating diversified patterns, protecting data privacy,combining external
knowledge, and improving the quality of generation, which provides an effective
method to optimize the text generation capability of large language models,and
make it better applied to the field of intelligent dialogue and content
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Judging LLM-as-a-judge with MT-Bench and Chatbot Arena 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, including position, verbosity, and
self-enhancement biases, as well as limited reasoning ability, and propose
solutions to mitigate some of them. We then verify the agreement between LLM
judges and human preferences by introducing two benchmarks: MT-bench, a
multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our
results reveal that strong LLM judges like GPT-4 can match both controlled and
crowdsourced human preferences well, achieving over 80\% agreement, the same
level of agreement between humans. Hence, LLM-as-a-judge is a scalable and
explainable way to approximate human preferences, which are otherwise very
expensive to obtain. Additionally, we show our benchmark and traditional
benchmarks complement each other by evaluating several variants of LLaMA and
Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K
conversations with human preferences from Chatbot Arena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polysemanticity and Capacity in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01892v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01892v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Scherlis, Kshitij Sachan, Adam S. Jermyn, Joe Benton, Buck Shlegeris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures. Corrected typos in Figure 7, improved notation
  to distinguish column and row vectors, corrected proof in Appendix A, and
  other misc changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effectiveness of World Models for Continual Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Kessler, Mateusz Ostaszewski, Michał Bortkiewicz, Mateusz Żarski, Maciej Wołczyk, Jack Parker-Holder, Stephen J. Roberts, Piotr Miłoś
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models power some of the most efficient reinforcement learning
algorithms. In this work, we showcase that they can be harnessed for continual
learning - a situation when the agent faces changing environments. World models
typically employ a replay buffer for training, which can be naturally extended
to continual learning. We systematically study how different selective
experience replay methods affect performance, forgetting, and transfer. We also
provide recommendations regarding various modeling options for using world
models. The best set of choices is called Continual-Dreamer, it is
task-agnostic and utilizes the world model for continual exploration.
Continual-Dreamer is sample efficient and outperforms state-of-the-art
task-agnostic continual reinforcement learning methods on Minigrid and Minihack
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoLLAs 2023, 21 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FR3D: Three-dimensional Flow Reconstruction and Force Estimation for
  Unsteady Flows Around Extruded Bluff Bodies via Conformal Mapping Aided
  Convolutional Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Girayhan Özbay, Sylvain Laizet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many practical fluid dynamics experiments, measuring variables such as
velocity and pressure is possible only at a limited number of sensor locations,
\textcolor{black}{for a few two-dimensional planes, or for a small 3D domain in
the flow}. However, knowledge of the full fields is necessary to understand the
dynamics of many flows. Deep learning reconstruction of full flow fields from
sparse measurements has recently garnered significant research interest, as a
way of overcoming this limitation. This task is referred to as the flow
reconstruction (FR) task. In the present study, we propose a convolutional
autoencoder based neural network model, dubbed FR3D, which enables FR to be
carried out for three-dimensional flows around extruded 3D objects with
different cross-sections. An innovative mapping approach, whereby multiple
fluid domains are mapped to an annulus, enables FR3D to generalize its
performance to objects not encountered during training. We conclusively
demonstrate this generalization capability using a dataset composed of 80
training and 20 testing geometries, all randomly generated. We show that the
FR3D model reconstructs pressure and velocity components with a few percentage
points of error. Additionally, using these predictions, we accurately estimate
the Q-criterion fields as well lift and drag forces on the geometries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 10 figures. Accepted at International Journal of Heat and
  Fluid Flow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework
  for Model Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandita Bhaskhar, Daniel L. Rubin, Christopher Lee-Messer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous monitoring of trained ML models to determine when their
predictions should and should not be trusted is essential for their safe
deployment. Such a framework ought to be high-performing, explainable, post-hoc
and actionable. We propose TRUST-LAPSE, a "mistrust" scoring framework for
continuous model monitoring. We assess the trustworthiness of each input
sample's model prediction using a sequence of latent-space embeddings.
Specifically, (a) our latent-space mistrust score estimates mistrust using
distance metrics (Mahalanobis distance) and similarity metrics (cosine
similarity) in the latent-space and (b) our sequential mistrust score
determines deviations in correlations over the sequence of past input
representations in a non-parametric, sliding-window based algorithm for
actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream
tasks: (1) distributionally shifted input detection, and (2) data drift
detection. We evaluate across diverse domains - audio and vision using public
datasets and further benchmark our approach on challenging, real-world
electroencephalograms (EEG) datasets for seizure detection. Our latent-space
mistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision),
73.9 (audio), and 77.1 (clinical EEGs), outperforming baselines by over 10
points. We expose critical failures in popular baselines that remain
insensitive to input semantic content, rendering them unfit for real-world
model monitoring. We show that our sequential mistrust scores achieve high
drift detection rates; over 90% of the streams show < 20% error for all
domains. Through extensive qualitative and quantitative evaluations, we show
that our mistrust scores are more robust and provide explainability for easy
adoption into practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: Mistrust Scores, Latent-Space, Model monitoring,
  Trustworthy AI, Explainable AI, Semantic-guided AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FishRecGAN: An End to End GAN Based Network for Fisheye Rectification
  and Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05222v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05222v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Shen, Kyungdon Joo, Jean Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an end-to-end deep learning approach to rectify fisheye images and
simultaneously calibrate camera intrinsic and distortion parameters. Our method
consists of two parts: a Quick Image Rectification Module developed with a
Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a
CNN architecture. Our Quick Rectification Network performs robust rectification
with good resolution, making it suitable for constant calibration in
camera-based surveillance equipment. To achieve high-quality calibration, we
use the straightened output from the Quick Rectification Module as a
guidance-like semantic feature map for the Calibration Module to learn the
geometric relationship between the straightened feature and the distorted
feature. We train and validate our method with a large synthesized dataset
labeled with well-simulated parameters applied to a perspective image dataset.
Our solution has achieved robust performance in high-resolution with a
significant PSNR value of 22.343.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 4 tables, accepted by AAIML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Formalism, Method and Open Issues for Zero-Shot Coordination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.06613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.06613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Treutlein, Michael Dennis, Caspar Oesterheld, Jakob Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many coordination problems, independently reasoning humans are able to
discover mutually compatible policies. In contrast, independently trained
self-play policies are often mutually incompatible. Zero-shot coordination
(ZSC) has recently been proposed as a new frontier in multi-agent reinforcement
learning to address this fundamental issue. Prior work approaches the ZSC
problem by assuming players can agree on a shared learning algorithm but not on
labels for actions and observations, and proposes other-play as an optimal
solution. However, until now, this "label-free" problem has only been
informally defined. We formalize this setting as the label-free coordination
(LFC) problem by defining the label-free coordination game. We show that
other-play is not an optimal solution to the LFC problem as it fails to
consistently break ties between incompatible maximizers of the other-play
objective. We introduce an extension of the algorithm, other-play with
tie-breaking, and prove that it is optimal in the LFC problem and an
equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the
ZSC setting aims to prevent, we conclude that the LFC problem does not reflect
the aims of ZSC. To address this, we introduce an alternative informal
operationalization of ZSC as a starting point for future work.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-11T00:00:00Z">2023-07-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProgGP: From GuitarPro Tablature Neural Generation To Progressive Metal
  Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jackson Loth, Pedro Sarmento, CJ Carr, Zack Zukowski, Mathieu Barthet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in the field of symbolic music generation has shown value in
using a tokenization based on the GuitarPro format, a symbolic representation
supporting guitar expressive attributes, as an input and output representation.
We extend this work by fine-tuning a pre-trained Transformer model on ProgGP, a
custom dataset of 173 progressive metal songs, for the purposes of creating
compositions from that genre through a human-AI partnership. Our model is able
to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We
examine the validity of the generated music using a mixed methods approach by
combining quantitative analyses following a computational musicology paradigm
and qualitative analyses following a practice-based research paradigm. Finally,
we demonstrate the value of the model by using it as a tool to create a
progressive metal song, fully produced and mixed by a human metal producer
based on AI-generated music.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print accepted for publication at CMMR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShredGP: Guitarist Style-Conditioned Tablature Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Sarmento, Adarsh Kumar, Dekun Xie, CJ Carr, Zack Zukowski, Mathieu Barthet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GuitarPro format tablatures are a type of digital music notation that
encapsulates information about guitar playing techniques and fingerings. We
introduce ShredGP, a GuitarPro tablature generative Transformer-based model
conditioned to imitate the style of four distinct iconic electric guitarists.
In order to assess the idiosyncrasies of each guitar player, we adopt a
computational musicology methodology by analysing features computed from the
tokens yielded by the DadaGP encoding scheme. Statistical analyses of the
features evidence significant differences between the four guitarists. We
trained two variants of the ShredGP model, one using a multi-instrument corpus,
the other using solo guitar data. We present a BERT-based model for guitar
player classification and use it to evaluate the generated examples. Overall,
results from the classifier show that ShredGP is able to generate content
congruent with the style of the targeted guitar player. Finally, we reflect on
prospective applications for ShredGP for human-AI music interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at CMMR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Effectiveness of Speech <span class="highlight-title">Self-supervised</span> Learning for Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Ma, Ruibin Yuan, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Ruibo Liu, Gus Xia, Roger Dannenberg, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has shown promising results in various speech
and natural language processing applications. However, its efficacy in music
information retrieval (MIR) still remains largely unexplored. While previous
SSL models pre-trained on music recordings may have been mostly closed-sourced,
recent speech models such as wav2vec2.0 have shown promise in music modelling.
Nevertheless, research exploring the effectiveness of applying speech SSL
models to music recordings has been limited. We explore the music adaption of
SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and
refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL
models with 95M parameters under various pre-training configurations and
systematically evaluate the MIR task performances with 13 different MIR tasks.
Our findings suggest that training with music data can generally improve
performance on MIR tasks, even when models are trained using paradigms designed
for speech. However, we identify the limitations of such existing
speech-oriented designs, especially in modelling polyphonic information. Based
on the experimental results, empirical suggestions are also given for designing
future musical SSL strategies and paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aeroacoustic testing on a full aircraft model at high Reynolds numbers
  in the European Transonic Windtunnel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Ahlefeldt, Daniel Ernst, Armin Goudarzi,  Hans-Georg-Raumer, Carsten Spehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an end-to-end approach for the assessment of pressurized
and cryogenic wind tunnel measurements of an EMBRAER scaled full model close to
real-world Reynolds numbers. The choice of microphones, measurement parameters,
the design of the array, and the selection of flow parameters are discussed.
Different wind tunnel conditions are proposed which allow separating the
influence of the Reynolds number from the Mach number, as well as the influence
of slotted and closed test sections. The paper provides three-dimensional
beamforming results with CLEAN-SC deconvolution, the selection of regions of
interest, and the corresponding source spectra. The results suggest that
slotted test sections have little influence on the beamforming results compared
to closed test sections and that the Reynolds number has a profound, non-linear
impact on the aeroacoustic emission that lessens with increasing Reynolds
number. Further, sources show a non-linear Mach number dependency at constant
Reynolds number but are self-similar in the observed Mach number range. The
findings suggest that it is possible to study real-world phenomena on
small-scale full models at real-world Reynolds numbers, which enable further
investigations in the future such as the directivity of sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Use of <span class="highlight-title">Self-Supervised</span> Speech Representations in Spontaneous
  Speech Synthesis <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) speech representations learned from large
amounts of diverse, mixed-quality speech data without transcriptions are
gaining ground in many speech technology applications. Prior work has shown
that SSL is an effective intermediate representation in two-stage
text-to-speech (TTS) for both read and spontaneous speech. However, it is still
not clear which SSL and which layer from each SSL model is most suited for
spontaneous TTS. We address this shortcoming by extending the scope of
comparison for SSL in spontaneous TTS to 6 different SSLs and 3 layers within
each SSL. Furthermore, SSL has also shown potential in predicting the mean
opinion scores (MOS) of synthesized speech, but this has only been done in
read-speech MOS prediction. We extend an SSL-based MOS prediction framework
previously developed for scoring read speech synthesis and evaluate its
performance on synthesized spontaneous speech. All experiments are conducted
twice on two different spontaneous corpora in order to find generalizable
trends. Overall, we present comprehensive experimental results on the use of
SSL in spontaneous TTS and MOS prediction to further quantify and understand
how SSL can be used in spontaneous TTS. Audios samples:
https://www.speech.kth.se/tts-demos/sp_ssl_tts
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures. 12th ISCA Speech Synthesis Workshop (SSW) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Feature Extraction for Symbolic Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Simonetta, Ana Llorens, Martín Serrano, Eduardo García-Portugués, Álvaro Torrente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive investigation of existing feature
extraction tools for symbolic music and contrasts their performance to
determine the set of features that best characterizes the musical style of a
given music score. In this regard, we propose a novel feature extraction tool,
named musif, and evaluate its efficacy on various repertoires and file formats,
including MIDI, MusicXML, and **kern. Musif approximates existing tools such as
jSymbolic and music21 in terms of computational efficiency while attempting to
enhance the usability for custom feature development. The proposed tool also
enhances classification accuracy when combined with other sets of features. We
demonstrate the contribution of each set of features and the computational
resources they require. Our findings indicate that the optimal tool for feature
extraction is a combination of the best features from each tool rather than
those of a single one. To facilitate future research in music information
retrieval, we release the source code of the tool and benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The smarty4covid <span class="highlight-title">dataset</span> and knowledge base: a framework enabling
  interpretable analysis of audio signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantia Zarkogianni, Edmund Dervakos, George Filandrianos, Theofanis Ganitidis, Vasiliki Gkatzou, Aikaterini Sakagianni, Raghu Raghavendra, C. L. Max Nikias, Giorgos Stamou, Konstantina S. Nikita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harnessing the power of Artificial Intelligence (AI) and m-health towards
detecting new bio-markers indicative of the onset and progress of respiratory
abnormalities/conditions has greatly attracted the scientific and research
interest especially during COVID-19 pandemic. The smarty4covid dataset contains
audio signals of cough (4,676), regular breathing (4,665), deep breathing
(4,695) and voice (4,291) as recorded by means of mobile devices following a
crowd-sourcing approach. Other self reported information is also included (e.g.
COVID-19 virus tests), thus providing a comprehensive dataset for the
development of COVID-19 risk detection models. The smarty4covid dataset is
released in the form of a web-ontology language (OWL) knowledge base enabling
data consolidation from other relevant datasets, complex queries and reasoning.
It has been utilized towards the development of models able to: (i) extract
clinically informative respiratory indicators from regular breathing records,
and (ii) identify cough, breath and voice segments in crowd-sourced audio
recordings. A new framework utilizing the smarty4covid OWL knowledge base
towards generating counterfactual explanations in opaque AI-based COVID-19 risk
detection models is proposed and validated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication in Nature Scientific Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SnakeSynth: New Interactions for Generative Audio Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Easthope
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I present "SnakeSynth," a web-based lightweight audio synthesizer that
combines audio generated by a deep generative model and real-time continuous
two-dimensional (2D) input to create and control variable-length generative
sounds through 2D interaction gestures. Interaction gestures are touch and
mobile-compatible with analogies to strummed, bowed, and plucked musical
instrument controls. Point-and-click and drag-and-drop gestures directly
control audio playback length and I show that sound length and intensity are
modulated by interactions with a programmable 2D coordinate grid. Leveraging
the speed and ubiquity of browser-based audio and hardware acceleration in
Google's TensorFlow.js we generate time-varying high-fidelity sounds with
real-time interactivity. SnakeSynth adaptively reproduces and interpolates
between sounds encountered during model training, notably without long training
times, and I briefly discuss possible futures for deep generative models as an
interactive paradigm for musical expression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer
  Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denise Moussa, Germans Hirsch, Sebastian Wankerl, Christian Riess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifying the integrity of voice recording evidence for criminal
investigations is an integral part of an audio forensic analyst's work. Here,
one focus is on detecting deletion or insertion operations, so called audio
splicing. While this is a rather easy approach to alter spoken statements,
careful editing can yield quite convincing results. For difficult cases or big
amounts of data, automated tools can support in detecting potential editing
locations. To this end, several analytical and deep learning methods have been
proposed by now. Still, few address unconstrained splicing scenarios as
expected in practice. With SigPointer, we propose a pointer network framework
for continuous input that uncovers splice locations naturally and more
efficiently than existing works. Extensive experiments on forensically
challenging data like strongly compressed and noisy signals quantify the
benefit of the pointer mechanism with performance increases between about 6 to
10 percentage points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at Interspeech 2023, code will be made available after paper
  presentation (at the latest)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech Diarization and ASR with GMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aayush Kumar Sharma, Vineet Bhavikatti, Amogh Nidawani, Dr. Siddappaji, Sanath P, Dr Geetishree Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research paper, we delve into the topics of Speech Diarization and
Automatic Speech Recognition (ASR). Speech diarization involves the separation
of individual speakers within an audio stream. By employing the ASR transcript,
the diarization process aims to segregate each speaker's utterances, grouping
them based on their unique audio characteristics. On the other hand, Automatic
Speech Recognition refers to the capability of a machine or program to identify
and convert spoken words and phrases into a machine-readable format. In our
speech diarization approach, we utilize the Gaussian Mixer Model (GMM) to
represent speech segments. The inter-cluster distance is computed based on the
GMM parameters, and the distance threshold serves as the stopping criterion.
ASR entails the conversion of an unknown speech waveform into a corresponding
written transcription. The speech signal is analyzed using synchronized
algorithms, taking into account the pitch frequency. Our primary objective
typically revolves around developing a model that minimizes the Word Error Rate
(WER) metric during speech transcription.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnuraSet: A <span class="highlight-title">dataset</span> for benchmarking Neotropical anuran calls
  identification in passive acoustic monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Sebastián Cañas, Maria Paula Toro-Gómez, Larissa Sayuri Moreira Sugai, Hernán Darío Benítez Restrepo, Jorge Rudas, Breyner Posso Bautista, Luís Felipe Toledo, Simone Dena, Adão Henrique Rosa Domingos, Franco Leandro de Souza, Selvino Neckel-Oliveira, Anderson da Rosa, Vítor Carvalho-Rocha, José Vinícius Bernardy, José Luiz Massao Moreira Sugai, Carolina Emília dos Santos, Rogério Pereira Bastos, Diego Llusia, Juan Sebastián Ulloa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global change is predicted to induce shifts in anuran acoustic behavior,
which can be studied through passive acoustic monitoring (PAM). Understanding
changes in calling behavior requires the identification of anuran species,
which is challenging due to the particular characteristics of neotropical
soundscapes. In this paper, we introduce a large-scale multi-species dataset of
anuran amphibians calls recorded by PAM, that comprises 27 hours of expert
annotations for 42 different species from two Brazilian biomes. We provide open
access to the dataset, including the raw recordings, experimental setup code,
and a benchmark with a baseline model of the fine-grained categorization
problem. Additionally, we highlight the challenges of the dataset to encourage
machine learning researchers to solve the problem of anuran call identification
towards conservation policy. All our experiments and resources can be found on
our GitHub repository https://github.com/soundclim/anuraset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LegoNN: Building Modular Encoder-Decoder Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Dalmia, Dmytro Okhonko, Mike Lewis, Sergey Edunov, Shinji Watanabe, Florian Metze, Luke Zettlemoyer, Abdelrahman Mohamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or
automatic speech recognition (ASR)) are constructed and trained end-to-end as
an atomic unit. No component of the model can be (re-)used without the others,
making it impossible to share parts, e.g. a high resourced decoder, across
tasks. We describe LegoNN, a procedure for building encoder-decoder
architectures in a way so that its parts can be applied to other tasks without
the need for any fine-tuning. To achieve this reusability, the interface
between encoder and decoder modules is grounded to a sequence of marginal
distributions over a pre-defined discrete vocabulary. We present two approaches
for ingesting these marginals; one is differentiable, allowing the flow of
gradients across the entire network, and the other is gradient-isolating. To
enable the portability of decoder modules between MT tasks for different source
languages and across other tasks like ASR, we introduce a modality agnostic
encoder which consists of a length control mechanism to dynamically adapt
encoders' output lengths in order to match the expected input length range of
pre-trained decoders. We present several experiments to demonstrate the
effectiveness of LegoNN models: a trained language generation LegoNN decoder
module from German-English (De-En) MT task can be reused without any
fine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT
tasks, matching or beating the performance of baseline. After fine-tuning,
LegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5%
relative WER reduction on the Europarl ASR task. To show how the approach
generalizes, we compose a LegoNN ASR model from three modules -- each has been
learned within different end-to-end trained models on three different datasets
-- achieving an overall WER reduction of 19.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM Transactions on Audio, Speech, and Language Processing
  (TASLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WhisperX: Time-Accurate Speech Transcription of Long-Form Audio <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Bain, Jaesung Huh, Tengda Han, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale, weakly-supervised speech recognition models, such as Whisper,
have demonstrated impressive results on speech recognition across domains and
languages. However, their application to long audio transcription via buffered
or sliding window approaches is prone to drifting, hallucination & repetition;
and prohibits batched transcription due to their sequential nature. Further,
timestamps corresponding each utterance are prone to inaccuracies and
word-level timestamps are not available out-of-the-box. To overcome these
challenges, we present WhisperX, a time-accurate speech recognition system with
word-level timestamps utilising voice activity detection and forced phoneme
alignment. In doing so, we demonstrate state-of-the-art performance on
long-form transcription and word segmentation benchmarks. Additionally, we show
that pre-segmenting audio with our proposed VAD Cut & Merge strategy improves
transcription quality and enables a twelve-fold transcription speedup via
batched inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Style Transfer for Text-to-Speech with ControlVAE and
  Diffusion Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Guan, Tao Li, Yishuang Li, Hukai Huang, Qingyang Hong, Lin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the demand for autonomous control and personalized speech generation,
the style control and transfer in Text-to-Speech (TTS) is becoming more and
more important. In this paper, we propose a new TTS system that can perform
style transfer with interpretability and high fidelity. Firstly, we design a
TTS system that combines variational autoencoder (VAE) and diffusion refiner to
get refined mel-spectrograms. Specifically, a two-stage and a one-stage system
are designed respectively, to improve the audio quality and the performance of
style transfer. Secondly, a diffusion bridge of quantized VAE is designed to
efficiently learn complex discrete style representations and improve the
performance of style transfer. To have a better ability of style transfer, we
introduce ControlVAE to improve the reconstruction quality and have good
interpretability simultaneously. Experiments on LibriTTS dataset demonstrate
that our method is more effective than baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProgGP: From GuitarPro Tablature Neural Generation To Progressive Metal
  Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jackson Loth, Pedro Sarmento, CJ Carr, Zack Zukowski, Mathieu Barthet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in the field of symbolic music generation has shown value in
using a tokenization based on the GuitarPro format, a symbolic representation
supporting guitar expressive attributes, as an input and output representation.
We extend this work by fine-tuning a pre-trained Transformer model on ProgGP, a
custom dataset of 173 progressive metal songs, for the purposes of creating
compositions from that genre through a human-AI partnership. Our model is able
to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We
examine the validity of the generated music using a mixed methods approach by
combining quantitative analyses following a computational musicology paradigm
and qualitative analyses following a practice-based research paradigm. Finally,
we demonstrate the value of the model by using it as a tool to create a
progressive metal song, fully produced and mixed by a human metal producer
based on AI-generated music.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print accepted for publication at CMMR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShredGP: Guitarist Style-Conditioned Tablature Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Sarmento, Adarsh Kumar, Dekun Xie, CJ Carr, Zack Zukowski, Mathieu Barthet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GuitarPro format tablatures are a type of digital music notation that
encapsulates information about guitar playing techniques and fingerings. We
introduce ShredGP, a GuitarPro tablature generative Transformer-based model
conditioned to imitate the style of four distinct iconic electric guitarists.
In order to assess the idiosyncrasies of each guitar player, we adopt a
computational musicology methodology by analysing features computed from the
tokens yielded by the DadaGP encoding scheme. Statistical analyses of the
features evidence significant differences between the four guitarists. We
trained two variants of the ShredGP model, one using a multi-instrument corpus,
the other using solo guitar data. We present a BERT-based model for guitar
player classification and use it to evaluate the generated examples. Overall,
results from the classifier show that ShredGP is able to generate content
congruent with the style of the targeted guitar player. Finally, we reflect on
prospective applications for ShredGP for human-AI music interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at CMMR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Effectiveness of Speech <span class="highlight-title">Self-supervised</span> Learning for Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Ma, Ruibin Yuan, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Ruibo Liu, Gus Xia, Roger Dannenberg, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has shown promising results in various speech
and natural language processing applications. However, its efficacy in music
information retrieval (MIR) still remains largely unexplored. While previous
SSL models pre-trained on music recordings may have been mostly closed-sourced,
recent speech models such as wav2vec2.0 have shown promise in music modelling.
Nevertheless, research exploring the effectiveness of applying speech SSL
models to music recordings has been limited. We explore the music adaption of
SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and
refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL
models with 95M parameters under various pre-training configurations and
systematically evaluate the MIR task performances with 13 different MIR tasks.
Our findings suggest that training with music data can generally improve
performance on MIR tasks, even when models are trained using paradigms designed
for speech. However, we identify the limitations of such existing
speech-oriented designs, especially in modelling polyphonic information. Based
on the experimental results, empirical suggestions are also given for designing
future musical SSL strategies and paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aeroacoustic testing on a full aircraft model at high Reynolds numbers
  in the European Transonic Windtunnel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Ahlefeldt, Daniel Ernst, Armin Goudarzi,  Hans-Georg-Raumer, Carsten Spehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an end-to-end approach for the assessment of pressurized
and cryogenic wind tunnel measurements of an EMBRAER scaled full model close to
real-world Reynolds numbers. The choice of microphones, measurement parameters,
the design of the array, and the selection of flow parameters are discussed.
Different wind tunnel conditions are proposed which allow separating the
influence of the Reynolds number from the Mach number, as well as the influence
of slotted and closed test sections. The paper provides three-dimensional
beamforming results with CLEAN-SC deconvolution, the selection of regions of
interest, and the corresponding source spectra. The results suggest that
slotted test sections have little influence on the beamforming results compared
to closed test sections and that the Reynolds number has a profound, non-linear
impact on the aeroacoustic emission that lessens with increasing Reynolds
number. Further, sources show a non-linear Mach number dependency at constant
Reynolds number but are self-similar in the observed Mach number range. The
findings suggest that it is possible to study real-world phenomena on
small-scale full models at real-world Reynolds numbers, which enable further
investigations in the future such as the directivity of sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Use of <span class="highlight-title">Self-Supervised</span> Speech Representations in Spontaneous
  Speech Synthesis <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) speech representations learned from large
amounts of diverse, mixed-quality speech data without transcriptions are
gaining ground in many speech technology applications. Prior work has shown
that SSL is an effective intermediate representation in two-stage
text-to-speech (TTS) for both read and spontaneous speech. However, it is still
not clear which SSL and which layer from each SSL model is most suited for
spontaneous TTS. We address this shortcoming by extending the scope of
comparison for SSL in spontaneous TTS to 6 different SSLs and 3 layers within
each SSL. Furthermore, SSL has also shown potential in predicting the mean
opinion scores (MOS) of synthesized speech, but this has only been done in
read-speech MOS prediction. We extend an SSL-based MOS prediction framework
previously developed for scoring read speech synthesis and evaluate its
performance on synthesized spontaneous speech. All experiments are conducted
twice on two different spontaneous corpora in order to find generalizable
trends. Overall, we present comprehensive experimental results on the use of
SSL in spontaneous TTS and MOS prediction to further quantify and understand
how SSL can be used in spontaneous TTS. Audios samples:
https://www.speech.kth.se/tts-demos/sp_ssl_tts
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures. 12th ISCA Speech Synthesis Workshop (SSW) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Feature Extraction for Symbolic Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Simonetta, Ana Llorens, Martín Serrano, Eduardo García-Portugués, Álvaro Torrente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive investigation of existing feature
extraction tools for symbolic music and contrasts their performance to
determine the set of features that best characterizes the musical style of a
given music score. In this regard, we propose a novel feature extraction tool,
named musif, and evaluate its efficacy on various repertoires and file formats,
including MIDI, MusicXML, and **kern. Musif approximates existing tools such as
jSymbolic and music21 in terms of computational efficiency while attempting to
enhance the usability for custom feature development. The proposed tool also
enhances classification accuracy when combined with other sets of features. We
demonstrate the contribution of each set of features and the computational
resources they require. Our findings indicate that the optimal tool for feature
extraction is a combination of the best features from each tool rather than
those of a single one. To facilitate future research in music information
retrieval, we release the source code of the tool and benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The smarty4covid <span class="highlight-title">dataset</span> and knowledge base: a framework enabling
  interpretable analysis of audio signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantia Zarkogianni, Edmund Dervakos, George Filandrianos, Theofanis Ganitidis, Vasiliki Gkatzou, Aikaterini Sakagianni, Raghu Raghavendra, C. L. Max Nikias, Giorgos Stamou, Konstantina S. Nikita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harnessing the power of Artificial Intelligence (AI) and m-health towards
detecting new bio-markers indicative of the onset and progress of respiratory
abnormalities/conditions has greatly attracted the scientific and research
interest especially during COVID-19 pandemic. The smarty4covid dataset contains
audio signals of cough (4,676), regular breathing (4,665), deep breathing
(4,695) and voice (4,291) as recorded by means of mobile devices following a
crowd-sourcing approach. Other self reported information is also included (e.g.
COVID-19 virus tests), thus providing a comprehensive dataset for the
development of COVID-19 risk detection models. The smarty4covid dataset is
released in the form of a web-ontology language (OWL) knowledge base enabling
data consolidation from other relevant datasets, complex queries and reasoning.
It has been utilized towards the development of models able to: (i) extract
clinically informative respiratory indicators from regular breathing records,
and (ii) identify cough, breath and voice segments in crowd-sourced audio
recordings. A new framework utilizing the smarty4covid OWL knowledge base
towards generating counterfactual explanations in opaque AI-based COVID-19 risk
detection models is proposed and validated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication in Nature Scientific Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving RNN-Transducers with Acoustic LookAhead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinit S. Unni, Ashish Mittal, Preethi Jyothi, Sunita Sarawagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end
model for speech to text conversion because of their high accuracy and
streaming capabilities. A typical RNN-T independently encodes the input audio
and the text context, and combines the two encodings by a thin joint network.
While this architecture provides SOTA streaming accuracy, it also makes the
model vulnerable to strong LM biasing which manifests as multi-step
hallucination of text without acoustic evidence. In this paper we propose
LookAhead that makes text representations more acoustically grounded by looking
ahead into the future within the audio input. This technique yields a
significant 5%-20% relative reduction in word error rate on both in-domain and
out-of-domain evaluation sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 fig, 7 tables, Proceedings of Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SnakeSynth: New Interactions for Generative Audio Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Easthope
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I present "SnakeSynth," a web-based lightweight audio synthesizer that
combines audio generated by a deep generative model and real-time continuous
two-dimensional (2D) input to create and control variable-length generative
sounds through 2D interaction gestures. Interaction gestures are touch and
mobile-compatible with analogies to strummed, bowed, and plucked musical
instrument controls. Point-and-click and drag-and-drop gestures directly
control audio playback length and I show that sound length and intensity are
modulated by interactions with a programmable 2D coordinate grid. Leveraging
the speed and ubiquity of browser-based audio and hardware acceleration in
Google's TensorFlow.js we generate time-varying high-fidelity sounds with
real-time interactivity. SnakeSynth adaptively reproduces and interpolates
between sounds encountered during model training, notably without long training
times, and I briefly discuss possible futures for deep generative models as an
interactive paradigm for musical expression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer
  Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denise Moussa, Germans Hirsch, Sebastian Wankerl, Christian Riess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifying the integrity of voice recording evidence for criminal
investigations is an integral part of an audio forensic analyst's work. Here,
one focus is on detecting deletion or insertion operations, so called audio
splicing. While this is a rather easy approach to alter spoken statements,
careful editing can yield quite convincing results. For difficult cases or big
amounts of data, automated tools can support in detecting potential editing
locations. To this end, several analytical and deep learning methods have been
proposed by now. Still, few address unconstrained splicing scenarios as
expected in practice. With SigPointer, we propose a pointer network framework
for continuous input that uncovers splice locations naturally and more
efficiently than existing works. Extensive experiments on forensically
challenging data like strongly compressed and noisy signals quantify the
benefit of the pointer mechanism with performance increases between about 6 to
10 percentage points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at Interspeech 2023, code will be made available after paper
  presentation (at the latest)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech Diarization and ASR with GMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aayush Kumar Sharma, Vineet Bhavikatti, Amogh Nidawani, Dr. Siddappaji, Sanath P, Dr Geetishree Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research paper, we delve into the topics of Speech Diarization and
Automatic Speech Recognition (ASR). Speech diarization involves the separation
of individual speakers within an audio stream. By employing the ASR transcript,
the diarization process aims to segregate each speaker's utterances, grouping
them based on their unique audio characteristics. On the other hand, Automatic
Speech Recognition refers to the capability of a machine or program to identify
and convert spoken words and phrases into a machine-readable format. In our
speech diarization approach, we utilize the Gaussian Mixer Model (GMM) to
represent speech segments. The inter-cluster distance is computed based on the
GMM parameters, and the distance threshold serves as the stopping criterion.
ASR entails the conversion of an unknown speech waveform into a corresponding
written transcription. The speech signal is analyzed using synchronized
algorithms, taking into account the pitch frequency. Our primary objective
typically revolves around developing a model that minimizes the Word Error Rate
(WER) metric during speech transcription.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnuraSet: A <span class="highlight-title">dataset</span> for benchmarking Neotropical anuran calls
  identification in passive acoustic monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Sebastián Cañas, Maria Paula Toro-Gómez, Larissa Sayuri Moreira Sugai, Hernán Darío Benítez Restrepo, Jorge Rudas, Breyner Posso Bautista, Luís Felipe Toledo, Simone Dena, Adão Henrique Rosa Domingos, Franco Leandro de Souza, Selvino Neckel-Oliveira, Anderson da Rosa, Vítor Carvalho-Rocha, José Vinícius Bernardy, José Luiz Massao Moreira Sugai, Carolina Emília dos Santos, Rogério Pereira Bastos, Diego Llusia, Juan Sebastián Ulloa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global change is predicted to induce shifts in anuran acoustic behavior,
which can be studied through passive acoustic monitoring (PAM). Understanding
changes in calling behavior requires the identification of anuran species,
which is challenging due to the particular characteristics of neotropical
soundscapes. In this paper, we introduce a large-scale multi-species dataset of
anuran amphibians calls recorded by PAM, that comprises 27 hours of expert
annotations for 42 different species from two Brazilian biomes. We provide open
access to the dataset, including the raw recordings, experimental setup code,
and a benchmark with a baseline model of the fine-grained categorization
problem. Additionally, we highlight the challenges of the dataset to encourage
machine learning researchers to solve the problem of anuran call identification
towards conservation policy. All our experiments and resources can be found on
our GitHub repository https://github.com/soundclim/anuraset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LegoNN: Building Modular Encoder-Decoder Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Dalmia, Dmytro Okhonko, Mike Lewis, Sergey Edunov, Shinji Watanabe, Florian Metze, Luke Zettlemoyer, Abdelrahman Mohamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or
automatic speech recognition (ASR)) are constructed and trained end-to-end as
an atomic unit. No component of the model can be (re-)used without the others,
making it impossible to share parts, e.g. a high resourced decoder, across
tasks. We describe LegoNN, a procedure for building encoder-decoder
architectures in a way so that its parts can be applied to other tasks without
the need for any fine-tuning. To achieve this reusability, the interface
between encoder and decoder modules is grounded to a sequence of marginal
distributions over a pre-defined discrete vocabulary. We present two approaches
for ingesting these marginals; one is differentiable, allowing the flow of
gradients across the entire network, and the other is gradient-isolating. To
enable the portability of decoder modules between MT tasks for different source
languages and across other tasks like ASR, we introduce a modality agnostic
encoder which consists of a length control mechanism to dynamically adapt
encoders' output lengths in order to match the expected input length range of
pre-trained decoders. We present several experiments to demonstrate the
effectiveness of LegoNN models: a trained language generation LegoNN decoder
module from German-English (De-En) MT task can be reused without any
fine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT
tasks, matching or beating the performance of baseline. After fine-tuning,
LegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5%
relative WER reduction on the Europarl ASR task. To show how the approach
generalizes, we compose a LegoNN ASR model from three modules -- each has been
learned within different end-to-end trained models on three different datasets
-- achieving an overall WER reduction of 19.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM Transactions on Audio, Speech, and Language Processing
  (TASLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WhisperX: Time-Accurate Speech Transcription of Long-Form Audio <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Bain, Jaesung Huh, Tengda Han, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale, weakly-supervised speech recognition models, such as Whisper,
have demonstrated impressive results on speech recognition across domains and
languages. However, their application to long audio transcription via buffered
or sliding window approaches is prone to drifting, hallucination & repetition;
and prohibits batched transcription due to their sequential nature. Further,
timestamps corresponding each utterance are prone to inaccuracies and
word-level timestamps are not available out-of-the-box. To overcome these
challenges, we present WhisperX, a time-accurate speech recognition system with
word-level timestamps utilising voice activity detection and forced phoneme
alignment. In doing so, we demonstrate state-of-the-art performance on
long-form transcription and word segmentation benchmarks. Additionally, we show
that pre-segmenting audio with our proposed VAD Cut & Merge strategy improves
transcription quality and enables a twelve-fold transcription speedup via
batched inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Style Transfer for Text-to-Speech with ControlVAE and
  Diffusion Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Guan, Tao Li, Yishuang Li, Hukai Huang, Qingyang Hong, Lin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the demand for autonomous control and personalized speech generation,
the style control and transfer in Text-to-Speech (TTS) is becoming more and
more important. In this paper, we propose a new TTS system that can perform
style transfer with interpretability and high fidelity. Firstly, we design a
TTS system that combines variational autoencoder (VAE) and diffusion refiner to
get refined mel-spectrograms. Specifically, a two-stage and a one-stage system
are designed respectively, to improve the audio quality and the performance of
style transfer. Secondly, a diffusion bridge of quantized VAE is designed to
efficiently learn complex discrete style representations and improve the
performance of style transfer. To have a better ability of style transfer, we
introduce ControlVAE to improve the reconstruction quality and have good
interpretability simultaneously. Experiments on LibriTTS dataset demonstrate
that our method is more effective than baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel State Information-Free Location-Privacy Enhancement: Fake Path
  Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxiu Li, Urbashi Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a channel state information-free, fake path injection (FPI)
scheme is proposed for location-privacy preservation. Specifically, structured
artificial noise is designed to introduce virtual fake paths into the channels
of the illegitimate devices. By leveraging the geometrical feasibility of the
fake paths, under mild conditions, it can be proved that the illegitimate
device cannot distinguish between a fake and true path, thus degrading the
illegitimate devices' ability to localize. Two closed-form, lower bounds on the
illegitimate devices' estimation error are derived via the analysis of the
Fisher information of the location-relevant channel parameters, thus
characterizing the enhanced location-privacy. A transmit beamformer is
proposed, which efficiently injects the virtual fake paths. The intended device
receives the two parameters of the beamformer design over a secure channel in
order to enable localization. The impact of leaking the beamformer structure
and associated localization leakage are analyzed. Theoretical analyses are
verified via simulation. Numerical results show that a 20dB degradation of the
illegitimate devices' localization accuracy can be achieved thus validating the
efficacy of the proposed FPI versus using unstructured Gaussian noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Variational Autoencoders for Parameterized MMSE Channel
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Baur, Benedikt Fesl, Wolfgang Utschick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this manuscript, we propose to utilize the generative neural network-based
variational autoencoder for channel estimation. The variational autoencoder
models the underlying true but unknown channel distribution as a conditional
Gaussian distribution in a novel way. The derived channel estimator exploits
the internal structure of the variational autoencoder to parameterize an
approximation of the mean squared error optimal estimator resulting from the
conditional Gaussian channel models. We provide a rigorous analysis under which
conditions a variational autoencoder-based estimator is mean squared error
optimal. We then present considerations that make the variational
autoencoder-based estimator practical and propose three different estimator
variants that differ in their access to channel knowledge during the training
and evaluation phase. In particular, the proposed estimator variant trained
solely on noisy pilot observations is particularly noteworthy as it does not
require access to noise-free, ground-truth channel data during training or
evaluation. Extensive numerical simulations first analyze the internal behavior
of the variational autoencoder-based estimators and then demonstrate excellent
channel estimation performance compared to related classical and machine
learning-based state-of-the-art channel estimators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization of Rate-Splitting Multiple Access in Beyond Diagonal
  RIS-assisted URLLC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Soleymani, Ignacio Santamaria, Eduard Jorswieck, Bruno Clerckx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a general optimization framework for rate splitting
multiple access (RSMA) in beyond diagonal (BD) reconfigurable intelligent
surface (RIS) assisted ultra-reliable low-latency communications (URLLC)
systems. This framework can solve a large family of optimization problems in
which the objective and/or constraints are linear functions of the rates and/or
energy efficiency (EE) of users. Using this framework, we show that RSMA and
RIS can be mutually beneficial tools when the system is overloaded, i.e., when
the number of users per cell is higher than the number of base station (BS)
antennas. Additionally, we show that the benefits of RSMA increase when the
packets are shorter and/or the reliability constraint is more stringent.
Furthermore, we show that the RSMA benefits increase with the number of users
per cell and decrease with the number of BS antennas. Finally, we show that RIS
(either diagonal or BD) can highly improve the system performance, and BD-RIS
outperforms regular RIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to at IEEE journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CareFall: Automatic Fall Detection through Wearable Devices and AI
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carlos Ruiz-Garcia, Ruben Tolosana, Ruben Vera-Rodriguez, Carlos Moro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aging population has led to a growing number of falls in our society,
affecting global public health worldwide. This paper presents CareFall, an
automatic Fall Detection System (FDS) based on wearable devices and Artificial
Intelligence (AI) methods. CareFall considers the accelerometer and gyroscope
time signals extracted from a smartwatch. Two different approaches are used for
feature extraction and classification: i) threshold-based, and ii) machine
learning-based. Experimental results on two public databases show that the
machine learning-based approach, which combines accelerometer and gyroscope
information, outperforms the threshold-based approach in terms of accuracy,
sensitivity, and specificity. This research contributes to the design of smart
and user-friendly solutions to mitigate the negative consequences of falls
among older people.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simplified Method for Optimising Geometrically Shaped Constellations
  of Higher Dimensionality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kadir Gümüş, Bin Chen, Thomas Bradley, Chigo Okonkwo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simplified method for calculating the loss function for use in
geometric shaping, allowing for the optimisation of high dimensional
constellations. We design constellations up to 12D with 4096 points, with gains
up to 0.31 dB compared to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted for the European Conference on Optical
  Communications (ECOC) 2023, this version is a pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Coordinated Transmit Beamforming for Networked Integrated
  Sensing and Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaoyuan Cheng, Yuan Fang, Jie Xu, Derrick Wing Kwan Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies a multi-antenna networked integrated sensing and
communications (ISAC) system, in which a set of multi-antenna base stations
(BSs) employ the coordinated transmit beamforming to serve multiple
single-antenna communication users (CUs) and perform joint target detection by
exploiting the reflected signals simultaneously. To facilitate target sensing,
the BSs transmit dedicated sensing signals combined with their information
signals. Accordingly, we consider two types of CU receivers with and without
the capability of canceling the interference from the dedicated sensing
signals, respectively. In addition, we investigate two scenarios with and
without time synchronization among the BSs. For the scenario with
synchronization, the BSs can exploit the target-reflected signals over both the
direct links (BS-to-target-to-originated BS links) and the cross-links
(BS-to-target-to-other BSs links) for joint detection, while in the
unsynchronized scenario, the BSs can only utilize the target-reflected signals
over the direct links. For each scenario under different types of CU receivers,
we optimize the coordinated transmit beamforming at the BSs to maximize the
minimum detection probability over a particular targeted area, while
guaranteeing the required minimum signal-to-interference-plus-noise ratio
(SINR) constraints at the CUs. These SINR-constrained detection probability
maximization problems are recast as non-convex quadratically constrained
quadratic programs (QCQPs), which are then optimally solved via the
semi-definite relaxation (SDR) technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2211.01085</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best Arm Identification Based Beam Acquisition in Stationary and
  Abruptly Changing Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gourab Ghatak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the initial beam acquisition problem in millimeter wave (mm-wave)
networks from the perspective of best arm identification in multi-armed bandits
(MABs). For the stationary environment, we propose a novel algorithm called
concurrent beam exploration, CBE, in which multiple beams are grouped based on
the beam indices and are simultaneously activated to detect the presence of the
user. The best beam is then identified using a Hamming decoding strategy. For
the case of orthogonal and highly directional thin beams, we characterize the
performance of CBE in terms of the probability of missed detection and false
alarm in a beam group (BG). Leveraging this, we derive the probability of beam
selection error and prove that CBE outperforms the state-of-the-art strategies
in this metric.
  Then, for the abruptly changing environments, e.g., in the case of moving
blockages, we characterize the performance of the classical sequential halving
(SH) algorithm. In particular, we derive the conditions on the distribution of
the change for which the beam selection error is exponentially bounded. In case
the change is restricted to a subset of the beams, we devise a strategy called
K-sequential halving and exhaustive search, K-SHES, that leads to an improved
bound for the beam selection error as compared to SH. This policy is
particularly useful when a near-optimal beam becomes optimal during the
beam-selection procedure due to abruptly changing channel conditions. Finally,
we demonstrate the efficacy of the proposed scheme by employing it in a tandem
beam refinement and data transmission scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Driven Sensing-Node Selection and Power Allocation for Tracking
  Maneuvering Targets in Perceptive Mobile Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Xie, Shenghui Song, Yonina C. Eldar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maneuvering target tracking will be an important service of future wireless
networks to assist innovative applications such as intelligent transportation.
However, tracking maneuvering targets by cellular networks faces many
challenges. For example, the dense network and high-speed targets make the
selection of the sensing nodes (SNs), e.g., base stations, and the associated
power allocation very difficult, given the stringent latency requirement of
sensing applications. Existing methods have demonstrated engaging tracking
performance, but with very high computational complexity. In this paper, we
propose a model-driven deep learning approach for SN selection to meet the
latency requirement. To this end, we first propose an iterative SN selection
method by jointly exploiting the majorization-minimization (MM) framework and
the alternating direction method of multipliers (ADMM). Then, we unfold the
iterative algorithm as a deep neural network (DNN) and prove its convergence.
The proposed model-driven method has a low computational complexity, because
the number of layers is less than the number of iterations required by the
original algorithm, and each layer only involves simple matrix-vector
additions/multiplications. Finally, we propose an efficient power allocation
method based on fixed point (FP) water filling (WF) and solve the joint SN
selection and power allocation problem under the alternative optimization
framework. Simulation results show that the proposed method achieves better
performance than the conventional optimization-based methods with much lower
computational complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Still Waters Run Deep: Extend THz Coverage with Non-Intelligent
  Reflecting Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Han, Yuanbo Li, Yinqin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large reflection and diffraction losses in the Terahertz (THz) band give rise
to degraded coverage abilities in non-line-of-sight (NLoS) areas. To overcome
this, a non-intelligent reflecting surface (NIRS) can be used, which is
essentially a rough surface made by metal materials. NIRS is not only able to
enhance received power in large NLoS areas through rich reflections and
scattering, but also costless and super-easy to fabricate and implement. In
this article, we first thoroughly compare NIRS with the lively discussed
intelligent reflecting surface (IRS) and point out the unique advantages of
NIRS over IRS. Furthermore, experimental results are elaborated to show the
effectiveness of NIRS in improving coverage. Last but not least, open problems
and future directions are highlighted to inspire future research efforts on
NIRS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 page, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Radio Frequency Fingerprints Identification via Multi-antenna
  Receiver 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofang Chen, Wenbo Xu, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Internet of Things (IoT), radio frequency fingerprints (RFF) technology
has been widely used for passive security authentication to identify the
special emitter. However, few works took advantage of independent oscillator
distortions at the receiver side, and no work has yet considered filtering
receiver distortions. In this paper, we investigate the RFF identification
(RFFI) involving unknown receiver distortions, where the phase noise caused by
each antenna oscillator is independent. Three RFF schemes are proposed
according to the number of receiving antennas. When the number is small, the
Mutual Information Weighting Scheme (MIWS) is developed by calculating the
weighted voting of RFFI result at each antenna; when the number is moderate,
the Distortions Filtering Scheme (DFS) is developed by filtering out the
channel noise and receiver distortions; when the number is large enough, the
Group-Distortions Filtering and Weighting Scheme (GDFWS) is developed, which
integrates the advantages of MIWS and DFS. Furthermore, the ability of DFS to
filter out the channel noise and receiver distortions is theoretically analyzed
at a specific confidence level. Experiments are provided when both channel
noise and receiver distortions exist, which verify the effectiveness and
robustness of the proposed schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Polarized IRS-Assisted MIMO Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muteen Munawar, Kyungchun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study considers a dual-polarized intelligent reflecting surface
(DP-IRS)-assisted multiple-input multiple-output (MIMO) single-user wireless
communication system. The transmitter and receiver are equipped with DP
antennas, and each antenna features a separate phase shifter for each
polarization. We attempt to maximize the system's spectral efficiency (SE) by
optimizing the operations of the reflecting elements at the DP-IRS,
precoder/combiner at the transmitter/receiver, and vertical/horizontal phase
shifters at the DP antennas. To address this problem, we propose a three-step
alternating optimization (AO) algorithm based on the semi-definite relaxation
method. Next, we consider asymptotically low/high signal-to-noise ratio (SNR)
regimes and propose low-complexity algorithms. In particular, for the low-SNR
regime, we derive computationally low-cost closed-form solutions. According to
the obtained numerical results, the proposed algorithm outperforms the various
benchmark schemes. Specifically, our main algorithm exhibits a 65.6 \% increase
in the SE performance compared to random operations. In addition, we compare
the SE performance of DP-IRS with that of simple IRS (S-IRS). For \(N = 50\),
DP-IRS achieves 24.8 \%, 28.2 \%, and 30.3 \% improvements in SE for \({4}
\times {4}\), \({8} \times {8}\), and \({16} \times {16}\) MIMO, respectively,
compared to S-IRS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 13 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thin-Film Lithium Niobate Acoustic Resonator with High Q of 237 and k2
  of 5.1% at 50.74 GHz 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Kramer, Vakhtang Chulukhadze, Kenny Huynh, Omar Barrera, Michael Liao, Sinwoo Cho, Lezli Matto, Mark S. Goorsky, Ruochen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work reports a 50.74 GHz lithium niobate (LiNbO3) acoustic resonator
with a high quality factor (Q) of 237 and an electromechanical coupling (k2) of
5.17% resulting in a figure of merit (FoM, Q x k2) of 12.2. The LiNbO3
resonator employs a novel bilayer periodically poled piezoelectric film (P3F)
128 Y-cut LiNbO3 on amorphous silicon (a-Si) on sapphire stack to achieve low
losses and high coupling at millimeter wave (mm-wave). The device also shows a
Q of 159, k2 of 65.06%, and FoM of 103.4 for the 16.99 GHz tone. This result
shows promising prospects of P3F LiNbO3 towards mm-wave front-end filters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, published in 2023 Joint Conference of the IEEE
  International Frequency Control Symposium & European Frequency and Time Forum
  (IEEE IFCS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Underdetermined Blind Identification via $k$-Sparse Component Analysis:
  RANSAC-driven Orthogonal Subspace Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.03739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.03739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Eqlimi, Bahador Makkiabadi, Mayadeh Kouti, Ardeshir Fotouhi, Saeid Sanei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two primary families of methods exist for underdetermined blind
identification (UBI) based on the sparsity of the source matrix: sparse
component analysis (SCA) and $k$-SCA. SCA assumes one active source at each
time instant, while $k$-SCA allows for varying numbers of active sources
represented by $k$. However, existing $k$-SCA methods, which claim to solve UBI
problems by accommodating $k$-sparse sources, predominantly rely on $1$-sparse
sources, limiting their effectiveness in real-world scenarios with high noise
levels.
  In this paper, we propose an effective and computationally less complex
approach for UBI, specifically focusing on the challenging case when the number
of active sources is equal to the number of sensors minus one ($k=m-1$). Our
approach overcomes limitations by using a two-step scenario: (1) estimating the
orthogonal complement subspaces of the overall space and (2) identifying the
mixing vectors. We present an integrated algorithm based on the Gram-Schmidt
process and random sample consensus (RANSAC) method to solve both steps.
Experimental results using simulated data demonstrate the superior
effectiveness of our proposed method compared to existing algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation Method and Design Guidance for Direction Finding Antenna
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Grundmann, Dirk Manteuffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A deterministic evaluation procedure for multi-port direction finding
antennas is proposed. It is based on a direction finding uncertainty parameter,
which describes how well different directions of arrival and polarizations are
distinguishable. By investigating a simple antenna array, it is shown that the
proposed parameter provides additional insight into the behavior of an antenna
system, when compared to established methods. Moreover, since the uncertainty
parameter is calculated from a set of far fields, it is applicable to port far
fields as well as Characteristic Modes. This finding is utilized to derive a
design guidance: Starting with a set of Characteristic Mode far fields, the
angular distribution of the uncertainty is investigated to verify that no
ambiguities are present. Different sets of far fields are compared and the
differences regarding their direction finding behavior are visualized and
explained using the uncertainty in conjunction with an estimate of the incident
field. To quantify these differences, a key performance indicator is introduced
that summarizes the direction finding capabilities over a selected angular
region. To demonstrate the design process, a multi-mode multi-port antenna with
three uncorrelated ports is developed, manufactured and measured.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in IEEE Transactions
  on Antennas and Propagation. This is the author's version which has not been
  fully edited and content may change prior to final publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbol-Level Noise-Guessing Decoding with Antenna Sorting for URLLC
  Massive MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Allahkaram, Francisco A. Monteiro, Ioannis Chatzigeorgiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supporting ultra-reliable and low-latency communication (URLLC) is a
challenge in current wireless systems. Channel codes that generate large
codewords improve reliability but necessitate the use of interleavers, which
introduce undesirable latency. Only short codewords can eliminate the
requirement for interleaving and reduce decoding latency. This paper suggests a
coding and decoding method which, when combined with the high spectral
efficiency of spatial multiplexing, can provide URLLC over a fading channel.
Random linear coding and high-order modulation are used to transmit information
over a massive multiple-input multiple-output (mMIMO) channel, followed by
zero-forcing detection and guessing random additive noise decoding (GRAND) at a
receiver. A variant of GRAND, called symbol-level GRAND, originally proposed
for single-antenna systems that employ high-order modulation schemes, is
generalized to spatial multiplexing. The paper studies the impact of the
orthogonality defect of the underlying mMIMO lattice on symbol-level GRAND, and
proposes to leverage side-information that comes from the mMIMO channel-state
information and relates to the reliability of each receive antenna. This
induces an antenna sorting step, which further reduces decoding complexity by
over 80\% when compared to bit-level GRAND.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Selection for Wi-Fi 7 Multi-Link Operation via
  Optimistic-Weighted VDN and Parallel Transfer Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Enrique Iturria-Rivera, Marcel Chenier, Bernard Herscovici, Burak Kantarci, Melike Erol-Kantarci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense and unplanned IEEE 802.11 Wireless Fidelity(Wi-Fi) deployments and the
continuous increase of throughput and latency stringent services for users have
led to machine learning algorithms to be considered as promising techniques in
the industry and the academia. Specifically, the ongoing IEEE 802.11be EHT --
Extremely High Throughput, known as Wi-Fi 7 -- amendment propose, for the first
time, Multi-Link Operation (MLO). Among others, this new feature will increase
the complexity of channel selection due the novel multiple interfaces proposal.
In this paper, we present a Parallel Transfer Reinforcement Learning
(PTRL)-based cooperative Multi-Agent Reinforcement Learning (MARL) algorithm
named Parallel Transfer Reinforcement Learning Optimistic-Weighted Value
Decomposition Networks (oVDN) to improve intelligent channel selection in IEEE
802.11be MLO-capable networks. Additionally, we compare the impact of different
parallel transfer learning alternatives and a centralized non-transfer MARL
baseline. Two PTRL methods are presented: Multi-Agent System (MAS) Joint
Q-function Transfer, where the joint Q-function is transferred and MAS
Best/Worst Experience Transfer where the best and worst experiences are
transferred among MASs. Simulation results show that oVDNg -- only the best
experiences are utilized -- is the best algorithm variant. Moreover, oVDNg
offers a gain up to 3%, 7.2% and 11% when compared with VDN, VDN-nonQ and
non-PTRL baselines. Furthermore, oVDNg experienced a reward convergence gain in
the 5 GHz interface of 33.3% over oVDNb and oVDN where only worst and both
types of experiences are considered, respectively. Finally, our best PTRL
alternative showed an improvement over the non-PTRL baseline in terms of speed
of convergence up to 40 episodes and reward up to 135%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE PIMRC'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Packet Detection for Random Access Networks: Analysis,
  Benchmark, and Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Du, Soung Chang Liew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reexamines and fundamentally improves the Schmidl-and-Cox (S&C)
algorithm, which is extensively used for packet detection in wireless networks,
and enhances its adaptability for multi-antenna receivers. First, we introduce
a new "compensated autocorrelation" metric, providing a more analytically
tractable solution with precise expressions for false-alarm and
missed-detection probabilities. Second, this paper proposes the Pareto
comparison principle for fair benchmarking packet-detection algorithms,
considering both false alarms and missed detections simultaneously. Third, with
the Pareto benchmarking scheme, we experimentally confirm that the performance
of S&C can be greatly improved by taking only the real part and discarding the
imaginary part of the autocorrelation, leading to the novel real-part S&C
(RP-S&C) scheme. Fourth, and perhaps most importantly, we utilize the
compensated autocorrelation metric we newly put forth to extend the
single-antenna algorithm to multi-antenna scenarios through a weighted-sum
approach. Two optimization problems, minimizing false-alarm and
missed-detection probabilities respectively, are formulated and solutions are
provided. Our experimental results reveal that the optimal weights for false
alarms (WFA) scheme is more desirable than the optimal weights for missed
detections (WMD) due to its simplicity, reliability, and superior performance.
This study holds considerable implications for the design and deployment of
packet-detection schemes in random-access networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Quantile Optimization for Edge-Cloud Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Du, He Zhang, Xiangle Cheng, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We seek the best traffic allocation scheme for the edge-cloud computing
network that satisfies constraints and minimizes the cost based on burstable
billing. First, for a fixed network topology, we formulate a family of integer
programming problems with random parameters describing the various traffic
demands. Then, to overcome the difficulty caused by the discrete feature of the
problem, we generalize the Gumbel-softmax reparameterization method to induce
an unconstrained continuous optimization problem as a regularized continuation
of the discrete problem. Finally, we introduce the Gumbel-softmax sampling
network to solve the optimization problems via unsupervised learning. The
network structure reflects the edge-cloud computing topology and is trained to
minimize the expectation of the cost function for unconstrained continuous
optimization problems. The trained network works as an efficient traffic
allocation scheme sampler, remarkably outperforming the random strategy in
feasibility and cost function value. Besides testing the quality of the output
allocation scheme, we examine the generalization property of the network by
increasing the time steps and the number of users. We also feed the solution to
existing integer optimization solvers as initial conditions and verify the
warm-starts can accelerate the short-time iteration process. The framework is
general with solid performance, and the decoupled feature of the random neural
networks is adequate for practical implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entanglement Distribution in the Quantum Internet: Knowing when to Stop! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angela Sara Cacciapuoti, Michele Viscardi, Jessica Illiano, Marcello Caleffi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entanglement distribution is a key functionality of the Quantum Internet.
However, quantum entanglement is very fragile, easily degraded by decoherence,
which strictly constraints the time horizon within the distribution has to be
completed. This, coupled with the quantum noise irremediably impinging on the
channels utilized for entanglement distribution, may imply the need to attempt
the distribution process multiple times before the targeted network nodes
successfully share the desired entangled state. And there is no guarantee that
this is accomplished within the time horizon dictated by the coherence times.
As a consequence, in noisy scenarios requiring multiple distribution attempts,
it may be convenient to stop the distribution process early. In this paper, we
take steps in the direction of knowing when to stop the entanglement
distribution by developing a theoretical framework, able to capture the quantum
noise effects. Specifically, we first prove that the entanglement distribution
process can be modeled as a Markov decision process. Then, we prove that the
optimal decision policy exhibits attractive features, which we exploit to
reduce the computational complexity. The developed framework provides quantum
network designers with flexible tools to optimally engineer the design
parameters of the entanglement distribution process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Non-Cumulative Objective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Cui, Wei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, the objective is almost always defined as a
\emph{cumulative} function over the rewards along the process. However, there
are many optimal control and reinforcement learning problems in various
application fields, especially in communications and networking, where the
objectives are not naturally expressed as summations of the rewards. In this
paper, we recognize the prevalence of non-cumulative objectives in various
problems, and propose a modification to existing algorithms for optimizing such
objectives. Specifically, we dive into the fundamental building block for many
optimal control and reinforcement learning algorithms: the Bellman optimality
equation. To optimize a non-cumulative objective, we replace the original
summation operation in the Bellman update rule with a generalized operation
corresponding to the objective. Furthermore, we provide sufficient conditions
on the form of the generalized operation as well as assumptions on the Markov
decision process under which the globally optimal convergence of the
generalized Bellman updates can be guaranteed. We demonstrate the idea
experimentally with the bottleneck objective, i.e., the objectives determined
by the minimum reward along the process, on classical optimal control and
reinforcement learning tasks, as well as on two network routing problems on
maximizing the flow rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures. To appear in IEEE Transactions on Machine
  Learning in Communications and Networking (TMLCN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What do LLMs need to Synthesize Correct Router Configurations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajdeep Mondal, Alan Tang, Ryan Beckett, Todd Millstein, George Varghese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate whether Large Language Models (e.g., GPT-4) can synthesize
correct router configurations with reduced manual effort. We find GPT-4 works
very badly by itself, producing promising draft configurations but with
egregious errors in topology, syntax, and semantics. Our strategy, that we call
Verified Prompt Programming, is to combine GPT-4 with verifiers, and use
localized feedback from the verifier to automatically correct errors.
Verification requires a specification and actionable localized feedback to be
effective. We show results for two use cases: translating from Cisco to Juniper
configurations on a single router, and implementing no-transit policy on
multiple routers. While human input is still required, if we define the
leverage as the number of automated prompts to the number of human prompts, our
experiments show a leverage of 10X for Juniper translation, and 6X for
implementing no-transit policy, ending with verified configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Federated Learning: Fundamentals, State of the Art,
  Frameworks, Trends, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08413v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08413v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrique Tomás Martínez Beltrán, Mario Quiles Pérez, Pedro Miguel Sánchez Sánchez, Sergio López Bernal, Gérôme Bovet, Manuel Gil Pérez, Gregorio Martínez Pérez, Alberto Huertas Celdrán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, Federated Learning (FL) has gained relevance in training
collaborative models without sharing sensitive data. Since its birth,
Centralized FL (CFL) has been the most common approach in the literature, where
a central entity creates a global model. However, a centralized approach leads
to increased latency due to bottlenecks, heightened vulnerability to system
failures, and trustworthiness concerns affecting the entity responsible for the
global model creation. Decentralized Federated Learning (DFL) emerged to
address these concerns by promoting decentralized model aggregation and
minimizing reliance on centralized architectures. However, despite the work
done in DFL, the literature has not (i) studied the main aspects
differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and
evaluate new solutions; and (iii) reviewed application scenarios using DFL.
Thus, this article identifies and analyzes the main fundamentals of DFL in
terms of federation architectures, topologies, communication mechanisms,
security approaches, and key performance indicators. Additionally, the paper at
hand explores existing mechanisms to optimize critical DFL fundamentals. Then,
the most relevant features of the current DFL frameworks are reviewed and
compared. After that, it analyzes the most used DFL application scenarios,
identifying solutions based on the fundamentals and frameworks previously
defined. Finally, the evolution of existing DFL solutions is studied to provide
a list of trends, lessons learned, and open challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Security and Privacy of IP-ICN Coexistence: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enkeleda Bardhi, Mauro Conti, Riccardo Lazzeretti, Eleonora Losiouk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today Internet is experiencing a massive number of users with a continuously
increasing need for data, which is the leading cause of introduced limitations
among security and privacy issues. To overcome these limitations, a shift from
host-centric to data-centric is proposed, and in this context,
Information-Centric Networking (ICN) represents a promising solution.
Nevertheless, unsettling the current Internet network layer, i.e., Internet
Protocol (IP), with ICN is a challenging, expensive task since it requires
worldwide coordination among Internet Service Providers (ISPs), backbone, and
Autonomous Services (AS). Therefore, researchers foresee that the replacement
process of the current Internet will transition through the coexistence of IP
and ICN. In this perspective, novel architectures combine IP and ICN protocols.
However, only a few of the proposed architectures place the security-by-design
feature. Therefore, this article provides the first comprehensive Security and
Privacy (SP) analysis of the state-of-the-art IP-ICN coexistence architectures
by horizontally comparing the SP features among three deployment approaches,
i.e., overlay, underlay, and hybrid, and vertically comparing among the ten
considered SP features. Lastly, the article sheds light on the open issues and
possible future directions for IP-ICN coexistence. Our analysis shows that most
architectures fail to provide several SP features, including data and traffic
flow confidentiality, availability, and anonymity of communication. Thus, this
article shows the secure combination of current and future protocol stacks
during the coexistence phase that the Internet will definitely walk across.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Worldwide Look Into Mobile Access Networks Through the Eyes of AmiGos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Varvello, Yasir Zaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How does the mobile experience compare between Germany and Nigeria? There is
currently no public data or test-bed to provide an answer to this question.
This is because deploying and maintaining such test-bed can be both challenging
and expensive. To fill this gap, this paper proposes a novel test-bed design
called "AmiGo", which relies on travelers carrying mobile phones to act as
vantage points and collect data on mobile network performance. The "AmiGo"
design has three key advantages: it is easy to deploy, has realistic user
mobility, and runs on real Android devices. We further developed a suite of
measurement tools for "AmiGo" to perform network measurements, e.g., pings,
speedtests, and webpage loads. We leverage these tools to measure the
performance of 24 mobile networks across five continents over a month via an
"AmiGo" deployment involving 31 students. We find that 50% of networks face a
40-70% chance of providing low data rates, only 20% achieve low latencies, and
networks in Asia, Central/South America, and Africa have significantly higher
CDN download times than in Europe. Most news websites load slowly, while
YouTube performs well. We made both test-bed and measurement tools open source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection Threshold of Audio Haptic Asynchrony in a Driving Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyanendra Sharma, Hiroshi Yasuda, Manuel Kuehner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to provide perceptually accurate multimodal feedback during driving
situations, it is vital to understand the threshold at which drivers are able
to recognize asyncrony between multiple incoming Stimuli. In this work, we
investigated and report the \textit{detection threshold} (DT) of asynchrony
between audio and haptic feedback, in the context of a force feedback steering
wheel. We designed the experiment to loosely resemble a driving situation where
the haptic feedback was provided through a steering wheel
(\textit{Sensodrive}), while the accompanying audio was played through noise
cancelling headphones. Both feedbacks were designed to resemble rumble strips,
that are generally installed on the side of major roadways as a safety tool.
The results indicate that, for $50\%$ of the participants, asynchrony was
detectable outside the range of -75 ms and 110 ms, where the former is related
to perceiving audio before haptic and vice versa for the latter. We were also
able to concur with previous studies, which state that latency is perceivable
at a lower threshold when audio precedes haptic stimuli.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing for <span class="highlight-title">Review</span>er Anchoring in Peer <span class="highlight-title">Review</span>: A Randomized Controlled
  Trial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Liu, Steven Jecmen, Vincent Conitzer, Fei Fang, Nihar B. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peer review frequently follows a process where reviewers first provide
initial reviews, authors respond to these reviews, then reviewers update their
reviews based on the authors' response. There is mixed evidence regarding
whether this process is useful, including frequent anecdotal complaints that
reviewers insufficiently update their scores. In this study, we aim to
investigate whether reviewers anchor to their original scores when updating
their reviews, which serves as a potential explanation for the lack of updates
in reviewer scores.
  We design a novel randomized controlled trial to test if reviewers exhibit
anchoring. In the experimental condition, participants initially see a flawed
version of a paper that is later corrected, while in the control condition,
participants only see the correct version. We take various measures to ensure
that in the absence of anchoring, reviewers in the experimental group should
revise their scores to be identically distributed to the scores from the
control group. Furthermore, we construct the reviewed paper to maximize the
difference between the flawed and corrected versions, and employ deception to
hide the true experiment purpose.
  Our randomized controlled trial consists of 108 researchers as participants.
First, we find that our intervention was successful at creating a difference in
perceived paper quality between the flawed and corrected versions: Using a
permutation test with the Mann-Whitney U statistic, we find that the
experimental group's initial scores are lower than the control group's scores
in both the Evaluation category (Vargha-Delaney A=0.64, p=0.0096) and Overall
score (A=0.59, p=0.058). Next, we test for anchoring by comparing the
experimental group's revised scores with the control group's scores. We find no
significant evidence of anchoring in either the Overall (A=0.50, p=0.61) or
Evaluation category (A=0.49, p=0.61).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages (19 including references and appendix), 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enriching Verbal Feedback from Usability Testing: Automatic Linking of
  Thinking-Aloud Recordings and Stimulus using Eye Tracking and Mouse Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supriya Murali, Tina Walber, Christoph Schaefer, Sezen Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The think aloud method is an important and commonly used tool for usability
optimization. However, analyzing think aloud data could be time consuming. In
this paper, we put forth an automatic analysis of verbal protocols and test the
link between spoken feedback and the stimulus using eye tracking and mouse
tracking. The gained data - user feedback linked to a specific area of the
stimulus - could be used to let an expert review the feedback on specific web
page elements or to visualize on which parts of the web page the feedback was
given. Specifically, we test if participants fixate on or point with the mouse
to the content of the webpage that they are verbalizing. During the testing,
participants were shown three websites and asked to verbally give their
opinion. The verbal responses, along with the eye and cursor movements were
recorded. We compared the hit rate, defined as the percentage of verbally
mentioned areas of interest (AOIs) that were fixated with gaze or pointed to
with the mouse. The results revealed a significantly higher hit rate for the
gaze compared to the mouse data. Further investigation revealed that, while the
mouse was mostly used passively to scroll, the gaze was often directed towards
relevant AOIs, thus establishing a strong association between spoken words and
stimuli. Therefore, eye tracking data possibly provides more detailed
information and more valuable insights about the verbalizations compared to the
mouse data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Use of <span class="highlight-title">Self-Supervised</span> Speech Representations in Spontaneous
  Speech Synthesis <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) speech representations learned from large
amounts of diverse, mixed-quality speech data without transcriptions are
gaining ground in many speech technology applications. Prior work has shown
that SSL is an effective intermediate representation in two-stage
text-to-speech (TTS) for both read and spontaneous speech. However, it is still
not clear which SSL and which layer from each SSL model is most suited for
spontaneous TTS. We address this shortcoming by extending the scope of
comparison for SSL in spontaneous TTS to 6 different SSLs and 3 layers within
each SSL. Furthermore, SSL has also shown potential in predicting the mean
opinion scores (MOS) of synthesized speech, but this has only been done in
read-speech MOS prediction. We extend an SSL-based MOS prediction framework
previously developed for scoring read speech synthesis and evaluate its
performance on synthesized spontaneous speech. All experiments are conducted
twice on two different spontaneous corpora in order to find generalizable
trends. Overall, we present comprehensive experimental results on the use of
SSL in spontaneous TTS and MOS prediction to further quantify and understand
how SSL can be used in spontaneous TTS. Audios samples:
https://www.speech.kth.se/tts-demos/sp_ssl_tts
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures. 12th ISCA Speech Synthesis Workshop (SSW) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OntoChat<span class="highlight-title">GPT</span> Information System: Ontology-Driven Structured <span class="highlight-title">Prompt</span>s for
  Chat<span class="highlight-title">GPT</span> Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Palagin, Vladislav Kaverinskiy, Anna Litvin, Kyrylo Malakhov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a comprehensive methodology for utilizing an
ontology-driven structured prompts system in interplay with ChatGPT, a widely
used large language model (LLM). The study develops formal models, both
information and functional, and establishes the methodological foundations for
integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.
The resulting productive triad comprises the methodological foundations,
advanced information technology, and the OntoChatGPT system, which collectively
enhance the effectiveness and performance of chatbot systems. The
implementation of this technology is demonstrated using the Ukrainian language
within the domain of rehabilitation. By applying the proposed methodology, the
OntoChatGPT system effectively extracts entities from contexts, classifies
them, and generates relevant responses. The study highlights the versatility of
the methodology, emphasizing its applicability not only to ChatGPT but also to
other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2
LLM. The underlying principles of meta-learning, structured prompts, and
ontology-driven information retrieval form the core of the proposed
methodology, enabling their adaptation and utilization in various LLM-based
systems. This versatile approach opens up new possibilities for NLP and
dialogue systems, empowering developers to enhance the performance and
functionality of chatbot systems across different domains and languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure. Published. International Journal of Computing,
  22(2), 170-183. https://doi.org/10.47839/ijc.22.2.3086</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SnakeSynth: New Interactions for Generative Audio Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Easthope
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I present "SnakeSynth," a web-based lightweight audio synthesizer that
combines audio generated by a deep generative model and real-time continuous
two-dimensional (2D) input to create and control variable-length generative
sounds through 2D interaction gestures. Interaction gestures are touch and
mobile-compatible with analogies to strummed, bowed, and plucked musical
instrument controls. Point-and-click and drag-and-drop gestures directly
control audio playback length and I show that sound length and intensity are
modulated by interactions with a programmable 2D coordinate grid. Leveraging
the speed and ubiquity of browser-based audio and hardware acceleration in
Google's TensorFlow.js we generate time-varying high-fidelity sounds with
real-time interactivity. SnakeSynth adaptively reproduces and interpolates
between sounds encountered during model training, notably without long training
times, and I briefly discuss possible futures for deep generative models as an
interactive paradigm for musical expression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can a Chatbot Support Exploratory Software Testing? Preliminary Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubens Copche, Yohan Duarte Pessanha, Vinicius Durelli, Marcelo Medeiros Eler, Andre Takeshi Endo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tests executed by human testers are still widespread in practice and fill the
gap left by limitations of automated approaches. Among the human-centered
approaches, exploratory testing is the de facto approach in agile teams.
Although it is focused on the expertise and creativity of the tester, the
activity of exploratory testing may benefit from support provided by an
automated agent that interacts with the human testers. This paper presents a
chatbot, called BotExpTest, designed to support testers while performing
exploratory tests of software applications. We implemented BotExpTest on top of
the instant messaging social platform Discord; this version includes
functionalities to report bugs and issues, time management of test sessions,
guidelines for app testing, and presentation of exploratory testing strategies.
To assess BotExpTest, we conducted a user study with six software engineering
professionals. They carried out two sessions performing exploratory tests along
with BotExpTest. Participants were capable of revealing bugs and found the
experience to interact with the chatbot positive. Preliminary analyses indicate
that chatbot-enabled exploratory testing may be as effective as similar
approaches and help testers to uncover different bugs. Bots are shown to be
valuable resources for Software Engineering, and initiatives like BotExpTest
may help to improve the effectiveness of testing activities like exploratory
testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research Protocol for the Google Health Digital Well-being Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel McDuff, Andrew Barakat, Ari Winbush, Allen Jiang, Felicia Cordeiro, Ryann Crowley, Lauren E. Kahn, John Hernandez, Nicholas B. Allen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impact of digital device use on health and well-being is a pressing
question to which individuals, families, schools, policy makers, legislators,
and digital designers are all demanding answers. However, the scientific
literature on this topic to date is marred by small and/or unrepresentative
samples, poor measurement of core constructs (e.g., device use, smartphone
addiction), and a limited ability to address the psychological and behavioral
mechanisms that may underlie the relationships between device use and
well-being. A number of recent authoritative reviews have made urgent calls for
future research projects to address these limitations. The critical role of
research is to identify which patterns of use are associated with benefits
versus risks, and who is more vulnerable to harmful versus beneficial outcomes,
so that we can pursue evidence-based product design, education, and regulation
aimed at maximizing benefits and minimizing risks of smartphones and other
digital devices. We describe a protocol for a Digital Well-Being (DWB) study to
help answer these questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidence-based Hand Hygiene. Can You Trust the Fluorescent-based
  Assessment Methods? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Száva Bánsághi, Viola Sári, Péter Szerémy, Ákos Lehotsky, Bence Takács, Brigitta K. Tóth, Tamás Haidegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare-Associated Infections present a major threat to patient safety
globally. According to studies, more than 50% of HAI could be prevented by
proper hand hygiene. Effectiveness of hand hygiene is regularly evaluated with
the fluorescent method: performing hand hygiene with a handrub containing an
ultra violet (UV) fluorescent marker. Typically, human experts evaluate the
hands under UV-A light, and decide whether the applied handrub covered the
whole hand surface. The aim of this study was to investigate how different
experts judge the same UV-pattern, and compare that to microbiology for
objective validation. Hands of volunteer participants were contaminated with
high concentration of a Staphylococcus epidermidis suspension. Hands were
incompletely disinfected with UV-labeled handrub. Four different UV-box type
devices were used to take CCD pictures of the hands under UV light. Size of
inadequately disinfected areas on the hands were determined in two different
ways. First, based on microbiology; the areas where colonies were grown were
measured. Second, four independent senior infection control specialists were
asked to mark the missed areas on printed image, captured under UV light. 8
hands of healthy volunteers were examined. Expert evaluations were highly
uncorrelated (regarding interrater reliability) and inconsistent. Microbiology
results weakly correlated with the expert evaluations. In half of the cases,
there were more than 10% difference in the size of properly disinfected area,
as measured by microbiology versus human experts. Considering the result of the
expert evaluations, variability was disconcertingly high. Evaluating the
fluorescent method is challenging, even for highly experienced professionals. A
patient safety quality assurance system cannot be built on these data quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying human-centered AI in developing effective human-AI teaming: A
  perspective of human-AI joint cognitive systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xu, Zaifeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the "teaming" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
  Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Instructions for Intuitive Human Interaction with
  Robotic Assistants in Field Construction Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somin Park, Xi Wang, Carol C. Menassa, Vineet R. Kamat, Joyce Y. Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of robots is widely considered to have significant potential
of alleviating the issues of worker shortage and stagnant productivity that
afflict the construction industry. However, it is challenging to use fully
automated robots in complex and unstructured construction sites. Human-Robot
Collaboration (HRC) has shown promise of combining human workers' flexibility
and robot assistants' physical abilities to jointly address the uncertainties
inherent in construction work. When introducing HRC in construction, it is
critical to recognize the importance of teamwork and supervision in field
construction and establish a natural and intuitive communication system for the
human workers and robotic assistants. Natural language-based interaction can
enable intuitive and familiar communication with robots for human workers who
are non-experts in robot programming. However, limited research has been
conducted on this topic in construction. This paper proposes a framework to
allow human workers to interact with construction robots based on natural
language instructions. The proposed method consists of three stages: Natural
Language Understanding (NLU), Information Mapping (IM), and Robot Control (RC).
Natural language instructions are input to a language model to predict a tag
for each word in the NLU module. The IM module uses the result of the NLU
module and building component information to generate the final instructional
output essential for a robot to acknowledge and perform the construction task.
A case study for drywall installation is conducted to evaluate the proposed
approach. The obtained results highlight the potential of using natural
language-based interaction to replicate the communication that occurs between
human workers within the context of human-robot teams.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">95</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and
  Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Bai, Steffi Agino Priyanka, Hsiao-Jung Tung, Yuankai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the low accuracy of object detection and recognition in many
intelligent surveillance systems at nighttime, the quality of night images is
crucial. Compared with the corresponding daytime image, nighttime image is
characterized as low brightness, low contrast and high noise. In this paper, a
bio-inspired image enhancement algorithm is proposed to convert a low
illuminance image to a brighter and clear one. Different from existing
bio-inspired algorithm, the proposed method doesn't use any training sequences,
we depend on a novel chain of contrast enhancement and denoising algorithms
without using any forms of recursive functions. Our method can largely improve
the brightness and contrast of night images, besides, suppress noise. Then we
implement on real experiment, and simulation experiment to test our algorithms.
Both results show the advantages of proposed algorithm over contrast pair,
Meylan and Retinex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Cognitive Systems and Signal Processing
  (2016)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ISLTranslate: <span class="highlight-title">Dataset</span> for Translating Indian Sign Language <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Joshi, Susmit Agrawal, Ashutosh Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign languages are the primary means of communication for many
hard-of-hearing people worldwide. Recently, to bridge the communication gap
between the hard-of-hearing community and the rest of the population, several
sign language translation datasets have been proposed to enable the development
of statistical sign language translation systems. However, there is a dearth of
sign language resources for the Indian sign language. This resource paper
introduces ISLTranslate, a translation dataset for continuous Indian Sign
Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best
of our knowledge, it is the largest translation dataset for continuous Indian
Sign Language. We provide a detailed analysis of the dataset. To validate the
performance of existing end-to-end Sign language to spoken language translation
systems, we benchmark the created dataset with a transformer-based model for
ISL translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023 Findings, 8 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D detection of roof sections from a single satellite image and
  application to LOD2-building reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johann Lussange, Mulin Yu, Yuliya Tarabalka, Florent Lafarge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing urban areas in 3D out of satellite raster images has been a
long-standing and challenging goal of both academical and industrial research.
The rare methods today achieving this objective at a Level Of Details $2$ rely
on procedural approaches based on geometry, and need stereo images and/or LIDAR
data as input. We here propose a method for urban 3D reconstruction named
KIBS(\textit{Keypoints Inference By Segmentation}), which comprises two novel
features: i) a full deep learning approach for the 3D detection of the roof
sections, and ii) only one single (non-orthogonal) satellite raster image as
model input. This is achieved in two steps: i) by a Mask R-CNN model performing
a 2D segmentation of the buildings' roof sections, and after blending these
latter segmented pixels within the RGB satellite raster image, ii) by another
identical Mask R-CNN model inferring the heights-to-ground of the roof
sections' corners via panoptic segmentation, unto full 3D reconstruction of the
buildings and city. We demonstrate the potential of the KIBS method by
reconstructing different urban areas in a few minutes, with a Jaccard index for
the 2D segmentation of individual roof sections of $88.55\%$ and $75.21\%$ on
our two data sets resp., and a height's mean error of such correctly segmented
pixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets
resp., hence within the LOD2 precision range.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-Agnostic Neural Architecture for Class Incremental Continual
  Learning in Document Processing Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Wójcik, Witold Kościukiewicz, Mateusz Baran, Tomasz Kajdanowicz, Adam Gonczarek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Production deployments in complex systems require ML architectures to be
highly efficient and usable against multiple tasks. Particularly demanding are
classification problems in which data arrives in a streaming fashion and each
class is presented separately. Recent methods with stochastic gradient learning
have been shown to struggle in such setups or have limitations like memory
buffers, and being restricted to specific domains that disable its usage in
real-world scenarios. For this reason, we present a fully differentiable
architecture based on the Mixture of Experts model, that enables the training
of high-performance classifiers when examples from each class are presented
separately. We conducted exhaustive experiments that proved its applicability
in various domains and ability to learn online in production environments. The
proposed technique achieves SOTA results without a memory buffer and clearly
outperforms the reference methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2211.14963</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handwritten Text Recognition Using Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atman Mishra, A. Sharath Ram, Kavyashree C
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OCR (Optical Character Recognition) is a technology that offers comprehensive
alphanumeric recognition of handwritten and printed characters at electronic
speed by merely scanning the document. Recently, the understanding of visual
data has been termed Intelligent Character Recognition (ICR). Intelligent
Character Recognition (ICR) is the OCR module that can convert scans of
handwritten or printed characters into ASCII text. ASCII data is the standard
format for data encoding in electronic communication. ASCII assigns standard
numeric values to letters, numeral, symbols, white-spaces and other characters.
In more technical terms, OCR is the process of using an electronic device to
transform 2-Dimensional textual information into machine-encoded text. Anything
that contains text both machine written or handwritten can be scanned either
through a scanner or just simply a picture of the text is enough for the
recognition system to distinguish the text. The goal of this papers is to show
the results of a Convolutional Neural Network model which has been trained on
National Institute of Science and Technology (NIST) dataset containing over a
100,000 images. The network learns from the features extracted from the images
and use it to generate the probability of each class to which the picture
belongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combating Data Imbalances in Federated Semi-supervised Learning with
  Dual Regulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sikai Bai, Shuaicheng Li, Weiming Zhuang, Kunlin Yang, Jun Hou, Shuai Yi, Shuai Zhang, Junyu Gao, Jie Zhang, Song Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has become a popular method to learn from decentralized
heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train
models from a small fraction of labeled data due to label scarcity on
decentralized clients. Existing FSSL methods assume independent and identically
distributed (IID) labeled data across clients and consistent class distribution
between labeled and unlabeled data within a client. This work studies a more
practical and challenging scenario of FSSL, where data distribution is
different not only across clients but also within a client between labeled and
unlabeled data. To address this challenge, we propose a novel FSSL framework
with dual regulators, FedDure.} FedDure lifts the previous assumption with a
coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg
regularizes the updating of the local model by tracking the learning effect on
labeled data distribution; F-reg learns an adaptive weighting scheme tailored
for unlabeled instances in each client. We further formulate the client model
training as bi-level optimization that adaptively optimizes the model in the
client with two regulators. Theoretically, we show the convergence guarantee of
the dual regulators. Empirically, we demonstrate that FedDure is superior to
the existing methods across a wide range of settings, notably by more than 11%
on CIFAR-10 and CINIC-10 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProgGP: From GuitarPro Tablature Neural Generation To Progressive Metal
  Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jackson Loth, Pedro Sarmento, CJ Carr, Zack Zukowski, Mathieu Barthet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in the field of symbolic music generation has shown value in
using a tokenization based on the GuitarPro format, a symbolic representation
supporting guitar expressive attributes, as an input and output representation.
We extend this work by fine-tuning a pre-trained Transformer model on ProgGP, a
custom dataset of 173 progressive metal songs, for the purposes of creating
compositions from that genre through a human-AI partnership. Our model is able
to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We
examine the validity of the generated music using a mixed methods approach by
combining quantitative analyses following a computational musicology paradigm
and qualitative analyses following a practice-based research paradigm. Finally,
we demonstrate the value of the model by using it as a tool to create a
progressive metal song, fully produced and mixed by a human metal producer
based on AI-generated music.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print accepted for publication at CMMR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Generation of Semantic Parts for Face Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic image synthesis (SIS) refers to the problem of generating realistic
imagery given a semantic segmentation mask that defines the spatial layout of
object classes. Most of the approaches in the literature, other than the
quality of the generated images, put effort in finding solutions to increase
the generation diversity in terms of style i.e. texture. However, they all
neglect a different feature, which is the possibility of manipulating the
layout provided by the mask. Currently, the only way to do so is manually by
means of graphical users interfaces. In this paper, we describe a network
architecture to address the problem of automatically manipulating or generating
the shape of object classes in semantic segmentation masks, with specific focus
on human faces. Our proposed model allows embedding the mask class-wise into a
latent space where each class embedding can be independently edited. Then, a
bi-directional LSTM block and a convolutional decoder output a new, locally
manipulated mask. We report quantitative and qualitative results on the
CelebMask-HQ dataset, which show our model can both faithfully reconstruct and
modify a segmentation mask at the class level. Also, we show our model can be
put before a SIS generator, opening the way to a fully automatic generation
control of both shape and texture. Code available at
https://github.com/TFonta/Semantic-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, accepted for publication at ICIAP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Vision and Language <span class="highlight-title">Pre-train</span>ing with Unimodal and Multimodal
  Contrastive Losses for Medical Visual Question Answering <span class="chip">MICCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Li, Gang Liu, Jinlong He, Zixu Zhao, Shenjun Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical visual question answering (VQA) is a challenging task that requires
answering clinical questions of a given medical image, by taking consider of
both visual and language information. However, due to the small scale of
training data for medical VQA, pre-training fine-tuning paradigms have been a
commonly used solution to improve model generalization performance. In this
paper, we present a novel self-supervised approach that learns unimodal and
multimodal feature representations of input images and text using medical image
caption datasets, by leveraging both unimodal and multimodal contrastive
losses, along with masked language modeling and image text matching as
pretraining objectives. The pre-trained model is then transferred to downstream
medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA)
performance on three publicly available medical VQA datasets with significant
accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we
conduct a comprehensive analysis to validate the effectiveness of different
components of the approach and study different pre-training settings. Our codes
and models are available at https://github.com/pengfeiliHEU/MUMC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by MICCAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Cognitive Synergy in Large Language Models: A Task-Solving
  Agent through Multi-Persona Self-Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence thrives on the concept of cognitive synergy, where
collaboration and information integration among different cognitive processes
yield superior outcomes compared to individual cognitive processes in
isolation. Although Large Language Models (LLMs) have demonstrated promising
performance as general task-solving agents, they still struggle with tasks that
require intensive domain knowledge and complex reasoning. In this work, we
propose Solo Performance Prompting (SPP), which transforms a single LLM into a
cognitive synergist by engaging in multi-turn self-collaboration with multiple
personas. A cognitive synergist refers to an intelligent agent that
collaborates with multiple minds, combining their individual strengths and
knowledge, to enhance problem-solving and overall performance in complex tasks.
By dynamically identifying and simulating different personas based on task
inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have
discovered that assigning multiple, fine-grained personas in LLMs elicits
better problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP
effectively elicits internal knowledge acquisition abilities, reduces
hallucination, and maintains strong reasoning capabilities. Code, data, and
prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Need for a Language Describing Distribution Shifts: Illustrations
  on Tabular <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Liu, Tianyu Wang, Peng Cui, Hongseok Namkoong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different distribution shifts require different algorithmic and operational
interventions. Methodological research must be grounded by the specific shifts
they address. Although nascent benchmarks provide a promising empirical
foundation, they implicitly focus on covariate shifts, and the validity of
empirical findings depends on the type of shift, e.g., previous observations on
algorithmic performance can fail to be valid when the $Y|X$ distribution
changes. We conduct a thorough investigation of natural shifts in 5 tabular
datasets over 86,000 model configurations, and find that $Y|X$-shifts are most
prevalent. To encourage researchers to develop a refined language for
distribution shifts, we build WhyShift, an empirical testbed of curated
real-world shifts where we characterize the type of shift we benchmark
performance over. Since $Y|X$-shifts are prevalent in tabular settings, we
identify covariate regions that suffer the biggest $Y|X$-shifts and discuss
implications for algorithmic and data-based interventions. Our testbed
highlights the importance of future research that builds an understanding of
how distributions differ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-CREAT: Unsupervised Case Retrieval using Events extrAcTion <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Joshi, Akshat Sharma, Sai Kiran Tanikella, Ashutosh Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Prior Case Retrieval (PCR) in the legal domain is about
automatically citing relevant (based on facts and precedence) prior legal cases
in a given query case. To further promote research in PCR, in this paper, we
propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian
Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance
and the long size of legal documents, BM25 remains a strong baseline for
ranking the cited prior documents. In this work, we explore the role of events
in legal case retrieval and propose an unsupervised retrieval method-based
pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find
that the proposed unsupervised retrieval method significantly increases
performance compared to BM25 and makes retrieval faster by a considerable
margin, making it applicable to real-time case retrieval systems. Our proposed
system is generic, we show that it generalizes across two different legal
systems (Indian and Canadian), and it shows state-of-the-art performance on the
benchmarks for both the legal systems (IL-PCR and COLIEE corpora).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023, 15 pages (12 main + 3 Appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrated Planning in Hospitals: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Rachuba, Melanie Reuter-Oppermann, Clemens Thielen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient planning of scarce resources in hospitals is a challenging task for
which a large variety of Operations Research and Management Science approaches
have been developed since the 1950s. While efficient planning of single
resources such as operating rooms, beds, or specific types of staff can already
lead to enormous efficiency gains, integrated planning of several resources has
been shown to hold even greater potential, and a large number of integrated
planning approaches have been presented in the literature over the past
decades.
  This paper provides the first literature review that focuses specifically on
the Operations Research and Management Science literature related to integrated
planning of different resources in hospitals. We collect the relevant
literature and analyze it regarding different aspects such as uncertainty
modeling and the use of real-life data. Several cross comparisons reveal
interesting insights concerning, e.g., relations between the modeling and
solution methods used and the practical implementation of the approaches
developed. Moreover, we provide a high-level taxonomy for classifying different
resource-focused integration approaches and point out gaps in the literature as
well as promising directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score Function Gradient Estimation to Widen the Applicability of
  Decision-Focused Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Silvestri, Senne Berden, Jayanta Mandi, Ali İrfan Mahmutoğulları, Maxime Mulamba, Allegra De Filippo, Tias Guns, Michele Lombardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world optimization problems contain unknown parameters that must be
predicted prior to solving. To train the predictive machine learning (ML)
models involved, the commonly adopted approach focuses on maximizing predictive
accuracy. However, this approach does not always lead to the minimization of
the downstream task loss. Decision-focused learning (DFL) is a recently
proposed paradigm whose goal is to train the ML model by directly minimizing
the task loss. However, state-of-the-art DFL methods are limited by the
assumptions they make about the structure of the optimization problem (e.g.,
that the problem is linear) and by the fact that can only predict parameters
that appear in the objective function. In this work, we address these
limitations by instead predicting \textit{distributions} over parameters and
adopting score function gradient estimation (SFGE) to compute decision-focused
updates to the predictive model, thereby widening the applicability of DFL. Our
experiments show that by using SFGE we can: (1) deal with predictions that
occur both in the objective function and in the constraints; and (2)
effectively tackle two-stage stochastic optimization problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Pre-Planning on Reward Machine Abstractions for Enhanced
  Transfer in Deep Reinforcement Learning <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Azran, Mohamad H. Danesh, Stefano V. Albrecht, Sarah Keren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that deep reinforcement learning (DRL) agents tend to
overfit to the task on which they were trained and fail to adapt to minor
environment changes. To expedite learning when transferring to unseen tasks, we
propose a novel approach to representing the current task using reward machines
(RM), state machine abstractions that induce subtasks based on the current
task's rewards and dynamics. Our method provides agents with symbolic
representations of optimal transitions from their current abstract state and
rewards them for achieving these transitions. These representations are shared
across tasks, allowing agents to exploit knowledge of previously encountered
symbols and transitions, thus enhancing transfer. Our empirical evaluation
shows that our representations improve sample efficiency and few-shot transfer
in a variety of domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI Workshop on Planning and Reinforcement Learning, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Statistical Inference through $β$-Divergence
  One Posterior Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Jewson, Sahra Ghalebikesabi, Chris Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy guarantees allow the results of a statistical analysis
involving sensitive data to be released without compromising the privacy of any
individual taking part. Achieving such guarantees generally requires the
injection of noise, either directly into parameter estimates or into the
estimation process. Instead of artificially introducing perturbations, sampling
from Bayesian posterior distributions has been shown to be a special case of
the exponential mechanism, producing consistent, and efficient private
estimates without altering the data generative process. The application of
current approaches has, however, been limited by their strong bounding
assumptions which do not hold for basic models, such as simple linear
regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling
scheme from a generalised posterior targeting the minimisation of the
$\beta$-divergence between the model and the data generating process. This
provides private estimation that is generally applicable without requiring
changes to the underlying model and consistently learns the data generating
parameter. We show that $\beta$D-Bayes produces more precise inference
estimation for the same privacy guarantees, and further facilitates
differentially private estimation via posterior sampling for complex
classifiers and continuous regression models such as neural networks for the
first time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Attention Gated Vision-Language Embedding for Visual Question
  Localized-Answering in Robotic Surgery <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Bai, Mobarakol Islam, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical students and junior surgeons often rely on senior surgeons and
specialists to answer their questions when learning surgery. However, experts
are often busy with clinical and academic work, and have little time to give
guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question
Answering (VQA) systems can only provide simple answers without the location of
the answers. In addition, vision-language (ViL) embedding is still a less
explored research in these kinds of tasks. Therefore, a surgical Visual
Question Localized-Answering (VQLA) system would be helpful for medical
students and junior surgeons to learn and understand from recorded surgical
videos. We propose an end-to-end Transformer with Co-Attention gaTed
Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not
require feature extraction through detection models. The CAT-ViL embedding
module is designed to fuse heterogeneous features from visual and textual
sources. The fused embedding will feed a standard Data-Efficient Image
Transformer (DeiT) module, before the parallel classifier and detector for
joint prediction. We conduct the experimental validation on public surgical
videos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results
highlight the superior performance and robustness of our proposed model
compared to the state-of-the-art approaches. Ablation studies further prove the
outstanding performance of all the proposed components. The proposed method
provides a promising solution for surgical scene understanding, and opens up a
primary step in the Artificial Intelligence (AI)-based VQLA system for surgical
training. Our code is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in MICCAI 2023. Code availability:
  https://github.com/longbai1006/CAT-ViL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enriching Verbal Feedback from Usability Testing: Automatic Linking of
  Thinking-Aloud Recordings and Stimulus using Eye Tracking and Mouse Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supriya Murali, Tina Walber, Christoph Schaefer, Sezen Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The think aloud method is an important and commonly used tool for usability
optimization. However, analyzing think aloud data could be time consuming. In
this paper, we put forth an automatic analysis of verbal protocols and test the
link between spoken feedback and the stimulus using eye tracking and mouse
tracking. The gained data - user feedback linked to a specific area of the
stimulus - could be used to let an expert review the feedback on specific web
page elements or to visualize on which parts of the web page the feedback was
given. Specifically, we test if participants fixate on or point with the mouse
to the content of the webpage that they are verbalizing. During the testing,
participants were shown three websites and asked to verbally give their
opinion. The verbal responses, along with the eye and cursor movements were
recorded. We compared the hit rate, defined as the percentage of verbally
mentioned areas of interest (AOIs) that were fixated with gaze or pointed to
with the mouse. The results revealed a significantly higher hit rate for the
gaze compared to the mouse data. Further investigation revealed that, while the
mouse was mostly used passively to scroll, the gaze was often directed towards
relevant AOIs, thus establishing a strong association between spoken words and
stimuli. Therefore, eye tracking data possibly provides more detailed
information and more valuable insights about the verbalizations compared to the
mouse data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Quantile Optimization for Edge-Cloud Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Du, He Zhang, Xiangle Cheng, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We seek the best traffic allocation scheme for the edge-cloud computing
network that satisfies constraints and minimizes the cost based on burstable
billing. First, for a fixed network topology, we formulate a family of integer
programming problems with random parameters describing the various traffic
demands. Then, to overcome the difficulty caused by the discrete feature of the
problem, we generalize the Gumbel-softmax reparameterization method to induce
an unconstrained continuous optimization problem as a regularized continuation
of the discrete problem. Finally, we introduce the Gumbel-softmax sampling
network to solve the optimization problems via unsupervised learning. The
network structure reflects the edge-cloud computing topology and is trained to
minimize the expectation of the cost function for unconstrained continuous
optimization problems. The trained network works as an efficient traffic
allocation scheme sampler, remarkably outperforming the random strategy in
feasibility and cost function value. Besides testing the quality of the output
allocation scheme, we examine the generalization property of the network by
increasing the time steps and the number of users. We also feed the solution to
existing integer optimization solvers as initial conditions and verify the
warm-starts can accelerate the short-time iteration process. The framework is
general with solid performance, and the decoupled feature of the random neural
networks is adequate for practical implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Suri, Prakhar Mishra, Saumajit Saha, Atul Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finetuning Large Language Models helps improve the results for
domain-specific use cases. End-to-end finetuning of large language models is
time and resource intensive and has high storage requirements to store the
finetuned version of the large language model. Parameter Efficient Fine Tuning
(PEFT) methods address the time and resource challenges by keeping the large
language model as a fixed base and add additional layers, which the PEFT
methods finetune. This paper demonstrates the evaluation results for one such
PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.
The evaluation results show that LoRA works at par with end-to-end finetuning
for a large language model. The paper presents the evaluations done for solving
both the Subtask A and B from ImageCLEFmedical
{https://www.imageclef.org/2023/medical}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Effectiveness of Speech <span class="highlight-title">Self-supervised</span> Learning for Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Ma, Ruibin Yuan, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Ruibo Liu, Gus Xia, Roger Dannenberg, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has shown promising results in various speech
and natural language processing applications. However, its efficacy in music
information retrieval (MIR) still remains largely unexplored. While previous
SSL models pre-trained on music recordings may have been mostly closed-sourced,
recent speech models such as wav2vec2.0 have shown promise in music modelling.
Nevertheless, research exploring the effectiveness of applying speech SSL
models to music recordings has been limited. We explore the music adaption of
SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and
refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL
models with 95M parameters under various pre-training configurations and
systematically evaluate the MIR task performances with 13 different MIR tasks.
Our findings suggest that training with music data can generally improve
performance on MIR tasks, even when models are trained using paradigms designed
for speech. However, we identify the limitations of such existing
speech-oriented designs, especially in modelling polyphonic information. Based
on the experimental results, empirical suggestions are also given for designing
future musical SSL strategies and paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Normative Explanations: From Argumentation to Deontic Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cecilia Di Florio, Guido Governatori, Antonino Rotolo, Giovanni Sartor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines how a notion of stable explanation developed elsewhere in
Defeasible Logic can be expressed in the context of formal argumentation. With
this done, we discuss the deontic meaning of this reconstruction and show how
to build from argumentation neighborhood structures for deontic logic where
this notion of explanation can be characterised. Some direct complexity results
are offered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, extended version of the short paper accepted at JELIA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Modal Logic for Explaining some Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Nunn, François Schwarzentruber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a modal logic in which counting modalities appear
in linear inequalities. We show that each formula can be transformed into an
equivalent graph neural network (GNN). We also show that each GNN can be
transformed into a formula. We show that the satisfiability problem is
decidable. We also discuss some variants that are in PSPACE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the latent noise used as a seed for the images. We also quantify
the influence of the number of concepts in the prompt, their order as well as
their (color) attributes. Finally, our method allows us to identify some latent
seeds that produce better images than others, opening novel directions of
research on this understudied topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Dive into Perturbations as Evaluation Technique for Time Series
  XAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Udo Schlegel, Daniel A. Keim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) has gained significant attention
recently as the demand for transparency and interpretability of machine
learning models has increased. In particular, XAI for time series data has
become increasingly important in finance, healthcare, and climate science.
However, evaluating the quality of explanations, such as attributions provided
by XAI techniques, remains challenging. This paper provides an in-depth
analysis of using perturbations to evaluate attributions extracted from time
series models. A perturbation analysis involves systematically modifying the
input data and evaluating the impact on the attributions generated by the XAI
method. We apply this approach to several state-of-the-art XAI techniques and
evaluate their performance on three time series classification datasets. Our
results demonstrate that the perturbation analysis approach can effectively
evaluate the quality of attributions and provide insights into the strengths
and limitations of XAI techniques. Such an approach can guide the selection of
XAI methods for time series data, e.g., focusing on return time rather than
precision, and facilitate the development of more reliable and interpretable
machine learning models for time series analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 pages references, 5 figures, 3 tables, submitted and
  accepted at xAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATWM: Defense against adversarial malware based on adversarial training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Li, Fan Zhang, Wei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning technology has made great achievements in the field of image.
In order to defend against malware attacks, researchers have proposed many
Windows malware detection models based on deep learning. However, deep learning
models are vulnerable to adversarial example attacks. Malware can generate
adversarial malware with the same malicious function to attack the malware
detection model and evade detection of the model. Currently, many adversarial
defense studies have been proposed, but existing adversarial defense studies
are based on image sample and cannot be directly applied to malware sample.
Therefore, this paper proposes an adversarial malware defense method based on
adversarial training. This method uses preprocessing to defend simple
adversarial examples to reduce the difficulty of adversarial training.
Moreover, this method improves the adversarial defense capability of the model
through adversarial training. We experimented with three attack methods in two
sets of datasets, and the results show that the method in this paper can
improve the adversarial defense capability of the model without reducing the
accuracy of the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OntoChat<span class="highlight-title">GPT</span> Information System: Ontology-Driven Structured <span class="highlight-title">Prompt</span>s for
  Chat<span class="highlight-title">GPT</span> Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Palagin, Vladislav Kaverinskiy, Anna Litvin, Kyrylo Malakhov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a comprehensive methodology for utilizing an
ontology-driven structured prompts system in interplay with ChatGPT, a widely
used large language model (LLM). The study develops formal models, both
information and functional, and establishes the methodological foundations for
integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.
The resulting productive triad comprises the methodological foundations,
advanced information technology, and the OntoChatGPT system, which collectively
enhance the effectiveness and performance of chatbot systems. The
implementation of this technology is demonstrated using the Ukrainian language
within the domain of rehabilitation. By applying the proposed methodology, the
OntoChatGPT system effectively extracts entities from contexts, classifies
them, and generates relevant responses. The study highlights the versatility of
the methodology, emphasizing its applicability not only to ChatGPT but also to
other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2
LLM. The underlying principles of meta-learning, structured prompts, and
ontology-driven information retrieval form the core of the proposed
methodology, enabling their adaptation and utilization in various LLM-based
systems. This versatile approach opens up new possibilities for NLP and
dialogue systems, empowering developers to enhance the performance and
functionality of chatbot systems across different domains and languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure. Published. International Journal of Computing,
  22(2), 170-183. https://doi.org/10.47839/ijc.22.2.3086</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing
  Multiple Degradations in Real-World Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongheng Zhang, Danfeng Yan, Yuanqiang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Removing multiple degradations, such as haze, rain, and blur, from real-world
images poses a challenging and illposed problem. Recently, unified models that
can handle different degradations have been proposed and yield promising
results. However, these approaches focus on synthetic images and experience a
significant performance drop when applied to realworld images. In this paper,
we introduce Uni-Removal, a twostage semi-supervised framework for addressing
the removal of multiple degradations in real-world images using a unified model
and parameters. In the knowledge transfer stage, Uni-Removal leverages a
supervised multi-teacher and student architecture in the knowledge transfer
stage to facilitate learning from pretrained teacher networks specialized in
different degradation types. A multi-grained contrastive loss is introduced to
enhance learning from feature and image spaces. In the domain adaptation stage,
unsupervised fine-tuning is performed by incorporating an adversarial
discriminator on real-world images. The integration of an extended
multi-grained contrastive loss and generative adversarial loss enables the
adaptation of the student network from synthetic to real-world domains.
Extensive experiments on real-world degraded datasets demonstrate the
effectiveness of our proposed method. We compare our Uni-Removal framework with
state-of-the-art supervised and unsupervised methods, showcasing its promising
results in real-world image dehazing, deraining, and deblurring simultaneously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-augmented <span class="highlight-title">GPT</span>-3.5-based Text-to-SQL Framework with
  Sample-aware <span class="highlight-title">Prompt</span>ing and Dynamic Revision Chain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li, Zhihua Wen, Kaixuan Wang, Ting Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL aims at generating SQL queries for the given natural language
questions and thus helping users to query databases. Prompt learning with large
language models (LLMs) has emerged as a recent approach, which designs prompts
to lead LLMs to understand the input question and generate the corresponding
SQL. However, it faces challenges with strict SQL syntax requirements. Existing
work prompts the LLMs with a list of demonstration examples (i.e. question-SQL
pairs) to generate SQL, but the fixed prompts can hardly handle the scenario
where the semantic gap between the retrieved demonstration and the input
question is large. In this paper, we propose a retrieval-augmented prompting
method for a LLM-based Text-to-SQL framework, involving sample-aware prompting
and a dynamic revision chain. Our approach incorporates sample-aware
demonstrations, which include the composition of SQL operators and fine-grained
information related to the given question. To retrieve questions sharing
similar intents with input questions, we propose two strategies for assisting
retrieval. Firstly, we leverage LLMs to simplify the original questions,
unifying the syntax and thereby clarifying the users' intentions. To generate
executable and accurate SQLs without human intervention, we design a dynamic
revision chain which iteratively adapts fine-grained feedback from the
previously generated SQL. Experimental results on three Text-to-SQL benchmarks
demonstrate the superiority of our method over strong baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aggregating Credences into Beliefs: Agenda Conditions for Impossibility
  Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyung Wang, Chisu Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binarizing belief aggregation addresses how to rationally aggregate
individual probabilistic beliefs into collective binary beliefs. Similar to the
development of judgment aggregation theory, formulating axiomatic requirements,
proving impossibility theorems, and identifying exact agenda conditions of
impossibility theorems are natural and important research topics in binarizing
belief aggregation. Building on our previous research on impossibility
theorems, we use an agenda-theoretic approach to generalize the results and to
determine the necessary and sufficient level of logical interconnection between
the issues in an agenda for the impossibility theorems to arise. We demonstrate
that (1) path-connectedness and even-negatability constitute the exact agenda
condition for the oligarchy result stating that binarizing belief aggregation
satisfying proposition-wise independence and deductive closure of collective
beliefs yields the oligarchies under minor conditions; (2)
negation-connectedness is the condition for the triviality result obtained by
adding anonymity to the oligarchy result; and (3) blockedness is the condition
for the impossibility result, which follows by adding completeness and
consistency of collective beliefs. Moreover, we compare these novel findings
with existing agenda-theoretic characterization theorems in judgment
aggregation and belief binarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining for Unknown Unknowns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernard Sinclair-Desgagné
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unknown unknowns are future relevant contingencies that lack an ex ante
description. While there are numerous retrospective accounts showing that
significant gains or losses might have been achieved or avoided had such
contingencies been previously uncovered, getting hold of unknown unknowns still
remains elusive, both in practice and conceptually. Using Formal Concept
Analysis (FCA) - a subfield of lattice theory which is increasingly applied for
mining and organizing data - this paper introduces a simple framework to
systematically think out of the box and direct the search for unknown unknowns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cognitive Bias and Belief Revision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Papadamos, Nina Gierasimczuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we formalise three types of cognitive bias within the framework
of belief revision: confirmation bias, framing bias, and anchoring bias. We
interpret them generally, as restrictions on the process of iterated revision,
and we apply them to three well-known belief revision methods: conditioning,
lexicographic revision, and minimal revision. We investigate the reliability of
biased belief revision methods in truth tracking. We also run computer
simulations to assess the performance of biased belief revision in random
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theory of Bounded Inductive Rationality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caspar Oesterheld, Abram Demski, Vincent Conitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dominant theories of rational choice assume logical omniscience. That is,
they assume that when facing a decision problem, an agent can perform all
relevant computations and determine the truth value of all relevant
logical/mathematical claims. This assumption is unrealistic when, for example,
we offer bets on remote digits of pi or when an agent faces a computationally
intractable planning problem. Furthermore, the assumption of logical
omniscience creates contradictions in cases where the environment can contain
descriptions of the agent itself. Importantly, strategic interactions as
studied in game theory are decision problems in which a rational agent is
predicted by its environment (the other players). In this paper, we develop a
theory of rational decision making that does not assume logical omniscience. We
consider agents who repeatedly face decision problems (including ones like
betting on digits of pi or games against other agents). The main contribution
of this paper is to provide a sensible theory of rationality for such agents.
Roughly, we require that a boundedly rational inductive agent tests each
efficiently computable hypothesis infinitely often and follows those hypotheses
that keep their promises of high rewards. We then prove that agents that are
rational in this sense have other desirable properties. For example, they learn
to value random and pseudo-random lotteries at their expected reward. Finally,
we consider strategic interactions between different agents and prove a folk
theorem for what strategies bounded rational inductive agents can converge to.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Asymmetry in Logic Puzzles: Using ZDDs for Symbolic Model
  Checking Dynamic Epistemic Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Miedema, Malvin Gattinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary decision diagrams (BDDs) are widely used to mitigate the
state-explosion problem in model checking. A variation of BDDs are
Zero-suppressed Decision Diagrams (ZDDs) which omit variables that must be
false, instead of omitting variables that do not matter. We use ZDDs to
symbolically encode Kripke models used in Dynamic Epistemic Logic, a framework
to reason about knowledge and information dynamics in multi-agent systems. We
compare the memory usage of different ZDD variants for three well-known
examples from the literature: the Muddy Children, the Sum and Product puzzle
and the Dining Cryptographers. Our implementation is based on the existing
model checker SMCDEL and the CUDD library. Our results show that replacing BDDs
with the right variant of ZDDs can significantly reduce memory usage. This
suggests that ZDDs are a useful tool for model checking multi-agent systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tableaux for the Logic of Strategically Knowing How 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The logic of goal-directed knowing-how extends the standard epistemic logic
with an operator of knowing-how. The knowing-how operator is interpreted as
that there exists a strategy such that the agent knows that the strategy can
make sure that p. This paper presents a tableau procedure for the multi-agent
version of the logic of strategically knowing-how and shows the soundness and
completeness of this tableau procedure. This paper also shows that the
satisfiability problem of the logic can be decided in PSPACE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ System of Spheres-based Two Level Credibility-limited Revisions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Garapa, Eduardo Ferme, Maurício D. L. Reis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two level credibility-limited revision is a non-prioritized revision
operation. When revising by a two level credibility-limited revision, two
levels of credibility and one level of incredibility are considered. When
revising by a sentence at the highest level of credibility, the operator
behaves as a standard revision, if the sentence is at the second level of
credibility, then the outcome of the revision process coincides with a standard
contraction by the negation of that sentence. If the sentence is not credible,
then the original belief set remains unchanged. In this paper, we propose a
construction for two level credibility-limited revision operators based on
Grove's systems of spheres and present an axiomatic characterization for these
operators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Imperfect Recall in Multi-Agent Influence Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Fox, Matt MacDermott, Lewis Hammond, Paul Harrenstein, Alessandro Abate, Michael Wooldridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent influence diagrams (MAIDs) are a popular game-theoretic model
based on Bayesian networks. In some settings, MAIDs offer significant
advantages over extensive-form game representations. Previous work on MAIDs has
assumed that agents employ behavioural policies, which set independent
conditional probability distributions over actions for each of their decisions.
In settings with imperfect recall, however, a Nash equilibrium in behavioural
policies may not exist. We overcome this by showing how to solve MAIDs with
forgetful and absent-minded agents using mixed policies and two types of
correlated equilibrium. We also analyse the computational complexity of key
decision problems in MAIDs, and explore tractable cases. Finally, we describe
applications of MAIDs to Markov games and team situations, where imperfect
recall is often unavoidable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strengthening Consistency Results in Modal Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Allen Alexander, Arthur Paul Pedersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question asked in modal logic is whether a given theory is
consistent. But consistent with what? A typical way to address this question
identifies a choice of background knowledge axioms (say, S4, D, etc.) and then
shows the assumptions codified by the theory in question to be consistent with
those background axioms. But determining the specific choice and division of
background axioms is, at least sometimes, little more than tradition. This
paper introduces **generic theories** for propositional modal logic to address
consistency results in a more robust way. As building blocks for background
knowledge, generic theories provide a standard for categorical determinations
of consistency. We argue that the results and methods of this paper help to
elucidate problems in epistemology and enjoy sufficient scope and power to have
purchase on problems bearing on modalities in judgement, inference, and
decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005. The authors thank three
  anonymous reviewers as well as Rineke Verbrugge for valuable comments and
  suggestions to help improve this manuscript. The authors also extend their
  gratitude to Alessandro Aldini, Michael Grossberg, Ali Kahn, Rohit Parikh,
  and Max Stinchcombe, for their generous feedback on prior drafts of this
  manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding In-Context Learning with Contrastive
  Demonstrations and Saliency Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxia Li, Paiheng Xu, Fuxiao Liu, Hyemi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the role of various demonstration components in the in-context
learning (ICL) performance of large language models (LLMs). Specifically, we
explore the impacts of ground-truth labels, input distribution, and
complementary explanations, particularly when these are altered or perturbed.
We build on previous work, which offers mixed findings on how these elements
influence ICL. To probe these questions, we employ explainable NLP (XNLP)
methods and utilize saliency maps of contrastive demonstrations for both
qualitative and quantitative analysis. Our findings reveal that flipping
ground-truth labels significantly affects the saliency, though it's more
noticeable in larger LLMs. Our analysis of the input distribution at a granular
level reveals that changing sentiment-indicative terms in a sentiment analysis
task to neutral ones does not have as substantial an impact as altering
ground-truth labels. Finally, we find that the effectiveness of complementary
explanations in boosting ICL performance is task-dependent, with limited
benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.
These insights are critical for understanding the functionality of LLMs and
guiding the development of effective demonstrations, which is increasingly
relevant in light of the growing use of LLMs in applications such as ChatGPT.
Our research code is publicly available at https://github.com/paihengxu/XICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Epistemic Syllogistic: First Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yipu Li, Yanjing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aristotle's discussions on modal syllogistic have often been viewed as
error-prone and have garnered significant attention in the literature due to
historical and philosophical interests. However, from a contemporary
standpoint, they also introduced natural fragments of first-order modal logic,
warranting a comprehensive technical analysis. In this paper, drawing
inspiration from the natural logic program, we propose and examine several
variants of modal syllogistic within the epistemic context, thereby coining the
term Epistemic Syllogistic. Specifically, we concentrate on the de re
interpretation of epistemic syllogisms containing non-trivial yet natural
expressions such as "all things known to be A are also known to be not B." We
explore the epistemic apodeictic syllogistic and its extensions, which
accommodate more complex terms. Our main contributions include several
axiomatizations of these logics, with completeness proofs that may be of
independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural-Symbolic Recommendation with Graph-Enhanced Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Chen, Wei Peng, Maonian Wu, Bo Zheng, Shaojun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recommendation system is not only a problem of inductive statistics from
data but also a cognitive task that requires reasoning ability. The most
advanced graph neural networks have been widely used in recommendation systems
because they can capture implicit structured information from graph-structured
data. However, like most neural network algorithms, they only learn matching
patterns from a perception perspective. Some researchers use user behavior for
logic reasoning to achieve recommendation prediction from the perspective of
cognitive reasoning, but this kind of reasoning is a local one and ignores
implicit information on a global scale. In this work, we combine the advantages
of graph neural networks and propositional logic operations to construct a
neuro-symbolic recommendation model with both global implicit reasoning ability
and local explicit logic reasoning ability. We first build an item-item graph
based on the principle of adjacent interaction and use graph neural networks to
capture implicit information in global data. Then we transform user behavior
into propositional logic expressions to achieve recommendations from the
perspective of cognitive reasoning. Extensive experiments on five public
datasets show that our proposed model outperforms several state-of-the-art
methods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Potential of Regularization Strategies in Learning with
  Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Kang, Sheng Liu, Huaxi Huang, Jun Yu, Bo Han, Dadong Wang, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, research on learning with noisy labels has focused on
devising novel algorithms that can achieve robustness to noisy training labels
while generalizing to clean data. These algorithms often incorporate
sophisticated techniques, such as noise modeling, label correction, and
co-training. In this study, we demonstrate that a simple baseline using
cross-entropy loss, combined with widely used regularization strategies like
learning rate decay, model weights average, and data augmentations, can
outperform state-of-the-art methods. Our findings suggest that employing a
combination of regularization strategies can be more effective than intricate
algorithms in tackling the challenges of learning with noisy labels. While some
of these regularization strategies have been utilized in previous noisy label
learning research, their full potential has not been thoroughly explored. Our
results encourage a reevaluation of benchmarks for learning with noisy labels
and prompt reconsideration of the role of specialized learning algorithms
designed for training with noisy labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Activation Map: Visual Explanation of Deep Learning Models for
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Liao, Yongsheng Gao, Weichuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decisions made by convolutional neural networks(CNN) can be understood and
explained by visualizing discriminative regions on images. To this end, Class
Activation Map (CAM) based methods were proposed as powerful interpretation
tools, making the prediction of deep learning models more explainable,
transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM,
Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with
fully-connected (FC) layers as a classifier. It is worth noting that many deep
learning models classify images without FC layers, e.g., few-shot learning
image classification, contrastive learning image classification, and image
retrieval tasks. In this work, a post-hoc interpretation tool named feature
activation map (FAM) is proposed, which can interpret deep learning models
without FC layers as a classifier. In the proposed FAM algorithm, the
channel-wise contribution weights are derived from the similarity scores
between two image embeddings. The activation maps are linearly combined with
the corresponding normalized contribution weights, forming the explanation map
for visualization. The quantitative and qualitative experiments conducted on
ten deep learning models for few-shot image classification, contrastive
learning image classification and image retrieval tasks demonstrate the
effectiveness of the proposed FAM algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control as Probabilistic Inference as an Emergent Communication
  Mechanism in Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoaki Nakamura, Akira Taniguchi, Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a generative probabilistic model integrating emergent
communication and multi-agent reinforcement learning. The agents plan their
actions by probabilistic inference, called control as inference, and
communicate using messages that are latent variables and estimated based on the
planned actions. Through these messages, each agent can send information about
its actions and know information about the actions of another agent. Therefore,
the agents change their actions according to the estimated messages to achieve
cooperative tasks. This inference of messages can be considered as
communication, and this procedure can be formulated by the Metropolis-Hasting
naming game. Through experiments in the grid world environment, we show that
the proposed PGM can infer meaningful messages to achieve the cooperative task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Sampling and Imitation Learning via Online Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of Imitation Learning (IL) by actively querying noisy
expert for feedback. While imitation learning has been empirically successful,
much of prior work assumes access to noiseless expert feedback which is not
practical in many applications. In fact, when one only has access to noisy
expert feedback, algorithms that rely on purely offline data (non-interactive
IL) can be shown to need a prohibitively large number of samples to be
successful. In contrast, in this work, we provide an interactive algorithm for
IL that uses selective sampling to actively query the noisy expert for
feedback. Our contributions are twofold: First, we provide a new selective
sampling algorithm that works with general function classes and multiple
actions, and obtains the best-known bounds for the regret and the number of
queries. Next, we extend this analysis to the problem of IL with noisy expert
feedback and provide a new IL algorithm that makes limited queries.
  Our algorithm for selective sampling leverages function approximation, and
relies on an online regression oracle w.r.t.~the given model class to predict
actions, and to decide whether to query the expert for its label. On the
theoretical side, the regret bound of our algorithm is upper bounded by the
regret of the online regression oracle, while the query complexity additionally
depends on the eluder dimension of the model class. We complement this with a
lower bound that demonstrates that our results are tight. We extend our
selective sampling algorithm for IL with general function approximation and
provide bounds on both the regret and the number of queries made to the noisy
expert. A key novelty here is that our regret and query complexity bounds only
depend on the number of times the optimal policy (and not the noisy expert, or
the learner) go to states that have a small margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering recommender systems using automatically generated Knowledge
  Graphs and Reinforcement Learning <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghanshyam Verma, Shovon Sengupta, Simon Simanta, Huan Chen, Janos A. Perge, Devishree Pillai, John P. McCrae, Paul Buitelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized recommendations have a growing importance in direct marketing,
which motivates research to enhance customer experiences by knowledge graph
(KG) applications. For example, in financial services, companies may benefit
from providing relevant financial articles to their customers to cultivate
relationships, foster client engagement and promote informed financial
decisions. While several approaches center on KG-based recommender systems for
improved content, in this study we focus on interpretable KG-based recommender
systems for decision making.To this end, we present two knowledge graph-based
approaches for personalized article recommendations for a set of customers of a
large multinational financial services company. The first approach employs
Reinforcement Learning and the second approach uses the XGBoost algorithm for
recommending articles to the customers. Both approaches make use of a KG
generated from both structured (tabular data) and unstructured data (a large
body of text data).Using the Reinforcement Learning-based recommender system we
could leverage the graph traversal path leading to the recommendation as a way
to generate interpretations (Path Directed Reasoning (PDR)). In the
XGBoost-based approach, one can also provide explainable results using post-hoc
methods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I
am Five).Importantly, our approach offers explainable results, promoting better
decision-making. This study underscores the potential of combining advanced
machine learning techniques with KG-driven insights to bolster experience in
customer relationship management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD (OARS) 2023 [https://oars-workshop.github.io/]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monotone deep Boltzmann machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Feng, Ezra Winston, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods
ever studied, are multi-layered probabilistic models governed by a pairwise
energy function that describes the likelihood of all variables/nodes in the
network. In practice, DBMs are often constrained, i.e., via the
\emph{restricted} Boltzmann machine (RBM) architecture (which does not permit
intra-layer connections), in order to allow for more efficient inference. In
this work, we revisit the generic DBM approach, and ask the question: are there
other possible restrictions to their design that would enable efficient
(approximate) inference? In particular, we develop a new class of restricted
model, the monotone DBM, which allows for arbitrary self-connection in each
layer, but restricts the \emph{weights} in a manner that guarantees the
existence and global uniqueness of a mean-field fixed point. To do this, we
leverage tools from the recently-proposed monotone Deep Equilibrium model and
show that a particular choice of activation results in a fixed-point iteration
that gives a variational mean-field solution. While this approach is still
largely conceptual, it is the first architecture that allows for efficient
approximate inference in fully-general weight structures for DBMs. We apply
this approach to simple deep convolutional Boltzmann architectures and
demonstrate that it allows for tasks such as the joint completion and
classification of images, within a single deep probabilistic setting, while
avoiding the pitfalls of mean-field inference in traditional RBMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Epidemic Modeling with Generative Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Williams, Niyousha Hosseinichimeh, Aritra Majumdar, Navid Ghaffarzadegan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study offers a new paradigm of individual-level modeling to address the
grand challenge of incorporating human behavior in epidemic models. Using
generative artificial intelligence in an agent-based epidemic model, each agent
is empowered to make its own reasonings and decisions via connecting to a large
language model such as ChatGPT. Through various simulation experiments, we
present compelling evidence that generative agents mimic real-world behaviors
such as quarantining when sick and self-isolation when cases rise.
Collectively, the agents demonstrate patterns akin to multiple waves observed
in recent pandemics followed by an endemic period. Moreover, the agents
successfully flatten the epidemic curve. This study creates potential to
improve dynamic system modeling by offering a way to represent human brain,
reasoning, and decision making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secrets of RLHF in Large Language Models Part I: PPO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Limao Xiong, Lu Chen, Zhiheng Xi, Yuhao Zhou, Nuo Xu, Wenbin Lai, Minghao Zhu, Rongxiang Weng, Wensen Cheng, Cheng Chang, Zhangyue Yin, Yuan Hua, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsically motivated graph exploration using network theories of
  human curiosity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar P. Patankar, Mathieu Ouellet, Juan Cervino, Alejandro Ribeiro, Kieran A. Murphy, Dani S. Bassett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by the visited nodes in the environment. We use these
proposed features as rewards for graph neural-network-based reinforcement
learning. On multiple classes of synthetically generated graphs, we find that
trained agents generalize to larger environments and to longer exploratory
walks than are seen during training. Our method computes more efficiently than
the greedy evaluation of the relevant topological properties. The proposed
intrinsic motivations bear particular relevance for recommender systems. We
demonstrate that curiosity-based recommendations are more predictive of human
behavior than PageRank centrality for several real-world graph datasets,
including MovieLens, Amazon Books, and Wikispeedia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures in main text, and 15 pages, 8 figures in
  supplement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Non-Cumulative Objective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Cui, Wei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, the objective is almost always defined as a
\emph{cumulative} function over the rewards along the process. However, there
are many optimal control and reinforcement learning problems in various
application fields, especially in communications and networking, where the
objectives are not naturally expressed as summations of the rewards. In this
paper, we recognize the prevalence of non-cumulative objectives in various
problems, and propose a modification to existing algorithms for optimizing such
objectives. Specifically, we dive into the fundamental building block for many
optimal control and reinforcement learning algorithms: the Bellman optimality
equation. To optimize a non-cumulative objective, we replace the original
summation operation in the Bellman update rule with a generalized operation
corresponding to the objective. Furthermore, we provide sufficient conditions
on the form of the generalized operation as well as assumptions on the Markov
decision process under which the globally optimal convergence of the
generalized Bellman updates can be guaranteed. We demonstrate the idea
experimentally with the bottleneck objective, i.e., the objectives determined
by the minimum reward along the process, on classical optimal control and
reinforcement learning tasks, as well as on two network routing problems on
maximizing the flow rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures. To appear in IEEE Transactions on Machine
  Learning in Communications and Networking (TMLCN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influential Simplices Mining via Simplicial Convolutional Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Zeng, Yiming Huang, Qiang Wu, Linyuan Lü
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simplicial complexes have recently been in the limelight of higher-order
network analysis, where a minority of simplices play crucial roles in
structures and functions due to network heterogeneity. We find a significant
inconsistency between identifying influential nodes and simplices. Therefore,
it remains elusive how to characterize simplices' influence and identify
influential simplices, despite the relative maturity of research on influential
nodes (0-simplices) identification. Meanwhile, graph neural networks (GNNs) are
potent tools that can exploit network topology and node features
simultaneously, but they struggle to tackle higher-order tasks. In this paper,
we propose a higher-order graph learning model, named influential simplices
mining neural network (ISMnet), to identify vital h-simplices in simplicial
complexes. It can tackle higher-order tasks by leveraging novel higher-order
presentations: hierarchical bipartite graphs and higher-order hierarchical
(HoH) Laplacians, where targeted simplices are grouped into a hub set and can
interact with other simplices. Furthermore, ISMnet employs learnable graph
convolutional operators in each HoH Laplacian domain to capture interactions
among simplices, and it can identify influential simplices of arbitrary order
by changing the hub set. Empirical results demonstrate that ISMnet
significantly outperforms existing methods in ranking 0-simplices (nodes) and
2-simplices. In general, this novel framework excels in identifying influential
simplices and promises to serve as a potent tool in higher-order network
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Distributed Multi-task Reinforcement Learning with Experience
  Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanae Amani, Khushbu Pahwa, Vladimir Braverman, Lin F. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, DARPA launched the ShELL program, which aims to explore how
experience sharing can benefit distributed lifelong learning agents in adapting
to new challenges. In this paper, we address this issue by conducting both
theoretical and empirical research on distributed multi-task reinforcement
learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks
without prior knowledge of their identities. We approach the problem by
formulating it as linearly parameterized contextual Markov decision processes
(MDPs), where each task is represented by a context that specifies the
transition dynamics and rewards. To tackle this problem, we propose an
algorithm called DistMT-LSVI. First, the agents identify the tasks, and then
they exchange information through a central server to derive $\epsilon$-optimal
policies for the tasks. Our research demonstrates that to achieve
$\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI
needs to run a total number of episodes that is at most
$\tilde{\mathcal{O}}({d^3H^6(\epsilon^{-2}+c_{\rm sep}^{-2})}\cdot M/N)$, where
$c_{\rm sep}>0$ is a constant representing task separability, $H$ is the
horizon of each episode, and $d$ is the feature dimension of the dynamics and
rewards. Notably, DistMT-LSVI improves the sample complexity of non-distributed
settings by a factor of $1/N$, as each agent independently learns
$\epsilon$-optimal policies for all $M$ tasks using
$\tilde{\mathcal{O}}(d^3H^6M\epsilon^{-2})$ episodes. Additionally, we provide
numerical experiments conducted on OpenAI Gym Atari environments that validate
our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bag of Views: An Appearance-based Approach to Next-Best-View Planning
  for 3D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Hatami Gazani, Matthew Tucsok, Iraj Mantegh, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  UAV-based intelligent data acquisition for 3D reconstruction and monitoring
of infrastructure has been experiencing an increasing surge of interest due to
the recent advancements in image processing and deep learning-based techniques.
View planning is an essential part of this task that dictates the information
capture strategy and heavily impacts the quality of the 3D model generated from
the captured data. Recent methods have used prior knowledge or partial
reconstruction of the target to accomplish view planning for active
reconstruction; the former approach poses a challenge for complex or newly
identified targets while the latter is computationally expensive. In this work,
we present Bag-of-Views (BoV), a fully appearance-based model used to assign
utility to the captured views for both offline dataset refinement and online
next-best-view (NBV) planning applications targeting the task of 3D
reconstruction. With this contribution, we also developed the View Planning
Toolbox (VPT), a lightweight package for training and testing machine
learning-based view planning frameworks, custom view dataset generation of
arbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a
BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of
our model in reducing the number of required views for high-quality
reconstructions in dataset refinement and NBV planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Robotics and Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memorization Through the Lens of Curvature of Loss Function Around
  Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Garg, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are overparametrized and easily overfit the datasets they
train on. In the extreme case, it is shown that they can memorize a training
set with fully randomized labels. We propose using the curvature of loss
function around the training sample as a measure of its memorization, averaged
over all training epochs. We use this to study the generalization versus
memorization properties of different samples in popular image datasets. We
visualize samples with the highest curvature of loss around them, and show that
these visually correspond to long-tailed, mislabeled or conflicting samples.
This analysis helps us find a, to the best of our knowledge, novel failure
model on the CIFAR100 dataset, that of duplicated images with different labels.
We also synthetically mislabel a proportion of the dataset by randomly
corrupting the labels of a few samples, and show that sorting by curvature
yields high AUROC values for identifying the mislabeled samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Extraction on Wikipedia Tables using Convolutional and Memory
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arif Shahriar, Rohan Saha, Denilson Barbosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction (RE) is the task of extracting relations between entities
in text. Most RE methods extract relations from free-form running text and
leave out other rich data sources, such as tables. We explore RE from the
perspective of applying neural methods on tabularly organized data. We
introduce a new model consisting of Convolutional Neural Network (CNN) and
Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and
learn dependencies among them, respectively. We evaluate our model on a large
and recent dataset and compare results with previous neural methods.
Experimental results show that our model consistently outperforms the previous
model for the task of relation extraction on tabular data. We perform
comprehensive error analyses and ablation study to show the contribution of
various components of our model. Finally, we discuss the usefulness and
trade-offs of our approach, and provide suggestions for fostering further
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Inspired Efficient Map Building via Fragmentation and Recall 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaedong Hwang, Zhang-Wei Hong, Eric Chen, Akhilan Boopathy, Pulkit Agrawal, Ila Fiete
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animals and robots navigate through environments by building and refining
maps of the space. These maps enable functions including navigating back to
home, planning, search, and foraging. In large environments, exploration of the
space is a hard problem: agents can become stuck in local regions. Here, we use
insights from neuroscience to propose and apply the concept of
Fragmentation-and-Recall (FarMap), with agents solving the mapping problem by
building local maps via a surprisal-based clustering of space, which they use
to set subgoals for spatial exploration. Agents build and use a local map to
predict their observations; high surprisal leads to a ``fragmentation event''
that truncates the local map. At these events, the recent local map is placed
into long-term memory (LTM), and a different local map is initialized. If
observations at a fracture point match observations in one of the stored local
maps, that map is recalled (and thus reused) from LTM. The fragmentation points
induce a natural online clustering of the larger space, forming a set of
intrinsic potential subgoals that are stored in LTM as a topological graph.
Agents choose their next subgoal from the set of near and far potential
subgoals from within the current local map or LTM, respectively. Thus, local
maps guide exploration locally, while LTM promotes global exploration. We
evaluate FarMap on complex procedurally-generated spatial environments to
demonstrate that this mapping strategy much more rapidly covers the environment
(number of agent steps and wall clock time) and is more efficient in active
memory usage, without loss of performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Merging multiple input descriptors and supervisors in a deep neural
  network for tractogram filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Jörgens, Pierre-Marc Jodoin, Maxime Descoteaux, Rodrigo Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the main issues of the current tractography methods is their high
false-positive rate. Tractogram filtering is an option to remove false-positive
streamlines from tractography data in a post-processing step. In this paper, we
train a deep neural network for filtering tractography data in which every
streamline of a tractogram is classified as {\em plausible, implausible}, or
{\em inconclusive}. For this, we use four different tractogram filtering
strategies as supervisors: TractQuerier, RecobundlesX, TractSeg, and an
anatomy-inspired filter. Their outputs are combined to obtain the
classification labels for the streamlines. We assessed the importance of
different types of information along the streamlines for performing this
classification task, including the coordinates of the streamlines, diffusion
data, landmarks, T1-weighted information, and a brain parcellation. We found
that the streamline coordinates are the most relevant followed by the diffusion
data in this particular classification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoAdapt: A multi-stream evaluation study of adaptation to real-world
  egocentric user video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias De Lange, Hamid Eghbalzadeh, Reuben Tan, Michael Iuzzolino, Franziska Meier, Karl Ridgeway
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In egocentric action recognition a single population model is typically
trained and subsequently embodied on a head-mounted device, such as an
augmented reality headset. While this model remains static for new users and
environments, we introduce an adaptive paradigm of two phases, where after
pretraining a population model, the model adapts on-device and online to the
user's experience. This setting is highly challenging due to the change from
population to user domain and the distribution shifts in the user's data
stream. Coping with the latter in-stream distribution shifts is the focus of
continual learning, where progress has been rooted in controlled benchmarks but
challenges faced in real-world applications often remain unaddressed. We
introduce EgoAdapt, a benchmark for real-world egocentric action recognition
that facilitates our two-phased adaptive paradigm, and real-world challenges
naturally occur in the egocentric video streams from Ego4d, such as long-tailed
action distributions and large-scale classification over 2740 actions. We
introduce an evaluation framework that directly exploits the user's data stream
with new metrics to measure the adaptation gain over the population model,
online generalization, and hindsight performance. In contrast to single-stream
evaluation in existing works, our framework proposes a meta-evaluation that
aggregates the results from 50 independent user streams. We provide an
extensive empirical study for finetuning and experience replay.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology
  Reporting <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Matthias Keicher, Ege Özsoy, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. We will make all annotations and
our code for annotation generation, model evaluation, and training publicly
available upon acceptance. Our dataset and code is available at
https://github.com/ChantalMP/Rad-ReStruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards A Scalable Solution for Improving Multi-Group Fairness in
  Compositional Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Atwood, Tina Tian, Ben Packer, Meghana Deodhar, Jilin Chen, Alex Beutel, Flavien Prost, Ahmad Beirami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rich literature on machine learning fairness, relatively little
attention has been paid to remediating complex systems, where the final
prediction is the combination of multiple classifiers and where multiple groups
are present. In this paper, we first show that natural baseline approaches for
improving equal opportunity fairness scale linearly with the product of the
number of remediated groups and the number of remediated prediction labels,
rendering them impractical. We then introduce two simple techniques, called
{\em task-overconditioning} and {\em group-interleaving}, to achieve a constant
scaling in this multi-group multi-label setup. Our experimental results in
academic and real-world environments demonstrate the effectiveness of our
proposal at mitigation within this environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Open-Source Knowledge Graph Ecosystem for the Life Sciences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiffany J. Callahan, Ignacio J. Tripodi, Adrianne L. Stefanski, Luca Cappelletti, Sanya B. Taneja, Jordan M. Wyrwa, Elena Casiraghi, Nicolas A. Matentzoglu, Justin Reese, Jonathan C. Silverstein, Charles Tapley Hoyt, Richard D. Boyce, Scott A. Malec, Deepak R. Unni, Marcin P. Joachimiak, Peter N. Robinson, Christopher J. Mungall, Emanuele Cavalleri, Tommaso Fontana, Giorgio Valentini, Marco Mesiti, Lucas A. Gillenwater, Brook Santangelo, Nicole A. Vasilevsky, Robert Hoehndorf, Tellen D. Bennett, Patrick B. Ryan, George Hripcsak, Michael G. Kahn, Michael Bada, William A. Baumgartner Jr, Lawrence E. Hunter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translational research requires data at multiple scales of biological
organization. Advancements in sequencing and multi-omics technologies have
increased the availability of these data but researchers face significant
integration challenges. Knowledge graphs (KGs) are used to model complex
phenomena, and methods exist to automatically construct them. However, tackling
complex biomedical integration problems requires flexibility in the way
knowledge is modeled. Moreover, existing KG construction methods provide robust
tooling at the cost of fixed or limited choices among knowledge representation
models. PheKnowLator (Phenotype Knowledge Translator) is a semantic ecosystem
for automating the FAIR (Findable, Accessible, Interoperable, and Reusable)
construction of ontologically grounded KGs with fully customizable knowledge
representation. The ecosystem includes KG construction resources (e.g., data
preparation APIs), analysis tools (e.g., SPARQL endpoints and abstraction
algorithms), and benchmarks (e.g., prebuilt KGs and embeddings). We evaluate
the ecosystem by surveying open-source KG construction methods and analyzing
its computational performance when constructing 12 large-scale KGs. With
flexible knowledge representation, PheKnowLator enables fully customizable KGs
without compromising performance or usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Causal Ordering Prior for Unsupervised Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avinash Kori, Pedro Sanchez, Konstantinos Vilouras, Ben Glocker, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised representation learning with variational inference relies
heavily on independence assumptions over latent variables. Causal
representation learning (CRL), however, argues that factors of variation in a
dataset are, in fact, causally related. Allowing latent variables to be
correlated, as a consequence of causal relationships, is more realistic and
generalisable. So far, provably identifiable methods rely on: auxiliary
information, weak labels, and interventional or even counterfactual data.
Inspired by causal discovery with functional causal models, we propose a fully
unsupervised representation learning method that considers a data generation
process with a latent additive noise model (ANM). We encourage the latent space
to follow a causal ordering via loss function based on the Hessian of the
latent distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Objaverse-XL: A Universe of 10M+ 3D Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, Ali Farhadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing and 2D vision models have attained remarkable
proficiency on many tasks primarily by escalating the scale of training data.
However, 3D vision tasks have not seen the same progress, in part due to the
challenges of acquiring high-quality 3D data. In this work, we present
Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises
deduplicated 3D objects from a diverse set of sources, including manually
designed objects, photogrammetry scans of landmarks and everyday items, and
professional scans of historic and antique artifacts. Representing the largest
scale and diversity in the realm of 3D datasets, Objaverse-XL enables
significant new possibilities for 3D vision. Our experiments demonstrate the
improvements enabled with the scale provided by Objaverse-XL. We show that by
training Zero123 on novel view synthesis, utilizing over 100 million multi-view
rendered images, we achieve strong zero-shot generalization abilities. We hope
that releasing Objaverse-XL will enable further innovations in the field of 3D
vision at scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiobjective Hydropower Reservoir Operation Optimization with
  <span class="highlight-title">Transformer</span>-Based Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rixin Wu, Ran Wang, Jie Hao, Qiang Wu, Ping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to shortage of water resources and increasing water demands, the joint
operation of multireservoir systems for balancing power generation, ecological
protection, and the residential water supply has become a critical issue in
hydropower management. However, the numerous constraints and nonlinearity of
multiple reservoirs make solving this problem time-consuming. To address this
challenge, a deep reinforcement learning approach that incorporates a
transformer framework is proposed. The multihead attention mechanism of the
encoder effectively extracts information from reservoirs and residential areas,
and the multireservoir attention network of the decoder generates suitable
operational decisions. The proposed method is applied to Lake Mead and Lake
Powell in the Colorado River Basin. The experimental results demonstrate that
the transformer-based deep reinforcement learning approach can produce
appropriate operational outcomes. Compared to a state-of-the-art method, the
operation strategies produced by the proposed approach generate 10.11% more
electricity, reduce the amended annual proportional flow deviation by 39.69%,
and increase water supply revenue by 4.10%. Consequently, the proposed approach
offers an effective method for the multiobjective operation of multihydropower
reservoir systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 figures, 16 pages, submitted to Journal of Hydrology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Active Subspaces and Discovering Important Features with
  Gaussian Radial Basis Functions Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny D'Agostino, Ilija Ilievski, Christine Annette Shoemaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing a model that achieves a strong predictive performance and at the
same time is interpretable by humans is one of the most difficult challenges in
machine learning research due to the conflicting nature of these two
objectives. To address this challenge, we propose a modification of the Radial
Basis Function Neural Network model by equipping its Gaussian kernel with a
learnable precision matrix. We show that precious information is contained in
the spectrum of the precision matrix that can be extracted once the training of
the model is completed. In particular, the eigenvectors explain the directions
of maximum sensitivity of the model revealing the active subspace and
suggesting potential applications for supervised dimensionality reduction. At
the same time, the eigenvectors highlight the relationship in terms of absolute
variation between the input and the latent variables, thereby allowing us to
extract a ranking of the input variables based on their importance to the
prediction task enhancing the model interpretability. We conducted numerical
experiments for regression, classification, and feature selection tasks,
comparing our model against popular machine learning models and the
state-of-the-art deep learning-based embedding feature selection techniques.
Our results demonstrate that the proposed model does not only yield an
attractive prediction performance with respect to the competitors but also
provides meaningful and interpretable results that potentially could assist the
decision-making process in real-world applications. A PyTorch implementation of
the model is available on GitHub at the following link.
https://github.com/dannyzx/GRBF-NNs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> of Deep Transfer Learning for Anomaly Detection
  in Industrial Time Series: Methods, Applications, and Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Yan, Ahmed Abdulkadir, Matthias Rosenthal, Gerrit A. Schatte, Benjamin F. Grewe, Thilo Stadelmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating the monitoring of industrial processes has the potential to
enhance efficiency and optimize quality by promptly detecting abnormal events
and thus facilitating timely interventions. Deep learning, with its capacity to
discern non-trivial patterns within large datasets, plays a pivotal role in
this process. Standard deep learning methods are suitable to solve a specific
task given a specific type of data. During training, the algorithms demand
large volumes of labeled training data. However, due to the dynamic nature of
processes and the environment, it is impractical to acquire the needed data for
standard deep learning training for every slightly different case anew. Deep
transfer learning offers a solution to this problem. By leveraging knowledge
from related tasks and accounting for variations in data distributions, this
learning framework solves new tasks even with little or no additional labeled
data. The approach bypasses the need to retrain a model from scratch for every
new setup and dramatically reduces the labeled data requirement. This survey
provides an in-depth review of deep transfer learning, examining the problem
settings of transfer learning and classifying the prevailing deep transfer
learning methods. Moreover, we delve into applying deep transfer learning in
the context of a broad spectrum of time series anomaly detection tasks
prevalent in primary industrial domains, e.g., manufacturing process
monitoring, predictive maintenance, energy management, and infrastructure
facility monitoring. We conclude this survey by underlining the challenges and
limitations of deep transfer learning in industrial contexts. We also provide
practical directions for solution design and implementation for these tasks,
leading to specific, actionable suggestions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belief Revision from Probability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Goodman, Bernhard Salow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In previous work ("Knowledge from Probability", TARK 2021) we develop a
question-relative, probabilistic account of belief. On this account, what
someone believes relative to a given question is (i) closed under entailment,
(ii) sufficiently probable given their evidence, and (iii) sensitive to the
relative probabilities of the answers to the question. Here we explore the
implications of this account for the dynamics of belief. We show that the
principles it validates are much weaker than those of orthodox theories of
belief revision like AGM, but still stronger than those valid according to the
popular Lockean theory of belief, which equates belief with high subjective
probability. We then consider a restricted class of models, suitable for many
but not all applications, and identify some further natural principles valid on
this class. We conclude by arguing that the present framework compares
favorably to the rival probabilistic accounts of belief developed by Leitgeb
and by Lin and Kelly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Kripke Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Ding, Krishna Manoorkar, Apostolos Tzimoulis, Ruoding Wang, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work extends Halpern and Pearl's causal models for actual causality to a
possible world semantics environment. Using this framework we introduce a logic
of actual causality with modal operators, which allows for reasoning about
causality in scenarios involving multiple possibilities, temporality, knowledge
and uncertainty. We illustrate this with a number of examples, and conclude by
discussing some future directions for research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterization of AGM Belief Contraction in Terms of Conditionals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Bonanno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a semantic characterization of AGM belief contraction based on
frames consisting of a Kripke belief relation and a Stalnaker-Lewis selection
function. The central idea is as follows. Let K be the initial belief set and
K-A be the contraction of K by the formula A; then B belongs to the set K-A if
and only if, at the actual state, the agent believes B and believes that if
not-A is (were) the case then B is (would be) the case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CILF:Causality Inspired Learning Framework for Out-of-Distribution
  Vehicle Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyi Li, Qifan Xue, Yezhuo Zhang, Xuanpeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction is critical for autonomous driving vehicles. Most
existing methods tend to model the correlation between history trajectory
(input) and future trajectory (output). Since correlation is just a superficial
description of reality, these methods rely heavily on the i.i.d. assumption and
evince a heightened susceptibility to out-of-distribution data. To address this
problem, we propose an Out-of- Distribution Causal Graph (OOD-CG), which
explicitly defines the underlying causal structure of the data with three
entangled latent features: 1) domain-invariant causal feature (IC), 2)
domain-variant causal feature (VC), and 3) domain-variant non-causal feature
(VN ). While these features are confounded by confounder (C) and domain
selector (D). To leverage causal features for prediction, we propose a Causal
Inspired Learning Framework (CILF), which includes three steps: 1) extracting
domain-invariant causal feature by means of an invariance loss, 2) extracting
domain variant feature by domain contrastive learning, and 3) separating
domain-variant causal and non-causal feature by encouraging causal sufficiency.
We evaluate the performance of CILF in different vehicle trajectory prediction
models on the mainstream datasets NGSIM and INTERACTION. Experiments show
promising improvements in CILF on domain generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A DeepLearning Framework for Dynamic Estimation of Origin-Destination
  Sequence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheli Xiong, Defu Lian, Enhong Chen, Gang Chen, Xiaomin Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OD matrix estimation is a critical problem in the transportation domain. The
principle method uses the traffic sensor measured information such as traffic
counts to estimate the traffic demand represented by the OD matrix. The problem
is divided into two categories: static OD matrix estimation and dynamic OD
matrices sequence(OD sequence for short) estimation. The above two face the
underdetermination problem caused by abundant estimated parameters and
insufficient constraint information. In addition, OD sequence estimation also
faces the lag challenge: due to different traffic conditions such as
congestion, identical vehicle will appear on different road sections during the
same observation period, resulting in identical OD demands correspond to
different trips. To this end, this paper proposes an integrated method, which
uses deep learning methods to infer the structure of OD sequence and uses
structural constraints to guide traditional numerical optimization. Our
experiments show that the neural network(NN) can effectively infer the
structure of the OD sequence and provide practical constraints for numerical
optimization to obtain better results. Moreover, the experiments show that
provided structural information contains not only constraints on the spatial
structure of OD matrices but also provides constraints on the temporal
structure of OD sequence, which solve the effect of the lagging problem well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Cheap Talk <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11030v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11030v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Lu, Timon Willi, Alistair Letcher, Jakob Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks in reinforcement learning (RL) often assume
highly-privileged access to the victim's parameters, environment, or data.
Instead, this paper proposes a novel adversarial setting called a Cheap Talk
MDP in which an Adversary can merely append deterministic messages to the
Victim's observation, resulting in a minimal range of influence. The Adversary
cannot occlude ground truth, influence underlying environment dynamics or
reward signals, introduce non-stationarity, add stochasticity, see the Victim's
actions, or access their parameters. Additionally, we present a simple
meta-learning algorithm called Adversarial Cheap Talk (ACT) to train
Adversaries in this setting. We demonstrate that an Adversary trained with ACT
still significantly influences the Victim's training and testing performance,
despite the highly constrained setting. Affecting train-time performance
reveals a new attack vector and provides insight into the success and failure
modes of existing RL algorithms. More specifically, we show that an ACT
Adversary is capable of harming performance by interfering with the learner's
function approximation, or instead helping the Victim's performance by
outputting useful features. Finally, we show that an ACT Adversary can
manipulate messages during train-time to directly and arbitrarily control the
Victim at test-time. Project video and code are available at
https://sites.google.com/view/adversarial-cheap-talk
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at ICML 2023. Project video and code are available at
  https://sites.google.com/view/adversarial-cheap-talk</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Classification of Research Fields in the "Web of Science"
  Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susie Xi Rao, Peter H. Egger, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a hierarchical classification system that automatically
categorizes a scholarly publication using its abstract into a three-tier
hierarchical label set (discipline, field, subfield) in a multi-class setting.
This system enables a holistic categorization of research activities in the
mentioned hierarchy in terms of knowledge production through articles and
impact through citations, permitting those activities to fall into multiple
categories. The classification system distinguishes 44 disciplines, 718 fields
and 1,485 subfields among 160 million abstract snippets in Microsoft Academic
Graph (version 2018-05-17). We used batch training in a modularized and
distributed fashion to address and allow for interdisciplinary and interfield
classifications in single-label and multi-label settings. In total, we have
conducted 3,140 experiments in all considered models (Convolutional Neural
Networks, Recurrent Neural Networks, Transformers). The classification accuracy
is > 90% in 77.13% and 78.19% of the single-label and multi-label
classifications, respectively. We examine the advantages of our classification
by its ability to better align research texts and output with disciplines, to
adequately classify them in an automated way, and to capture the degree of
interdisciplinarity. The proposed system (a set of pre-trained models) can
serve as a backbone to an interactive system for indexing scientific
publications in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in QSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Neural Link Predictors for Data-Efficient Complex Query
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12313v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12313v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Arakelyan, Pasquale Minervini, Daniel Daza, Michael Cochez, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering complex queries on incomplete knowledge graphs is a challenging
task where a model needs to answer complex logical queries in the presence of
missing knowledge. Prior work in the literature has proposed to address this
problem by designing architectures trained end-to-end for the complex query
answering task with a reasoning process that is hard to interpret while
requiring data and resource-intensive training. Other lines of research have
proposed re-using simple neural link predictors to answer complex queries,
reducing the amount of training data by orders of magnitude while providing
interpretable answers. The neural link predictor used in such approaches is not
explicitly optimised for the complex query answering task, implying that its
scores are not calibrated to interact together. We propose to address these
problems via CQD$^{\mathcal{A}}$, a parameter-efficient score \emph{adaptation}
model optimised to re-calibrate neural link prediction scores for the complex
query answering task. While the neural link predictor is frozen, the adaptation
component -- which only increases the number of model parameters by $0.03\%$ --
is trained on the downstream complex query answering task. Furthermore, the
calibration component enables us to support reasoning over queries that include
atomic negations, which was previously impossible with link predictors. In our
experiments, CQD$^{\mathcal{A}}$ produces significantly more accurate results
than current state-of-the-art methods, improving from $34.4$ to $35.1$ Mean
Reciprocal Rank values averaged across all datasets and query types while using
$\leq 30\%$ of the available training query types. We further show that
CQD$^{\mathcal{A}}$ is data-efficient, achieving competitive results with only
$1\%$ of the training complex queries, and robust in out-of-domain evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>4Graph: Can Large Language Models Understand Graph Structured Data ?
  An Empirical Evaluation and Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models~(LLM) like ChatGPT have become indispensable to
artificial general intelligence~(AGI), demonstrating excellent performance in
various natural language processing tasks. In the real world, graph data is
ubiquitous and an essential part of AGI and prevails in domains like social
network analysis, bioinformatics and recommender systems. The training corpus
of large language models often includes some algorithmic components, which
allows them to achieve certain effects on some graph data-related problems.
However, there is still little research on their performance on a broader range
of graph-structured data. In this study, we conduct an extensive investigation
to assess the proficiency of LLMs in comprehending graph data, employing a
diverse range of structural and semantic-related tasks. Our analysis
encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph
understanding. Through our study, we not only uncover the current limitations
of language models in comprehending graph structures and performing associated
reasoning tasks but also emphasize the necessity for further advancements and
novel approaches to enhance their graph processing capabilities. Our findings
contribute valuable insights towards bridging the gap between language models
and graph understanding, paving the way for more effective graph mining and
knowledge extraction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAINE: Scientific Annotation and Inference Engine of Scientific Research <span class="chip">AACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susie Xi Rao, Yilei Tu, Peter H. Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SAINE, an Scientific Annotation and Inference ENgine based on a
set of standard open-source software, such as Label Studio and MLflow. We show
that our annotation engine can benefit the further development of a more
accurate classification. Based on our previous work on hierarchical discipline
classifications, we demonstrate its application using SAINE in understanding
the space for scholarly publications. The user study of our annotation results
shows that user input collected with the help of our system can help us better
understand the classification process. We believe that our work will help to
foster greater transparency and better understand scientific research. Our
annotation and inference engine can further support the downstream meta-science
projects. We welcome collaboration and feedback from the scientific community
on these projects. The demonstration video can be accessed from
https://youtu.be/yToO-G9YQK4. A live demo website is available at
https://app.heartex.com/user/signup/?token=e2435a2f97449fa1 upon free
registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in IJCNLP-AACL Demo 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defining data science: a new field of inquiry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael L Brodie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data science is not a science. It is a research paradigm. Its power, scope,
and scale will surpass science, our most powerful research paradigm, to enable
knowledge discovery and change our world. We have yet to understand and define
it, vital to realizing its potential and managing its risks. Modern data
science is in its infancy. Emerging slowly since 1962 and rapidly since 2000,
it is a fundamentally new field of inquiry, one of the most active, powerful,
and rapidly evolving 21st century innovations. Due to its value, power, and
applicability, it is emerging in 40+ disciplines, hundreds of research areas,
and thousands of applications. Millions of data science publications contain
myriad definitions of data science and data science problem solving. Due to its
infancy, many definitions are independent, application-specific, mutually
incomplete, redundant, or inconsistent, hence so is data science. This research
addresses this data science multiple definitions challenge by proposing the
development of coherent, unified definition based on a data science reference
framework using a data science journal for the data science community to
achieve such a definition. This paper provides candidate definitions for
essential data science artifacts that are required to discuss such a
definition. They are based on the classical research paradigm concept
consisting of a philosophy of data science, the data science problem solving
paradigm, and the six component data science reference framework (axiology,
ontology, epistemology, methodology, methods, technology) that is a frequently
called for unifying framework with which to define, unify, and evolve data
science. It presents challenges for defining data science, solution approaches,
i.e., means for defining data science, and their requirements and benefits as
the basis of a comprehensive solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Really Good Logical Reasoners? A Comprehensive
  Evaluation and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP), exhibiting impressive achievements across various
classic NLP tasks. However, the question of whether LLMs can effectively
address the task of logical reasoning, which requires gradual cognitive
inference similar to human intelligence, remains unanswered. To this end, we
aim to bridge this gap and provide comprehensive evaluations in this paper.
Firstly, to offer systematic evaluations, we select fifteen typical logical
reasoning datasets and organize them into deductive, inductive, abductive and
mixed-form reasoning settings. Considering the comprehensiveness of
evaluations, we include three representative LLMs (i.e., text-davinci-003,
ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,
one-shot and three-shot settings. Secondly, different from previous evaluations
relying only on simple metrics (e.g., accuracy), we propose fine-level
evaluations from objective and subjective manners, covering both answers and
explanations. Additionally, to uncover the logical flaws of LLMs, problematic
cases will be attributed to five error types from two dimensions, i.e.,
evidence selection process and reasoning process. Thirdly, to avoid the
influences of knowledge bias and purely focus on benchmarking the logical
reasoning capability of LLMs, we propose a new dataset with neutral content. It
contains 3,000 samples and covers deductive, inductive and abductive settings.
Based on the in-depth evaluations, this paper finally forms a general
evaluation scheme of logical reasoning capability from six dimensions. It
reflects the pros and cons of LLMs and gives guiding directions for future
works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Pruning Towards Tiny Neural Networks in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Huang, Lan Zhang, Chaoyue Sun, Ruogu Fang, Xiaoyong Yuan, Dapeng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network pruning is an essential technique for reducing the size and
complexity of deep neural networks, enabling large-scale models on devices with
limited resources. However, existing pruning approaches heavily rely on
training data for guiding the pruning strategies, making them ineffective for
federated learning over distributed and confidential datasets. Additionally,
the memory- and computation-intensive pruning process becomes infeasible for
recourse-constrained devices in federated learning. To address these
challenges, we propose FedTiny, a distributed pruning framework for federated
learning that generates specialized tiny models for memory- and
computing-constrained devices. We introduce two key modules in FedTiny to
adaptively search coarse- and finer-pruned specialized models to fit deployment
scenarios with sparse and cheap local computation. First, an adaptive batch
normalization selection module is designed to mitigate biases in pruning caused
by the heterogeneity of local data. Second, a lightweight progressive pruning
module aims to finer prune the models under strict memory and computational
budgets, allowing the pruning policy for each layer to be gradually determined
rather than evaluating the overall model structure. The experimental results
demonstrate the effectiveness of FedTiny, which outperforms state-of-the-art
approaches, particularly when compressing deep models to extremely sparse tiny
models. FedTiny achieves an accuracy improvement of 2.61% while significantly
reducing the computational cost by 95.91% and the memory footprint by 94.01%
compared to state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ICDCS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Isotuning With Applications To Scale-Free Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.14586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.14586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Orseau, Marcus Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We extend and combine several tools of the literature to design fast,
adaptive, anytime and scale-free online learning algorithms. Scale-free regret
bounds must scale linearly with the maximum loss, both toward large losses and
toward very small losses. Adaptive regret bounds demonstrate that an algorithm
can take advantage of easy data and potentially have constant regret. We seek
to develop fast algorithms that depend on as few parameters as possible, in
particular they should be anytime and thus not depend on the time horizon. Our
first and main tool, isotuning, is a generalization of the idea of balancing
the trade-off of the regret. We develop a set of tools to design and analyze
such learning rates easily and show that they adapts automatically to the rate
of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a
factor 2 of the optimal learning rate in hindsight for the same observed
quantities. The second tool is an online correction, which allows us to obtain
centered bounds for many algorithms, to prevent the regret bounds from being
vacuous when the domain is overly large or only partially constrained. The last
tool, null updates, prevents the algorithm from performing overly large
updates, which could result in unbounded regret, or even invalid updates. We
develop a general theory using these tools and apply it to several standard
algorithms. In particular, we (almost entirely) restore the adaptivity to small
losses of FTRL for unbounded domains, design and prove scale-free adaptive
guarantees for a variant of Mirror Descent (at least when the Bregman
divergence is convex in its second argument), extend Adapt-ML-Prod to
scale-free guarantees, and provide several other minor contributions about
Prod, AdaHedge, BOA and Soft-Bayes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distortion-Disentangled Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfeng Wang, Sifan Song, Jionglong Su, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning is well known for its remarkable performance in
representation learning and various downstream computer vision tasks. Recently,
Positive-pair-Only Contrastive Learning (POCL) has achieved reliable
performance without the need to construct positive-negative training sets. It
reduces memory requirements by lessening the dependency on the batch size. The
POCL method typically uses a single loss function to extract the distortion
invariant representation (DIR) which describes the proximity of positive-pair
representations affected by different distortions. This loss function
implicitly enables the model to filter out or ignore the distortion variant
representation (DVR) affected by different distortions. However, existing POCL
methods do not explicitly enforce the disentanglement and exploitation of the
actually valuable DVR. In addition, these POCL methods have been observed to be
sensitive to augmentation strategies. To address these limitations, we propose
a novel POCL framework named Distortion-Disentangled Contrastive Learning
(DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to
explicitly disentangle and exploit the DVR inside the model and feature stream
to improve the overall representation utilization efficiency, robustness and
representation ability. Experiments carried out demonstrate the superiority of
our framework to Barlow Twins and Simsiam in terms of convergence,
representation quality, and robustness on several benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth Monotonic Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Igel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monotonicity constraints are powerful regularizers in statistical modelling.
They can support fairness in computer supported decision making and increase
plausibility in data-driven scientific models. The seminal min-max (MM) neural
network architecture ensures monotonicity, but often gets stuck in undesired
local optima during training because of vanishing gradients. We propose a
simple modification of the MM network using strictly-increasing smooth
non-linearities that alleviates this problem. The resulting smooth min-max
(SMM) network module inherits the asymptotic approximation properties from the
MM architecture. It can be used within larger deep learning systems trained
end-to-end. The SMM module is considerably simpler and less computationally
demanding than state-of-the-art neural networks for monotonic modelling. Still,
in our experiments, it compared favorably to alternative neural and non-neural
approaches in terms of generalization performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Behavior and Common Belief 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meir Friedenberg, Joseph Y. Halpern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For over 25 years, common belief has been widely viewed as necessary for
joint behavior. But this is not quite correct. We show by example that what can
naturally be thought of as joint behavior can occur without common belief. We
then present two variants of common belief that can lead to joint behavior,
even without standard common belief ever being achieved, and show that one of
them, action-stamped common belief, is in a sense necessary and sufficient for
joint behavior. These observations are significant because, as is well known,
common belief is quite difficult to achieve in practice, whereas these variants
are more easily achievable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BTPK-based interpretable method for NER tasks based on Talmudic Public
  Announcement Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Chen, Beishui Liao, Bruno Bentzen, Bo Yuan, Zelai Yao, Haixiao Chi, Dov Gabbay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As one of the basic tasks in natural language processing (NLP), named entity
recognition (NER) is an important basic tool for downstream tasks of NLP, such
as information extraction, syntactic analysis, machine translation and so on.
The internal operation logic of current name entity recognition model is
black-box to the user, so the user has no basis to determine which name entity
makes more sense. Therefore, a user-friendly explainable recognition process
would be very useful for many people. In this paper, we propose a novel
interpretable method, BTPK (Binary Talmudic Public Announcement Logic model),
to help users understand the internal recognition logic of the name entity
recognition tasks based on Talmudic Public Announcement Logic. BTPK model can
also capture the semantic information in the input sentences, that is, the
context dependency of the sentence. We observed the public announcement of BTPK
presents the inner decision logic of BRNNs, and the explanations obtained from
a BTPK model show us how BRNNs essentially handle NER tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Responsive parallelized architecture for deploying deep learning models
  in production environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Verma, Krishna Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recruiters can easily shortlist candidates for jobs via viewing their
curriculum vitae (CV) document. Unstructured document CV beholds candidate's
portfolio and named entities listing details. The main aim of this study is to
design and propose a web oriented, highly responsive, computational pipeline
that systematically predicts CV entities using hierarchically-refined label
attention networks. Deep learning models specialized for named entity
recognition were trained on large dataset to predict relevant fields. The
article suggests an optimal strategy to use a number of deep learning models in
parallel and predict in real time. We demonstrate selection of light weight
micro web framework using Analytical Hierarchy Processing algorithm and focus
on an approach useful to deploy large deep learning model-based pipelines in
production ready environments using microservices. Deployed models and
architecture proposed helped in parsing normal CV in less than 700 milliseconds
for sequential flow of requests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying human-centered AI in developing effective human-AI teaming: A
  perspective of human-AI joint cognitive systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xu, Zaifeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the "teaming" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
  Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frontier AI Regulation: Managing Emerging Risks to Public Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen O'Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, Kevin Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced AI models hold the promise of tremendous benefits for humanity, but
society needs to proactively manage the accompanying risks. In this paper, we
focus on what we term "frontier AI" models: highly capable foundation models
that could possess dangerous capabilities sufficient to pose severe risks to
public safety. Frontier AI models pose a distinct regulatory challenge:
dangerous capabilities can arise unexpectedly; it is difficult to robustly
prevent a deployed model from being misused; and, it is difficult to stop a
model's capabilities from proliferating broadly. To address these challenges,
at least three building blocks for the regulation of frontier models are
needed: (1) standard-setting processes to identify appropriate requirements for
frontier AI developers, (2) registration and reporting requirements to provide
regulators with visibility into frontier AI development processes, and (3)
mechanisms to ensure compliance with safety standards for the development and
deployment of frontier AI models. Industry self-regulation is an important
first step. However, wider societal discussions and government intervention
will be needed to create standards and to ensure compliance with them. We
consider several options to this end, including granting enforcement powers to
supervisory authorities and licensure regimes for frontier AI models. Finally,
we propose an initial set of safety standards. These include conducting
pre-deployment risk assessments; external scrutiny of model behavior; using
risk assessments to inform deployment decisions; and monitoring and responding
to new information about model capabilities and uses post-deployment. We hope
this discussion contributes to the broader conversation on how to balance
public safety risks and innovation benefits from advances at the frontier of AI
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update July 11th: - Added missing footnote back in. - Adjusted author
  order (mistakenly non-alphabetical among the first 6 authors) and adjusted
  affiliations (Jess Whittlestone's affiliation was mistagged and Gillian
  Hadfield had SRI added to her affiliations)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Instructions for Intuitive Human Interaction with
  Robotic Assistants in Field Construction Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somin Park, Xi Wang, Carol C. Menassa, Vineet R. Kamat, Joyce Y. Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of robots is widely considered to have significant potential
of alleviating the issues of worker shortage and stagnant productivity that
afflict the construction industry. However, it is challenging to use fully
automated robots in complex and unstructured construction sites. Human-Robot
Collaboration (HRC) has shown promise of combining human workers' flexibility
and robot assistants' physical abilities to jointly address the uncertainties
inherent in construction work. When introducing HRC in construction, it is
critical to recognize the importance of teamwork and supervision in field
construction and establish a natural and intuitive communication system for the
human workers and robotic assistants. Natural language-based interaction can
enable intuitive and familiar communication with robots for human workers who
are non-experts in robot programming. However, limited research has been
conducted on this topic in construction. This paper proposes a framework to
allow human workers to interact with construction robots based on natural
language instructions. The proposed method consists of three stages: Natural
Language Understanding (NLU), Information Mapping (IM), and Robot Control (RC).
Natural language instructions are input to a language model to predict a tag
for each word in the NLU module. The IM module uses the result of the NLU
module and building component information to generate the final instructional
output essential for a robot to acknowledge and perform the construction task.
A case study for drywall installation is conducted to evaluate the proposed
approach. The obtained results highlight the potential of using natural
language-based interaction to replicate the communication that occurs between
human workers within the context of human-robot teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Power of Explanations for Incremental Training: A
  LIME-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Neelim Mazumder, Niall Lyons, Ashutosh Pandey, Avik Santra, Tinoosh Mohsenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability of neural network prediction is essential to understand
feature importance and gain interpretable insight into neural network
performance. However, explanations of neural network outcomes are mostly
limited to visualization, and there is scarce work that looks to use these
explanations as feedback to improve model performance. In this work, model
explanations are fed back to the feed-forward training to help the model
generalize better. To this extent, a custom weighted loss where the weights are
generated by considering the Euclidean distances between true LIME (Local
Interpretable Model-Agnostic Explanations) explanations and model-predicted
LIME explanations is proposed. Also, in practical training scenarios,
developing a solution that can help the model learn sequentially without losing
information on previous data distribution is imperative due to the
unavailability of all the training data at once. Thus, the framework
incorporates the custom weighted loss with Elastic Weight Consolidation (EWC)
to maintain performance in sequential testing sets. The proposed custom
training procedure results in a consistent enhancement of accuracy ranging from
0.5% to 1.5% throughout all phases of the incremental learning setup compared
to traditional loss-based training methods for the keyword spotting task using
the Google Speech Commands dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Instance Learning via Iterative Self-Paced Supervised
  Contrastive Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Liu, Weicheng Zhu, Yiqiu Shen, Sheng Liu, Narges Razavian, Krzysztof J. Geras, Carlos Fernandez-Granda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations for individual instances when only bag-level labels
are available is a fundamental challenge in multiple instance learning (MIL).
Recent works have shown promising results using contrastive self-supervised
learning (CSSL), which learns to push apart representations corresponding to
two different randomly-selected instances. Unfortunately, in real-world
applications such as medical image classification, there is often class
imbalance, so randomly-selected instances mostly belong to the same majority
class, which precludes CSSL from learning inter-class differences. To address
this issue, we propose a novel framework, Iterative Self-paced Supervised
Contrastive Learning for MIL Representations (ItS2CLR), which improves the
learned representation by exploiting instance-level pseudo labels derived from
the bag-level labels. The framework employs a novel self-paced sampling
strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three
medical datasets, showing that it improves the quality of instance-level pseudo
labels and representations, and outperforms existing MIL methods in terms of
both bag and instance level accuracy. Code is available at
https://github.com/Kangningthu/ItS2CLR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready version. The first two authors contribute
  equally. The last two authors are joint last authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Specialization as the Key to Make Large Language Models
  Disruptive: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18703v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18703v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Jian Pei, Carl Yang, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced the field of natural
language processing (NLP), providing a highly useful, task-agnostic foundation
for a wide range of applications. However, directly applying LLMs to solve
sophisticated problems in specific domains meets many hurdles, caused by the
heterogeneity of domain data, the sophistication of domain knowledge, the
uniqueness of domain objectives, and the diversity of the constraints (e.g.,
various social norms, cultural conformity, religious beliefs, and ethical
standards in the domain applications). Domain specification techniques are key
to make large language models disruptive in many applications. Specifically, to
solve these hurdles, there has been a notable increase in research and
practices conducted in recent years on the domain specialization of LLMs. This
emerging field of study, with its substantial potential for impact,
necessitates a comprehensive and systematic review to better summarize and
guide ongoing work in this area. In this article, we present a comprehensive
survey on domain specification techniques for large language models, an
emerging direction critical for large language model applications. First, we
propose a systematic taxonomy that categorizes the LLM domain-specialization
techniques based on the accessibility to LLMs and summarizes the framework for
all the subcategories as well as their relations and differences to each other.
Second, we present an extensive taxonomy of critical application domains that
can benefit dramatically from specialized LLMs, discussing their practical
significance and open challenges. Last, we offer our insights into the current
research status and future trends in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An <span class="highlight-title">Overview</span> of Catastrophic AI Risks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Hendrycks, Mantas Mazeika, Thomas Woodside
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in artificial intelligence (AI) have sparked growing
concerns among experts, policymakers, and world leaders regarding the potential
for increasingly advanced AI systems to pose catastrophic risks. Although
numerous risks have been detailed separately, there is a pressing need for a
systematic discussion and illustration of the potential dangers to better
inform efforts to mitigate them. This paper provides an overview of the main
sources of catastrophic AI risks, which we organize into four categories:
malicious use, in which individuals or groups intentionally use AIs to cause
harm; AI race, in which competitive environments compel actors to deploy unsafe
AIs or cede control to AIs; organizational risks, highlighting how human
factors and complex systems can increase the chances of catastrophic accidents;
and rogue AIs, describing the inherent difficulty in controlling agents far
more intelligent than humans. For each category of risk, we describe specific
hazards, present illustrative stories, envision ideal scenarios, and propose
practical suggestions for mitigating these dangers. Our goal is to foster a
comprehensive understanding of these risks and inspire collective and proactive
efforts to ensure that AIs are developed and deployed in a safe manner.
Ultimately, we hope this will allow us to realize the benefits of this powerful
technology while minimizing the potential for catastrophic outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Images are More Memorable to Machines? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Han, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, Ian Reid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of measuring and predicting how memorable an
image is to pattern recognition machines, as a path to explore machine
intelligence. Firstly, we propose a self-supervised machine memory
quantification pipeline, dubbed ``MachineMem measurer'', to collect machine
memorability scores of images. Similar to humans, machines also tend to
memorize certain kinds of images, whereas the types of images that machines and
humans memorize are different. Through in-depth analysis and comprehensive
visualizations, we gradually unveil that``complex" images are usually more
memorable to machines. We further conduct extensive experiments across 11
different machines (from linear classifiers to modern ViTs) and 9 pre-training
methods to analyze and understand machine memory. This work proposes the
concept of machine memorability and opens a new research direction at the
interface between machine memory and visual data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/JunlinHan/MachineMem Project page:
  https://junlinhan.github.io/projects/machinemem.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Subgroups of ICU Patients Using End-to-End Multivariate
  Time-Series Clustering Algorithm Based on Real-World Vital Signs Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongyue Shi, Zhilong Zhang, Wentie Liu, Junhua Fang, Jianguo Hao, Shuai Jin, Huiying Zhao, Guilan Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study employed the MIMIC-IV database as data source to investigate the
use of dynamic, high-frequency, multivariate time-series vital signs data,
including temperature, heart rate, mean blood pressure, respiratory rate, and
SpO2, monitored first 8 hours data in the ICU stay. Various clustering
algorithms were compared, and an end-to-end multivariate time series clustering
system called Time2Feat, combined with K-Means, was chosen as the most
effective method to cluster patients in the ICU. In clustering analysis, data
of 8,080 patients admitted between 2008 and 2016 was used for model development
and 2,038 patients admitted between 2017 and 2019 for model validation. By
analyzing the differences in clinical mortality prognosis among different
categories, varying risks of ICU mortality and hospital mortality were found
between different subgroups. Furthermore, the study visualized the trajectory
of vital signs changes. The findings of this study provide valuable insights
into the potential use of multivariate time-series clustering systems in
patient management and monitoring in the ICU setting.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-10T00:00:00Z">2023-07-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Spatial Features from Audio-Visual Correspondence in Egocentric
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagnik Majumder, Ziad Al-Halah, Kristen Grauman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a self-supervised method for learning representations based on
spatial audio-visual correspondences in egocentric videos. In particular, our
method leverages a masked auto-encoding framework to synthesize masked binaural
audio through the synergy of audio and vision, thereby learning useful spatial
relationships between the two modalities. We use our pretrained features to
tackle two downstream video tasks requiring spatial understanding in social
scenarios: active speaker detection and spatial audio denoising. We show
through extensive experiments that our features are generic enough to improve
over multiple state-of-the-art baselines on two public challenging egocentric
video datasets, EgoCom and EasyCom. Project:
http://vision.cs.utexas.edu/projects/ego_av_corr.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vocal Tract Area Estimation by Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Südholt, Mateo Cámara, Zhiyuan Xu, Joshua D. Reiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulatory features can provide interpretable and flexible controls for the
synthesis of human vocalizations by allowing the user to directly modify
parameters like vocal strain or lip position. To make this manipulation through
resynthesis possible, we need to estimate the features that result in a desired
vocalization directly from audio recordings. In this work, we propose a
white-box optimization technique for estimating glottal source parameters and
vocal tract shapes from audio recordings of human vowels. The approach is based
on inverse filtering and optimizing the frequency response of a wave\-guide
model of the vocal tract with gradient descent, propagating error gradients
through the mapping of articulatory features to the vocal tract area function.
We apply this method to the task of matching the sound of the Pink Trombone, an
interactive articulatory synthesizer, to a given vocalization. We find that our
method accurately recovers control functions for audio generated by the Pink
Trombone itself. We then compare our technique against evolutionary
optimization algorithms and a neural network trained to predict control
parameters from audio. A subjective evaluation finds that our approach
outperforms these black-box optimization baselines on the task of reproducing
human vocalizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to DAFx 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VampNet: Music Generation via Masked Acoustic Token Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VampNet, a masked acoustic token modeling approach to music
synthesis, compression, inpainting, and variation. We use a variable masking
schedule during training which allows us to sample coherent music from the
model by applying a variety of masking approaches (called prompts) during
inference. VampNet is non-autoregressive, leveraging a bidirectional
transformer architecture that attends to all tokens in a forward pass. With
just 36 sampling passes, VampNet can generate coherent high-fidelity musical
waveforms. We show that by prompting VampNet in various ways, we can apply it
to tasks like music compression, inpainting, outpainting, continuation, and
looping with variation (vamping). Appropriately prompted, VampNet is capable of
maintaining style, genre, instrumentation, and other high-level aspects of the
music. This flexible prompting capability makes VampNet a powerful music
co-creation tool. Code and audio samples are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023
  Speech-to-Speech Translation Task <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Song, Yi lei, Peikun Chen, Yiqing Cao, Kun Wei, Yongmao Zhang, Lei Xie, Ning Jiang, Guoqing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech
translation (S2ST) task which aims to translate from English speech of
multi-source to Chinese speech. The system is built in a cascaded manner
consisting of automatic speech recognition (ASR), machine translation (MT), and
text-to-speech (TTS). We make tremendous efforts to handle the challenging
multi-source input. Specifically, to improve the robustness to multi-source
speech input, we adopt various data augmentation strategies and a ROVER-based
score fusion on multiple ASR model outputs. To better handle the noisy ASR
transcripts, we introduce a three-stage fine-tuning strategy to improve
translation accuracy. Finally, we build a TTS model with high naturalness and
sound quality, which leverages a two-stage framework, using network bottleneck
features as a robust intermediate representation for speaker timbre and
linguistic content disentanglement. Based on the two-stage framework,
pre-trained speaker embedding is leveraged as a condition to transfer the
speaker timbre in the source English speech to the translated Chinese speech.
Experimental results show that our system has high translation accuracy, speech
naturalness, sound quality, and speaker similarity. Moreover, it shows good
robustness to multi-source data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT@ACL 2023 system paper. Our submitted system ranks 1st in the
  S2ST task of the IWSLT 2023 evaluation campaign</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EchoVest: Real-Time Sound Classification and Depth Perception Expressed
  through Transcutaneous Electrical Nerve Stimulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Choe, Siddhant Sood, Ryan Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over 1.5 billion people worldwide live with hearing impairment. Despite
various technologies that have been created for individuals with such
disabilities, most of these technologies are either extremely expensive or
inaccessible for everyday use in low-medium income countries. In order to
combat this issue, we have developed a new assistive device, EchoVest, for
blind/deaf people to intuitively become more aware of their environment.
EchoVest transmits vibrations to the user's body by utilizing transcutaneous
electric nerve stimulation (TENS) based on the source of the sounds. EchoVest
also provides various features, including sound localization, sound
classification, noise reduction, and depth perception. We aimed to outperform
CNN-based machine-learning models, the most commonly used machine learning
model for classification tasks, in accuracy and computational costs. To do so,
we developed and employed a novel audio pipeline that adapts the Audio
Spectrogram Transformer (AST) model, an attention-based model, for our sound
classification purposes, and Fast Fourier Transforms for noise reduction. The
application of Otsu's Method helped us find the optimal thresholds for
background noise sound filtering and gave us much greater accuracy. In order to
calculate direction and depth accurately, we applied Complex Time Difference of
Arrival algorithms and SOTA localization. Our last improvement was to use blind
source separation to make our algorithms applicable to multiple microphone
inputs. The final algorithm achieved state-of-the-art results on numerous
checkpoints, including a 95.7\% accuracy on the ESC-50 dataset for
environmental sound classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting an External Microphone for Binaural RTF-Vector-Based
  Direction of Arrival Estimation for Multiple Speakers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fejgin, Simon Doclo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In hearing aid applications, an important objective is to accurately estimate
the direction of arrival (DOA) of multiple speakers in noisy and reverberant
environments. Recently, we proposed a binaural DOA estimation method, where the
DOAs of the speakers are estimated by selecting the directions for which the
so-called Hermitian angle spectrum between the estimated relative transfer
function (RTF) vector and a database of prototype anechoic RTF vectors is
maximized. The RTF vector is estimated using the covariance whitening (CW)
method, which requires a computationally complex generalized eigenvalue
decomposition. The spatial spectrum is obtained by only considering frequencies
where it is likely that one speaker dominates over the other speakers, noise
and reverberation. In this contribution, we exploit the availability of an
external microphone that is spatially separated from the hearing aid
microphones and consider a low-complexity RTF vector estimation method that
assumes a low spatial coherence between the undesired components in the
external microphone and the hearing aid microphones. Using recordings of two
speakers and diffuse-like babble noise in acoustic environments with mild
reverberation and low signal-to-noise ratio, simulation results show that the
proposed method yields a comparable DOA estimation performance as the CW method
at a lower computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for Forum Acusticum 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HCLAS-X: Hierarchical and Cascaded Lyrics Alignment System Using
  Multimodal Cross-Correlation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsung Kang, Soochul Park, Keunwoo Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the challenge of lyrics alignment, which involves
aligning the lyrics and vocal components of songs. This problem requires the
alignment of two distinct modalities, namely text and audio. To overcome this
challenge, we propose a model that is trained in a supervised manner, utilizing
the cross-correlation matrix of latent representations between vocals and
lyrics. Our system is designed in a hierarchical and cascaded manner. It
predicts synced time first on a sentence-level and subsequently on a
word-level. This design enables the system to process long sequences, as the
cross-correlation uses quadratic memory with respect to sequence length. In our
experiments, we demonstrate that our proposed system achieves a significant
improvement in mean average error, showcasing its robustness in comparison to
the previous state-of-the-art model. Additionally, we conduct a qualitative
analysis of the system after successfully deploying it in several music
streaming services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Piano Transcription with Hierarchical Frequency-Time
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keisuke Toyama, Taketo Akama, Yukara Ikemiya, Yuhta Takida, Wei-Hsiang Liao, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Taking long-term spectral and temporal dependencies into account is essential
for automatic piano transcription. This is especially helpful when determining
the precise onset and offset for each note in the polyphonic piano content. In
this case, we may rely on the capability of self-attention mechanism in
Transformers to capture these long-term dependencies in the frequency and time
axes. In this work, we propose hFT-Transformer, which is an automatic music
transcription method that uses a two-level hierarchical frequency-time
Transformer architecture. The first hierarchy includes a convolutional block in
the time axis, a Transformer encoder in the frequency axis, and a Transformer
decoder that converts the dimension in the frequency axis. The output is then
fed into the second hierarchy which consists of another Transformer encoder in
the time axis. We evaluated our method with the widely used MAPS and MAESTRO
v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the
F1-scores of the metrics among Frame, Note, Note with Offset, and Note with
Offset and Velocity estimations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, to be published in ISMIR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge Storage Management Recipe with Zero-Shot Data Compression for Road
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Uju Gim, Myung Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show edge computing-based road anomaly detection systems which
may also conduct data collection simultaneously. However, the edge computers
will have small data storage but we need to store the collected audio samples
for a long time in order to update existing models or develop a novel method.
Therefore, we should consider an approach for efficient storage management
methods while preserving high-fidelity audio. A hardware-perspective approach,
such as using a low-resolution microphone, is an intuitive way to reduce file
size but is not recommended because it fundamentally cuts off high-frequency
components. On the other hand, a computational file compression approach that
encodes collected high-resolution audio into a compact code should be
recommended because it also provides a corresponding decoding method. Motivated
by this, we propose a way of simple yet effective pre-trained autoencoder-based
data compression method. The pre-trained autoencoder is trained for the purpose
of audio super-resolution so it can be utilized to encode or decode any
arbitrary sampling rate. Moreover, it will reduce the communication cost for
data transmission from the edge to the central server. Via the comparative
experiments, we confirm that the zero-shot audio compression and decompression
highly preserve anomaly detection performance while enhancing storage and
transmission efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Song <span class="highlight-title">Dataset</span> (CoSoD): An annotated <span class="highlight-title">dataset</span> of multi-artist
  collaborations in popular music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michèle Duguay, Kate Mancey, Johanna Devaney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist
collaborations from the 2010-2019 Billboard "Hot 100" year-end charts. The
corpus is annotated with formal sections, aspects of vocal production
(including reverberation, layering, panning, and gender of the performers), and
relevant metadata. CoSoD complements other popular music datasets by focusing
exclusively on musical collaborations between independent acts. In addition to
facilitating the study of song form and vocal production, CoSoD allows for the
in-depth study of gender as it relates to various timbral, pitch, and formal
parameters in musical collaborations. In this paper, we detail the contents of
the dataset and outline the annotation process. We also present an experiment
using CoSoD that examines how the use of reverberation, layering, and panning
are related to the gender of the artist. In this experiment, we find that men's
voices are on average treated with less reverberation and occupy a more narrow
position in the stereo mix than women's voices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 24th International Society
  for Music Information Retrieval Conference (ISMIR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study of <span class="highlight-title">Self-Supervised</span> Speech Representations in Read
  and Spontaneous TTS <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has explored using self-supervised learning (SSL) speech
representations such as wav2vec2.0 as the representation medium in standard
two-stage TTS, in place of conventionally used mel-spectrograms. It is however
unclear which speech SSL is the better fit for TTS, and whether or not the
performance differs between read and spontaneous TTS, the later of which is
arguably more challenging. This study aims at addressing these questions by
testing several speech SSLs, including different layers of the same SSL, in
two-stage TTS on both read and spontaneous corpora, while maintaining constant
TTS model architecture and training settings. Results from listening tests show
that the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other
tested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our work
sheds light on both how speech SSL can readily improve current TTS systems, and
how SSLs compare in the challenging generative task of TTS. Audio examples can
be found at https://www.speech.kth.se/tts-demos/ssr_tts
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures. ICASSP Workshop SASB (Self-Supervision in Audio,
  Speech and Beyond)2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mathematical Foundations of Complex Tonality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04974v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04974v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey R. Boland, Lane P. Hughston
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equal temperament, in which semitones are tuned in the irrational ratio of
$2^{1/12} : 1$, is best seen as a serviceable compromise, sacrificing purity
for flexibility. Just intonation, in which intervals are given by products of
powers of $2$, $3$, and $5$, is more natural, but of limited flexibility. We
propose a new scheme in which ratios of Gaussian integers form the basis of an
abstract tonal system. The tritone, so problematic in just temperament, given
ambiguously by the ratios $\tfrac{45}{32}$, $\tfrac{64}{45}$, $\tfrac{36}{25}$,
$\tfrac{25}{18}$, none satisfactory, is in our scheme represented by the
complex ratio $1 + \rm{i} : 1$. The major and minor whole tones, given by
intervals of $\tfrac{9}{8}$ and $\tfrac{10}{9}$, can each be factorized into
products of complex semitones, giving us a major complex semitone
$\tfrac{3}{4}(1 + \rm{i})$ and a minor complex semitone $\tfrac{1}{3}(3 +
\rm{i})$. The perfect third, given by the interval $\tfrac{5}{4}$, factorizes
into the product of a complex whole tone $\tfrac{1}{2}(1 + 2\rm{i})$ and its
complex conjugate. Augmented with these supplementary tones, the resulting
scheme of complex intervals based on products of low powers of Gaussian primes
leads to the construction of a complete system of major and minor scales in all
keys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, to appear in Journal of Mathematics and Music</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion-Conditioned Melody Harmonization with Hierarchical Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03718v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03718v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing melody harmonization models have made great progress in improving
the quality of generated harmonies, but most of them ignored the emotions
beneath the music. Meanwhile, the variability of harmonies generated by
previous methods is insufficient. To solve these problems, we propose a novel
LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the
influence of emotional conditions on melody harmonization, while improving the
quality of generated harmonies and capturing the abundant variability of chord
progressions. Specifically, LHVAE incorporates latent variables and emotional
conditions at different levels (piece- and bar-level) to model the global and
local music properties. Additionally, we introduce an attention-based melody
context vector at each step to better learn the correspondence between melodies
and harmonies. Objective experimental results show that our proposed model
outperforms other LSTM-based models. Through subjective evaluation, we conclude
that only altering the type of chord hardly changes the overall emotion of the
music. The qualitative analysis demonstrates the ability of our model to
generate variable harmonies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SMC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What do End-to-End Speech Models Learn about Speaker, Language and
  Channel Information? A Layer-wise and Neuron-level Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.00439v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.00439v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shammur Absar Chowdhury, Nadir Durrani, Ahmed Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are inherently opaque and challenging to interpret.
Unlike hand-crafted feature-based models, we struggle to comprehend the
concepts learned and how they interact within these models. This understanding
is crucial not only for debugging purposes but also for ensuring fairness in
ethical decision-making. In our study, we conduct a post-hoc functional
interpretability analysis of pretrained speech models using the probing
framework [1]. Specifically, we analyze utterance-level representations of
speech models trained for various tasks such as speaker recognition and dialect
identification. We conduct layer and neuron-wise analyses, probing for speaker,
language, and channel properties. Our study aims to answer the following
questions: i) what information is captured within the representations? ii) how
is it represented and distributed? and iii) can we identify a minimal subset of
the network that possesses this information?
  Our results reveal several novel findings, including: i) channel and gender
information are distributed across the network, ii) the information is
redundantly available in neurons with respect to a task, iii) complex
properties such as dialectal information are encoded only in the task-oriented
pretrained network, iv) and is localised in the upper layers, v) we can extract
a minimal subset of neurons encoding the pre-defined property, vi) salient
neurons are sometimes shared between properties, vii) our analysis highlights
the presence of biases (for example gender) in the network. Our
cross-architectural comparison indicates that: i) the pretrained models capture
speaker-invariant information, and ii) CNN models are competitive with
Transformer models in encoding various understudied properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CSL journal. Keywords: Speech, Neuron Analysis,
  Interpretibility, Diagnostic Classifier, AI explainability, End-to-End
  Architecture</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Spatial Features from Audio-Visual Correspondence in Egocentric
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagnik Majumder, Ziad Al-Halah, Kristen Grauman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a self-supervised method for learning representations based on
spatial audio-visual correspondences in egocentric videos. In particular, our
method leverages a masked auto-encoding framework to synthesize masked binaural
audio through the synergy of audio and vision, thereby learning useful spatial
relationships between the two modalities. We use our pretrained features to
tackle two downstream video tasks requiring spatial understanding in social
scenarios: active speaker detection and spatial audio denoising. We show
through extensive experiments that our features are generic enough to improve
over multiple state-of-the-art baselines on two public challenging egocentric
video datasets, EgoCom and EasyCom. Project:
http://vision.cs.utexas.edu/projects/ego_av_corr.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behavioral Analysis of Pathological Speaker Embeddings of Patients
  During Oncological Treatment of Oral Cancer <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jenthe Thienpondt, Kris Demuynck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we analyze the behavior of speaker embeddings of patients
during oral cancer treatment. First, we found that pre- and post-treatment
speaker embeddings differ significantly, notifying a substantial change in
voice characteristics. However, a partial recovery to pre-operative voice
traits is observed after 12 months post-operation. Secondly, the same-speaker
similarity at distinct treatment stages is similar to healthy speakers,
indicating that the embeddings can capture characterizing features of even
severely impaired speech. Finally, a speaker verification analysis signifies a
stable false positive rate and variable false negative rate when combining
speech samples of different treatment stages. This indicates robustness of the
embeddings towards other speakers, while still capturing the changing voice
characteristics during treatment. To the best of our knowledge, this is the
first analysis of speaker embeddings during oral cancer treatment of patients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>proceedings of INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vocal Tract Area Estimation by Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Südholt, Mateo Cámara, Zhiyuan Xu, Joshua D. Reiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulatory features can provide interpretable and flexible controls for the
synthesis of human vocalizations by allowing the user to directly modify
parameters like vocal strain or lip position. To make this manipulation through
resynthesis possible, we need to estimate the features that result in a desired
vocalization directly from audio recordings. In this work, we propose a
white-box optimization technique for estimating glottal source parameters and
vocal tract shapes from audio recordings of human vowels. The approach is based
on inverse filtering and optimizing the frequency response of a wave\-guide
model of the vocal tract with gradient descent, propagating error gradients
through the mapping of articulatory features to the vocal tract area function.
We apply this method to the task of matching the sound of the Pink Trombone, an
interactive articulatory synthesizer, to a given vocalization. We find that our
method accurately recovers control functions for audio generated by the Pink
Trombone itself. We then compare our technique against evolutionary
optimization algorithms and a neural network trained to predict control
parameters from audio. A subjective evaluation finds that our approach
outperforms these black-box optimization baselines on the task of reproducing
human vocalizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to DAFx 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VampNet: Music Generation via Masked Acoustic Token Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VampNet, a masked acoustic token modeling approach to music
synthesis, compression, inpainting, and variation. We use a variable masking
schedule during training which allows us to sample coherent music from the
model by applying a variety of masking approaches (called prompts) during
inference. VampNet is non-autoregressive, leveraging a bidirectional
transformer architecture that attends to all tokens in a forward pass. With
just 36 sampling passes, VampNet can generate coherent high-fidelity musical
waveforms. We show that by prompting VampNet in various ways, we can apply it
to tasks like music compression, inpainting, outpainting, continuation, and
looping with variation (vamping). Appropriately prompted, VampNet is capable of
maintaining style, genre, instrumentation, and other high-level aspects of the
music. This flexible prompting capability makes VampNet a powerful music
co-creation tool. Code and audio samples are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023
  Speech-to-Speech Translation Task <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Song, Yi lei, Peikun Chen, Yiqing Cao, Kun Wei, Yongmao Zhang, Lei Xie, Ning Jiang, Guoqing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech
translation (S2ST) task which aims to translate from English speech of
multi-source to Chinese speech. The system is built in a cascaded manner
consisting of automatic speech recognition (ASR), machine translation (MT), and
text-to-speech (TTS). We make tremendous efforts to handle the challenging
multi-source input. Specifically, to improve the robustness to multi-source
speech input, we adopt various data augmentation strategies and a ROVER-based
score fusion on multiple ASR model outputs. To better handle the noisy ASR
transcripts, we introduce a three-stage fine-tuning strategy to improve
translation accuracy. Finally, we build a TTS model with high naturalness and
sound quality, which leverages a two-stage framework, using network bottleneck
features as a robust intermediate representation for speaker timbre and
linguistic content disentanglement. Based on the two-stage framework,
pre-trained speaker embedding is leveraged as a condition to transfer the
speaker timbre in the source English speech to the translated Chinese speech.
Experimental results show that our system has high translation accuracy, speech
naturalness, sound quality, and speaker similarity. Moreover, it shows good
robustness to multi-source data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT@ACL 2023 system paper. Our submitted system ranks 1st in the
  S2ST task of the IWSLT 2023 evaluation campaign</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EchoVest: Real-Time Sound Classification and Depth Perception Expressed
  through Transcutaneous Electrical Nerve Stimulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Choe, Siddhant Sood, Ryan Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over 1.5 billion people worldwide live with hearing impairment. Despite
various technologies that have been created for individuals with such
disabilities, most of these technologies are either extremely expensive or
inaccessible for everyday use in low-medium income countries. In order to
combat this issue, we have developed a new assistive device, EchoVest, for
blind/deaf people to intuitively become more aware of their environment.
EchoVest transmits vibrations to the user's body by utilizing transcutaneous
electric nerve stimulation (TENS) based on the source of the sounds. EchoVest
also provides various features, including sound localization, sound
classification, noise reduction, and depth perception. We aimed to outperform
CNN-based machine-learning models, the most commonly used machine learning
model for classification tasks, in accuracy and computational costs. To do so,
we developed and employed a novel audio pipeline that adapts the Audio
Spectrogram Transformer (AST) model, an attention-based model, for our sound
classification purposes, and Fast Fourier Transforms for noise reduction. The
application of Otsu's Method helped us find the optimal thresholds for
background noise sound filtering and gave us much greater accuracy. In order to
calculate direction and depth accurately, we applied Complex Time Difference of
Arrival algorithms and SOTA localization. Our last improvement was to use blind
source separation to make our algorithms applicable to multiple microphone
inputs. The final algorithm achieved state-of-the-art results on numerous
checkpoints, including a 95.7\% accuracy on the ESC-50 dataset for
environmental sound classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Timbre transfer using image-to-image denoising diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Comanducci, Fabio Antonacci, Augusto Sarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timbre transfer techniques aim at converting the sound of a musical piece
generated by one instrument into the same one as if it was played by another
instrument, while maintaining as much as possible the content in terms of
musical characteristics such as melody and dynamics. Following their recent
breakthroughs in deep learning-based generation, we apply Denoising Diffusion
Models (DDMs) to perform timbre transfer. Specifically, we apply the recently
proposed Denoising Diffusion Implicit Models (DDIMs) that enable to accelerate
the sampling procedure. Inspired by the recent application of DDMs to image
translation problems we formulate the timbre transfer task similarly, by first
converting the audio tracks into log mel spectrograms and by conditioning the
generation of the desired timbre spectrogram through the input timbre
spectrogram. We perform both one-to-one and many-to-many timbre transfer, by
converting audio waveforms containing only single instruments and multiple
instruments, respectively. We compare the proposed technique with existing
state-of-the-art methods both through listening tests and objective measures in
order to demonstrate the effectiveness of the proposed model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Study on the Correlation between Objective Evaluations and Subjective
  Speech Quality and Intelligibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsin-Tien Chiang, Kuo-Hsuan Hung, Szu-Wei Fu, Heng-Cheng Kuo, Ming-Hsueh Tsai, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subjective tests are the gold standard for evaluating speech quality and
intelligibility, but they are time-consuming and expensive. Thus, objective
measures that align with human perceptions are crucial. This study evaluates
the correlation between commonly used objective measures and subjective speech
quality and intelligibility using a Chinese speech dataset. Moreover, new
objective measures are proposed combining current objective measures using deep
learning techniques to predict subjective quality and intelligibility. The
proposed deep learning model reduces the amount of training data without
significantly impacting prediction performance. We interpret the deep learning
model to understand how objective measures reflect subjective quality and
intelligibility. We also explore the impact of including subjective speech
quality ratings on speech intelligibility prediction. Our findings offer
valuable insights into the relationship between objective measures and human
perceptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting an External Microphone for Binaural RTF-Vector-Based
  Direction of Arrival Estimation for Multiple Speakers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fejgin, Simon Doclo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In hearing aid applications, an important objective is to accurately estimate
the direction of arrival (DOA) of multiple speakers in noisy and reverberant
environments. Recently, we proposed a binaural DOA estimation method, where the
DOAs of the speakers are estimated by selecting the directions for which the
so-called Hermitian angle spectrum between the estimated relative transfer
function (RTF) vector and a database of prototype anechoic RTF vectors is
maximized. The RTF vector is estimated using the covariance whitening (CW)
method, which requires a computationally complex generalized eigenvalue
decomposition. The spatial spectrum is obtained by only considering frequencies
where it is likely that one speaker dominates over the other speakers, noise
and reverberation. In this contribution, we exploit the availability of an
external microphone that is spatially separated from the hearing aid
microphones and consider a low-complexity RTF vector estimation method that
assumes a low spatial coherence between the undesired components in the
external microphone and the hearing aid microphones. Using recordings of two
speakers and diffuse-like babble noise in acoustic environments with mild
reverberation and low signal-to-noise ratio, simulation results show that the
proposed method yields a comparable DOA estimation performance as the CW method
at a lower computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for Forum Acusticum 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HCLAS-X: Hierarchical and Cascaded Lyrics Alignment System Using
  Multimodal Cross-Correlation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsung Kang, Soochul Park, Keunwoo Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the challenge of lyrics alignment, which involves
aligning the lyrics and vocal components of songs. This problem requires the
alignment of two distinct modalities, namely text and audio. To overcome this
challenge, we propose a model that is trained in a supervised manner, utilizing
the cross-correlation matrix of latent representations between vocals and
lyrics. Our system is designed in a hierarchical and cascaded manner. It
predicts synced time first on a sentence-level and subsequently on a
word-level. This design enables the system to process long sequences, as the
cross-correlation uses quadratic memory with respect to sequence length. In our
experiments, we demonstrate that our proposed system achieves a significant
improvement in mean average error, showcasing its robustness in comparison to
the previous state-of-the-art model. Additionally, we conduct a qualitative
analysis of the system after successfully deploying it in several music
streaming services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Piano Transcription with Hierarchical Frequency-Time
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keisuke Toyama, Taketo Akama, Yukara Ikemiya, Yuhta Takida, Wei-Hsiang Liao, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Taking long-term spectral and temporal dependencies into account is essential
for automatic piano transcription. This is especially helpful when determining
the precise onset and offset for each note in the polyphonic piano content. In
this case, we may rely on the capability of self-attention mechanism in
Transformers to capture these long-term dependencies in the frequency and time
axes. In this work, we propose hFT-Transformer, which is an automatic music
transcription method that uses a two-level hierarchical frequency-time
Transformer architecture. The first hierarchy includes a convolutional block in
the time axis, a Transformer encoder in the frequency axis, and a Transformer
decoder that converts the dimension in the frequency axis. The output is then
fed into the second hierarchy which consists of another Transformer encoder in
the time axis. We evaluated our method with the widely used MAPS and MAESTRO
v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the
F1-scores of the metrics among Frame, Note, Note with Offset, and Note with
Offset and Velocity estimations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, to be published in ISMIR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge Storage Management Recipe with Zero-Shot Data Compression for Road
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Uju Gim, Myung Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show edge computing-based road anomaly detection systems which
may also conduct data collection simultaneously. However, the edge computers
will have small data storage but we need to store the collected audio samples
for a long time in order to update existing models or develop a novel method.
Therefore, we should consider an approach for efficient storage management
methods while preserving high-fidelity audio. A hardware-perspective approach,
such as using a low-resolution microphone, is an intuitive way to reduce file
size but is not recommended because it fundamentally cuts off high-frequency
components. On the other hand, a computational file compression approach that
encodes collected high-resolution audio into a compact code should be
recommended because it also provides a corresponding decoding method. Motivated
by this, we propose a way of simple yet effective pre-trained autoencoder-based
data compression method. The pre-trained autoencoder is trained for the purpose
of audio super-resolution so it can be utilized to encode or decode any
arbitrary sampling rate. Moreover, it will reduce the communication cost for
data transmission from the edge to the central server. Via the comparative
experiments, we confirm that the zero-shot audio compression and decompression
highly preserve anomaly detection performance while enhancing storage and
transmission efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Demand-Driven Perspective on Generative Audio AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangshin Oh, Minsung Kang, Hyeongi Moon, Keunwoo Choi, Ben Sangbae Chon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve successful deployment of AI research, it is crucial to understand
the demands of the industry. In this paper, we present the results of a survey
conducted with professional audio engineers, in order to determine research
priorities and define various research tasks. We also summarize the current
challenges in audio quality and controllability based on the survey. Our
analysis emphasizes that the availability of datasets is currently the main
bottleneck for achieving high-quality audio generation. Finally, we suggest
potential solutions for some revealed issues with empirical evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Tuberculosis from Real-World Cough Audio Recordings and
  Metadata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George P. Kafentzis, Stephane Tetsing, Joe Brew, Lola Jover, Mindaugas Galvosas, Carlos Chaccour, Peter M. Small
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tuberculosis (TB) is an infectious disease caused by the bacterium
Mycobacterium tuberculosis and primarily affects the lungs, as well as other
body parts. TB is spread through the air when an infected person coughs,
sneezes, or talks. Medical doctors diagnose TB in patients via clinical
examinations and specialized tests. However, coughing is a common symptom of
respiratory diseases such as TB. Literature suggests that cough sounds coming
from different respiratory diseases can be distinguished by both medical
doctors and computer algorithms. Therefore, cough recordings associated with
patients with and without TB seems to be a reasonable avenue of investigation.
In this work, we utilize a very large dataset of TB and non-TB cough audio
recordings obtained from the south-east of Africa, India, and the south-east of
Asia using a fully automated phone-based application (Hyfe), without manual
annotation. We fit statistical classifiers based on spectral and time domain
features with and without clinical metadata. A stratified grouped
cross-validation approach shows that an average Area Under Curve (AUC) of
approximately 0.70 $\pm$ 0.05 both for a cough-level and a participant-level
classification can be achieved using cough sounds alone. The addition of
demographic and clinical factors increases performance, resulting in an average
AUC of approximately 0.81 $\pm$ 0.05. Our results suggest mobile phone-based
applications that integrate clinical symptoms and cough sound analysis could
help community health workers and, most importantly, health service programs to
improve TB case-finding efforts while reducing costs, which could substantially
improve public health.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Song <span class="highlight-title">Dataset</span> (CoSoD): An annotated <span class="highlight-title">dataset</span> of multi-artist
  collaborations in popular music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michèle Duguay, Kate Mancey, Johanna Devaney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist
collaborations from the 2010-2019 Billboard "Hot 100" year-end charts. The
corpus is annotated with formal sections, aspects of vocal production
(including reverberation, layering, panning, and gender of the performers), and
relevant metadata. CoSoD complements other popular music datasets by focusing
exclusively on musical collaborations between independent acts. In addition to
facilitating the study of song form and vocal production, CoSoD allows for the
in-depth study of gender as it relates to various timbral, pitch, and formal
parameters in musical collaborations. In this paper, we detail the contents of
the dataset and outline the annotation process. We also present an experiment
using CoSoD that examines how the use of reverberation, layering, and panning
are related to the gender of the artist. In this experiment, we find that men's
voices are on average treated with less reverberation and occupy a more narrow
position in the stereo mix than women's voices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 24th International Society
  for Music Information Retrieval Conference (ISMIR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study of <span class="highlight-title">Self-Supervised</span> Speech Representations in Read
  and Spontaneous TTS <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has explored using self-supervised learning (SSL) speech
representations such as wav2vec2.0 as the representation medium in standard
two-stage TTS, in place of conventionally used mel-spectrograms. It is however
unclear which speech SSL is the better fit for TTS, and whether or not the
performance differs between read and spontaneous TTS, the later of which is
arguably more challenging. This study aims at addressing these questions by
testing several speech SSLs, including different layers of the same SSL, in
two-stage TTS on both read and spontaneous corpora, while maintaining constant
TTS model architecture and training settings. Results from listening tests show
that the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other
tested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our work
sheds light on both how speech SSL can readily improve current TTS systems, and
how SSLs compare in the challenging generative task of TTS. Audio examples can
be found at https://www.speech.kth.se/tts-demos/ssr_tts
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures. ICASSP Workshop SASB (Self-Supervision in Audio,
  Speech and Beyond)2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-TTSG: Denoising probabilistic integrated speech and gesture
  synthesis <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With read-aloud speech synthesis achieving high naturalness scores, there is
a growing research interest in synthesising spontaneous speech. However, human
spontaneous face-to-face conversation has both spoken and non-verbal aspects
(here, co-speech gestures). Only recently has research begun to explore the
benefits of jointly synthesising these two modalities in a single system. The
previous state of the art used non-probabilistic methods, which fail to capture
the variability of human speech and motion, and risk producing oversmoothing
artefacts and sub-optimal synthesis quality. We present the first
diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to
synthesise speech and gestures together. Our method can be trained on small
datasets from scratch. Furthermore, we describe a set of careful uni- and
multi-modal subjective tests for evaluating integrated speech and gesture
synthesis systems, and use them to validate our proposed approach. For
synthesised examples please see https://shivammehta25.github.io/Diff-TTSG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, Accepted at ISCA Speech Synthesis Workshop (SSW)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mathematical Foundations of Complex Tonality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04974v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04974v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey R. Boland, Lane P. Hughston
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equal temperament, in which semitones are tuned in the irrational ratio of
$2^{1/12} : 1$, is best seen as a serviceable compromise, sacrificing purity
for flexibility. Just intonation, in which intervals are given by products of
powers of $2$, $3$, and $5$, is more natural, but of limited flexibility. We
propose a new scheme in which ratios of Gaussian integers form the basis of an
abstract tonal system. The tritone, so problematic in just temperament, given
ambiguously by the ratios $\tfrac{45}{32}$, $\tfrac{64}{45}$, $\tfrac{36}{25}$,
$\tfrac{25}{18}$, none satisfactory, is in our scheme represented by the
complex ratio $1 + \rm{i} : 1$. The major and minor whole tones, given by
intervals of $\tfrac{9}{8}$ and $\tfrac{10}{9}$, can each be factorized into
products of complex semitones, giving us a major complex semitone
$\tfrac{3}{4}(1 + \rm{i})$ and a minor complex semitone $\tfrac{1}{3}(3 +
\rm{i})$. The perfect third, given by the interval $\tfrac{5}{4}$, factorizes
into the product of a complex whole tone $\tfrac{1}{2}(1 + 2\rm{i})$ and its
complex conjugate. Augmented with these supplementary tones, the resulting
scheme of complex intervals based on products of low powers of Gaussian primes
leads to the construction of a complete system of major and minor scales in all
keys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, to appear in Journal of Mathematics and Music</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion-Conditioned Melody Harmonization with Hierarchical Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03718v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03718v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing melody harmonization models have made great progress in improving
the quality of generated harmonies, but most of them ignored the emotions
beneath the music. Meanwhile, the variability of harmonies generated by
previous methods is insufficient. To solve these problems, we propose a novel
LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the
influence of emotional conditions on melody harmonization, while improving the
quality of generated harmonies and capturing the abundant variability of chord
progressions. Specifically, LHVAE incorporates latent variables and emotional
conditions at different levels (piece- and bar-level) to model the global and
local music properties. Additionally, we introduce an attention-based melody
context vector at each step to better learn the correspondence between melodies
and harmonies. Objective experimental results show that our proposed model
outperforms other LSTM-based models. Through subjective evaluation, we conclude
that only altering the type of chord hardly changes the overall emotion of the
music. The qualitative analysis demonstrates the ability of our model to
generate variable harmonies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SMC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What do End-to-End Speech Models Learn about Speaker, Language and
  Channel Information? A Layer-wise and Neuron-level Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.00439v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.00439v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shammur Absar Chowdhury, Nadir Durrani, Ahmed Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are inherently opaque and challenging to interpret.
Unlike hand-crafted feature-based models, we struggle to comprehend the
concepts learned and how they interact within these models. This understanding
is crucial not only for debugging purposes but also for ensuring fairness in
ethical decision-making. In our study, we conduct a post-hoc functional
interpretability analysis of pretrained speech models using the probing
framework [1]. Specifically, we analyze utterance-level representations of
speech models trained for various tasks such as speaker recognition and dialect
identification. We conduct layer and neuron-wise analyses, probing for speaker,
language, and channel properties. Our study aims to answer the following
questions: i) what information is captured within the representations? ii) how
is it represented and distributed? and iii) can we identify a minimal subset of
the network that possesses this information?
  Our results reveal several novel findings, including: i) channel and gender
information are distributed across the network, ii) the information is
redundantly available in neurons with respect to a task, iii) complex
properties such as dialectal information are encoded only in the task-oriented
pretrained network, iv) and is localised in the upper layers, v) we can extract
a minimal subset of neurons encoding the pre-defined property, vi) salient
neurons are sometimes shared between properties, vii) our analysis highlights
the presence of biases (for example gender) in the network. Our
cross-architectural comparison indicates that: i) the pretrained models capture
speaker-invariant information, and ii) CNN models are competitive with
Transformer models in encoding various understudied properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CSL journal. Keywords: Speech, Neuron Analysis,
  Interpretibility, Diagnostic Classifier, AI explainability, End-to-End
  Architecture</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deceptive Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajani Vithana, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the problem of deceptive information retrieval (DIR), in which a
user wishes to download a required file out of multiple independent files
stored in a system of databases while \emph{deceiving} the databases by making
the databases' predictions on the user-required file index incorrect with high
probability. Conceptually, DIR is an extension of private information retrieval
(PIR). In PIR, a user downloads a required file without revealing its index to
any of the databases. The metric of deception is defined as the probability of
error of databases' prediction on the user-required file, minus the
corresponding probability of error in PIR. The problem is defined on
time-sensitive data that keeps updating from time to time. In the proposed
scheme, the user deceives the databases by sending \emph{real} queries to
download the required file at the time of the requirement and \emph{dummy}
queries at multiple distinct future time instances to manipulate the
probabilities of sending each query for each file requirement, using which the
databases' make the predictions on the user-required file index. The proposed
DIR scheme is based on a capacity achieving probabilistic PIR scheme, and
achieves rates lower than the PIR capacity due to the additional downloads made
to deceive the databases. When the required level of deception is zero, the
proposed scheme achieves the PIR capacity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spoofing-Resilient LiDAR-GPS Factor Graph Localization with Chimera
  Authentication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Dai, Tara Minda, Ashwin Kanhere, Grace Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many vehicle platforms typically use sensors such as LiDAR or camera for
locally-referenced navigation with GPS for globally-referenced navigation.
However, due to the unencrypted nature of GPS signals, all civilian users are
vulner-able to spoofing attacks, where a malicious spoofer broadcasts
fabricated signals and causes the user to track a false position fix. To
protect against such GPS spoofing attacks, Chips-Message Robust Authentication
(Chimera) has been developed and will be tested on the Navigation Technology
Satellite 3 (NTS-3) satellite being launched later this year. However, Chimera
authentication is not continuously available and may not provide sufficient
protection for vehicles which rely on more frequent GPS measurements. In this
paper, we propose a factor graph-based state estimation framework which
integrates LiDAR and GPS while simultaneously detecting and mitigating spoofing
attacks experienced between consecutive Chimera authentications. Our proposed
framework combines GPS pseudorange measurements with LiDAR odometry to provide
a robust navigation solution. A chi-squared detector, based on pseudorange
residuals, is used to detect and mitigate any potential GPS spoofing attacks.
We evaluate our method using real-world LiDAR data from the KITTI dataset and
simulated GPS measurements, both nominal and with spoofing. Across multiple
trajectories and Monte Carlo runs, our method consistently achieves position
errors under 5 m during nominal conditions, and successfully bounds positioning
error to within odometry drift levels during spoofed conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Trustworthiness Model for DNN in Dedicated 6G Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anouar Nechi, Ahmed Mahmoudi, Christoph Herold, Daniel Widmer, Thomas Kürner, Mladen Berekovic, Saleh Mulhem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) is considered an efficient response to several
challenges facing 6G technology. However, AI still suffers from a huge trust
issue due to its ambiguous way of making predictions. Therefore, there is a
need for a method to evaluate the AI's trustworthiness in practice for future
6G applications. This paper presents a practical model to analyze the
trustworthiness of AI in a dedicated 6G application. In particular, we present
two customized Deep Neural Networks (DNNs) to solve the Automatic Modulation
Recognition (AMR) problem in Terahertz communications-based 6G technology.
Then, a specific trustworthiness model and its attributes, namely data
robustness, parameter sensitivity, and security covering adversarial examples,
are introduced. The evaluation results indicate that the proposed
trustworthiness attributes are crucial to evaluate the trustworthiness of DNN
for this 6G application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PREPRINT - accepted In Proceedings of the 19th International
  Conference on Wireless and Mobile Computing, Networking and Communications
  2023(STWiMob 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An End-To-End Analysis of Deep Learning-Based Remaining Useful Life
  Algorithms for Satefy-Critical 5G-Enabled IIoT Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Mario Amorosa, Nicolò Longhi, Giampaolo Cuozzo, Weronika Maria Bachan, Valerio Lieti, Enrico Buracchini, Roberto Verdone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remaining Useful Life (RUL) prediction is a critical task that aims to
estimate the amount of time until a system fails, where the latter is formed by
three main components, that is, the application, communication network, and RUL
logic. In this paper, we provide an end-to-end analysis of an entire RUL-based
chain. Specifically, we consider a factory floor where Automated Guided
Vehicles (AGVs) transport dangerous liquids whose fall may cause injuries to
workers. Regarding the communication infrastructure, the AGVs are equipped with
5G User Equipments (UEs) that collect real-time data of their movements and
send them to an application server. The RUL logic consists of a Deep Learning
(DL)-based pipeline that assesses if there will be liquid falls by analyzing
the collected data, and, eventually, sending commands to the AGVs to avoid such
a danger. According to this scenario, we performed End-to-End 5G NR-compliant
network simulations to study the Round-Trip Time (RTT) as a function of the
overall system bandwidth, subcarrier spacing, and modulation order. Then, via
real-world experiments, we collect data to train, test and compare 7 DL models
and 1 baseline threshold-based algorithm in terms of cost and average advance.
Finally, we assess whether or not the RTT provided by four different 5G NR
network architectures is compatible with the average advance provided by the
best-performing one-Dimensional Convolutional Neural Network (1D-CNN).
Numerical results show under which conditions the DL-based approach for RUL
estimation matches with the RTT performance provided by different 5G NR network
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures. This work has been accepted for publication at
  IEEE PIMRC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Memristor-Inspired Computation for Epileptiform Signals in Spheroids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iván Díez de los Ríos, John Wesley Ephraim, Gemma Palazzolo, Teresa Serrano-Gotarredona, Gabriella Panuccio, Bernabé Linares-Barranco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a memristor-inspired computational method for
obtaining a type of running spectrogram or fingerprint of epileptiform activity
generated by rodent hippocampal spheroids. It can be used to compute on the fly
and with low computational cost an alert-level signal for epileptiform events
onset. Here, we describe the computational method behind this fingerprint
technique and illustrate it using epileptiform events recorded from hippocampal
spheroids using a microelectrode array system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in 2023 IEEE 5th International Conference on Artificial
  Intelligence Circuits and Systems (AICAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EchoVest: Real-Time Sound Classification and Depth Perception Expressed
  through Transcutaneous Electrical Nerve Stimulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Choe, Siddhant Sood, Ryan Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over 1.5 billion people worldwide live with hearing impairment. Despite
various technologies that have been created for individuals with such
disabilities, most of these technologies are either extremely expensive or
inaccessible for everyday use in low-medium income countries. In order to
combat this issue, we have developed a new assistive device, EchoVest, for
blind/deaf people to intuitively become more aware of their environment.
EchoVest transmits vibrations to the user's body by utilizing transcutaneous
electric nerve stimulation (TENS) based on the source of the sounds. EchoVest
also provides various features, including sound localization, sound
classification, noise reduction, and depth perception. We aimed to outperform
CNN-based machine-learning models, the most commonly used machine learning
model for classification tasks, in accuracy and computational costs. To do so,
we developed and employed a novel audio pipeline that adapts the Audio
Spectrogram Transformer (AST) model, an attention-based model, for our sound
classification purposes, and Fast Fourier Transforms for noise reduction. The
application of Otsu's Method helped us find the optimal thresholds for
background noise sound filtering and gave us much greater accuracy. In order to
calculate direction and depth accurately, we applied Complex Time Difference of
Arrival algorithms and SOTA localization. Our last improvement was to use blind
source separation to make our algorithms applicable to multiple microphone
inputs. The final algorithm achieved state-of-the-art results on numerous
checkpoints, including a 95.7\% accuracy on the ESC-50 dataset for
environmental sound classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing the Power of Swarm Satellite Networks with Wideband
  Distributed Beamforming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carlos Merlano Duncan, Vu Nguyen Ha, Jevgenij Krivochiza, Rakesh Palisetty, Geoffrey Eappen, Juan Andres Vasquez, Wallace Alves Martins, Symeon Chatzinotas, Björn Ottersten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The space communications industry is challenged to develop a technology that
can deliver broadband services to user terminals equipped with miniature
antennas, such as handheld devices. One potential solution to establish links
with ground users is the deployment of massive antennas in one single
spacecraft. However, this is not cost-effective. Aligning with recent
\emph{NewSpace} activities directed toward miniaturization, mass production,
and a significant reduction in spacecraft launch costs, an alternative could be
distributed beamforming from multiple satellites. In this context, we propose a
distributed beamforming modeling technique for wideband signals. We also
consider the statistical behavior of the relative geometry of the swarm nodes.
The paper assesses the proposed technique via computer simulations, providing
interesting results on the beamforming gains in terms of power and the security
of the communication against potential eavesdroppers at non-intended pointing
angles. This approach paves the way for further exploration of wideband
distributed beamforming from satellite swarms in several future communication
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PIMRC'23 WS MCSN-6G (accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thin-Film Lithium Niobate Acoustic Filter at 23.5 GHz with 2.38 dB IL
  and 18.2% FBW 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Barrera, Sinwoo Cho, Lezli Matto, Jack Kramer, Kenny Huynh, Vakhtang Chulukhadze, Yen-Wei Chang, Mark S. Goorsky, Ruochen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work reports an acoustic filter at 23.5 GHz with a low insertion loss
(IL) of 2.38 dB and a 3-dB fractional bandwidth (FBW) of 18.2%, significantly
surpassing the state-of-the-art. The device leverages electrically coupled
acoustic resonators in 100 nm 128{\deg} Y-cut lithium niobate (LiNbO3)
piezoelectric thin film, operating in the first-order antisymmetric (A1) mode.
A new film stack, namely transferred thin-film LiNbO3 on silicon (Si) substrate
with an intermediate amorphous silicon (a-Si) layer, facilitates the
record-breaking performance at millimeter-wave (mmWave). The filter features a
compact footprint of 0.56 mm2. In this letter, acoustic and EM consideration,
along with material characterization with X-ray diffraction and verified with
cross-sectional electron microscopy are reported. Upon further development, the
reported filter platform can enable various front-end signal-processing
functions at mmWave.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 8 figures, submitted to IEEE JMEMS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Decisions on Optimal Load Balancing in Loss Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Liu, Chehao Wang, Ce Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When multiple users share a common link in direct transmission, packet loss
and network collision may occur due to the simultaneous arrival of traffics at
the source node. To tackle this problem, users may resort to an indirect path:
the packet flows are first relayed through a sidelink to another source node,
then transmitted to the destination. This behavior brings the problems of
packet routing or load balancing: (1) how to maximize the total traffic in a
collaborative way; (2) how self-interested users choose routing strategies to
minimize their individual packet loss independently. In this work, we propose a
generalized mathematical framework to tackle the packet and load balancing
issue in loss networks. In centralized scenarios with a planner, we provide a
polynomial-time algorithm to compute the system optimum point where the total
traffic rate is maximized. Conversely, in decentralized settings with
autonomous users making distributed decisions, the system converges to an
equilibrium where no user can reduce their loss probability through unilateral
deviation. We thereby provide a full characterization of Nash equilibrium and
examine the efficiency loss stemming from selfish behaviors, both theoretically
and empirically. In general, the performance degradation caused by selfish
behaviors is not catastrophic; however, this gap is not monotonic and can have
extreme values in certain specific scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, WiOPT workshop RAWNET</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting an External Microphone for Binaural RTF-Vector-Based
  Direction of Arrival Estimation for Multiple Speakers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fejgin, Simon Doclo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In hearing aid applications, an important objective is to accurately estimate
the direction of arrival (DOA) of multiple speakers in noisy and reverberant
environments. Recently, we proposed a binaural DOA estimation method, where the
DOAs of the speakers are estimated by selecting the directions for which the
so-called Hermitian angle spectrum between the estimated relative transfer
function (RTF) vector and a database of prototype anechoic RTF vectors is
maximized. The RTF vector is estimated using the covariance whitening (CW)
method, which requires a computationally complex generalized eigenvalue
decomposition. The spatial spectrum is obtained by only considering frequencies
where it is likely that one speaker dominates over the other speakers, noise
and reverberation. In this contribution, we exploit the availability of an
external microphone that is spatially separated from the hearing aid
microphones and consider a low-complexity RTF vector estimation method that
assumes a low spatial coherence between the undesired components in the
external microphone and the hearing aid microphones. Using recordings of two
speakers and diffuse-like babble noise in acoustic environments with mild
reverberation and low signal-to-noise ratio, simulation results show that the
proposed method yields a comparable DOA estimation performance as the CW method
at a lower computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for Forum Acusticum 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Behavioral Representations of Routines From Large-scale
  Unlabeled Wearable Time-series Data Streams using Hawkes Point Process <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian Feng, Brandon M Booth, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuously-worn wearable sensors enable researchers to collect copious
amounts of rich bio-behavioral time series recordings of real-life activities
of daily living, offering unprecedented opportunities to infer novel human
behavior patterns during daily routines. Existing approaches to routine
discovery through bio-behavioral data rely either on pre-defined notions of
activities or use additional non-behavioral measurements as contexts, such as
GPS location or localization within the home, presenting risks to user privacy.
In this work, we propose a novel wearable time-series mining framework, Hawkes
point process On Time series clusters for ROutine Discovery (HOT-ROD), for
uncovering behavioral routines from completely unlabeled wearable recordings.
We utilize a covariance-based method to generate time-series clusters and
discover routines via the Hawkes point process learning algorithm. We
empirically validate our approach for extracting routine behaviors using a
completely unlabeled time-series collected continuously from over 100
individuals both in and outside of the workplace during a period of ten weeks.
Furthermore, we demonstrate this approach intuitively captures daily
transitional relationships between physical activity states without using prior
knowledge. We also show that the learned behavioral patterns can assist in
illuminating an individual's personality and affect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 9th ACM SIGKDD International Workshop on Mining and Learning
  From Time Series (MiLeTS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-Frequency-Space Transmit Design and Signal Processing with Dynamic
  Subarray for Terahertz Integrated Sensing and Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongzhi Wu, Chong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terahertz (THz) integrated sensing and communication (ISAC) enables
simultaneous data transmission with Terabit-per-second (Tbps) rate and
millimeter-level accurate sensing. To realize such a blueprint, ultra-massive
antenna arrays with directional beamforming are used to compensate for severe
path loss in the THz band. In this paper, the time-frequency-space transmit
design is investigated for THz ISAC to generate time-varying scanning sensing
beams and stable communication beams. Specifically, with the dynamic
array-of-subarray (DAoSA) hybrid beamforming architecture and multi-carrier
modulation, two ISAC hybrid precoding algorithms are proposed, namely, a
vectorization (VEC) based algorithm that outperforms existing ISAC hybrid
precoding methods and a low-complexity sensing codebook assisted (SCA)
approach. Meanwhile, coupled with the transmit design, parameter estimation
algorithms are proposed to realize high-accuracy sensing, including a wideband
DAoSA MUSIC (W-DAoSA-MUSIC) method for angle estimation and a sum-DFT-GSS
(S-DFT-GSS) approach for range and velocity estimation. Numerical results
indicate that the proposed algorithms can realize centi-degree-level angle
estimation accuracy and millimeter-level range estimation accuracy, which are
one or two orders of magnitudes better than the methods in the millimeter-wave
band. In addition, to overcome the cyclic prefix limitation and Doppler effects
in the THz band, an inter-symbol interference- and inter-carrier
interference-tackled sensing algorithm is developed to refine sensing
capabilities for THz ISAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconfigurable Intelligent Surface Assisted Railway Communications: A
  <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aline Habib, Ammar El Falou, Charlotte Langlais, Marion Berbineau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of train passengers and the demand for high data rates to handle
new technologies such as video streaming and IoT technologies are continuously
increasing. Therefore the exploration of millimeter waves (mmWave) band is a
key technology to meet this demand. However, the high penetration loss makes
mmWave very sensitive to blocking, limiting its coverage area. One promising,
efficient, and low-cost solution is the reconfigurable intelligent surface
(RIS). This paper reviews the state of the art of RIS for railway
communications in the mmWave context. First, we present the different types of
RIS and review some optimization algorithms used in the literature to find the
RIS phase shift. Then, we review recent works on RIS in the railway domain and
provide future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Enabling Cardiac Digital Twins of Myocardial Infarction Using
  Deep Computational Models for Inverse Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Julia Camps, Zhinuo Wang, Abhirup Banerjee, Blanca Rodriguez, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Myocardial infarction (MI) demands precise and swift diagnosis. Cardiac
digital twins (CDTs) have the potential to offer individualized evaluation of
cardiac function in a non-invasive manner, making them a promising approach for
personalized diagnosis and treatment planning of MI. The inference of accurate
myocardial tissue properties is crucial in creating a reliable CDT platform,
and particularly in the context of studying MI. In this work, we investigate
the feasibility of inferring myocardial tissue properties from the
electrocardiogram (ECG), focusing on the development of a comprehensive CDT
platform specifically designed for MI. The platform integrates multi-modal
data, such as cardiac MRI and ECG, to enhance the accuracy and reliability of
the inferred tissue properties. We perform a sensitivity analysis based on
computer simulations, systematically exploring the effects of infarct location,
size, degree of transmurality, and electrical activity alteration on the
simulated QRS complex of ECG, to establish the limits of the approach. We
subsequently propose a deep computational model to infer infarct location and
distribution from the simulated QRS. The in silico experimental results show
that our model can effectively capture the complex relationships between the
QRS signals and the corresponding infarct regions, with promising potential for
clinical application in the future. The code will be released publicly once the
manuscript is accepted for publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Cardiac digital twins; Inverse inference; Myocardial infarction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vehicle Detection in 6G Systems with OTFS Modulation <span class="chip">KR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Karpovich, Tomasz P. Zielinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently introduced orthogonal time frequency space modulation (OTFSM) is
more robust to large narrow-band Doppler frequency shift than the orthogonal
frequency division multiplexing (OFDM), used in the 5G standard. In this paper
it is shown how the elecommunication OTFSM-based signal with random padding can
be used with success in the 6G standard for detection of high-speed vehicles.
Two approaches for detecting targets during the random padded OTFS based
transmission are compared in the paper
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Konferencja Radiokomunikacji i Teleinformatyki
  KRiT-2023, Krakow, Poland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Communications and Sensing Hybrid Beamforming Design via Deep
  Unfolding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhan Thanh Nguyen, Ly V. Nguyen, Nir Shlezinger, Yonina C. Eldar, A. Lee Swindlehurst, Markku Juntti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint communications and sensing (JCAS) is envisioned as a key feature in
future wireless communications networks. In massive MIMO-JCAS systems, hybrid
beamforming (HBF) is typically employed to achieve satisfactory beamforming
gains with reasonable hardware cost and power consumption. Due to the coupling
of the analog and digital precoders in HBF and the dual objective in JCAS,
JCAS-HBF design problems are very challenging and usually require highly
complex algorithms. In this paper, we propose a fast HBF design for JCAS based
on deep unfolding to optimize a tradeoff between the communications rate and
sensing accuracy. We first derive closed-form expressions for the gradients of
the communications and sensing objectives with respect to the precoders and
demonstrate that the magnitudes of the gradients pertaining to the analog
precoder are typically smaller than those associated with the digital precoder.
Based on this observation, we propose a modified projected gradient ascent
(PGA) method with significantly improved convergence. We then develop a deep
unfolded PGA scheme that efficiently optimizes the communications-sensing
performance tradeoff with fast convergence thanks to the well-trained
hyperparameters. In doing so, we preserve the interpretability and flexibility
of the optimizer while leveraging data to improve performance. Finally, our
simulations demonstrate the potential of the proposed deep unfolded method,
which achieves up to 33.5% higher communications sum rate and 2.5 dB lower
beampattern error compared with the conventional design based on successive
convex approximation and Riemannian manifold optimization. Furthermore, it
attains up to a 65% reduction in run time and computational complexity with
respect to the PGA procedure without unfolding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to Journal of Selected Topics in Signal
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automotive Radar Mutual Interference Mitigation Based on Hough Transform
  in Time-Frequency Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbing Li, Weichuan Zhang, Lianying Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of autonomous driving technology, automotive radar has
received unprecedented attention due to its day-and-night and all-weather
working capability. It is worthwhile to note that more and more vehicles are
equipped with automotive radars, resulting in mutual interference between
radars. The interference reduces radar target detection performance, making
perception information unreliable. In this paper, a novel interference
mitigation method based on power-weighted Hough transform is proposed for
solving the radar mutual interference and improving the safety of autonomous
driving systems. Firstly, the frequency modulation characteristics of
interference signals and target echo signals are analyzed, and differences
between the two signals are introduced. Secondly, based on the straight line
detection technique, the power of the mutual interference signal in
time-frequency domain is accumulated, and the accurate position of the
interference is located. Finally, the target echo is recovered by
autoregressive model. Compared with existing state-of-the-art methods, the
proposed method has the ability to retain more useful signals after the
interference mitigation, and achieve better interference detection robustness
under low signal-to-noise ratio conditions. Simulation experiments and real
scenario experiments verify the effectiveness of the proposed method and show
its superiority.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlimited Sampling of Bandpass Signals: Computational Demodulation via
  Undersampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gal Shtendel, Dorian Florescu, Ayush Bhandari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bandpass signals are an important sub-class of bandlimited signals that
naturally arise in a number of application areas but their high-frequency
content poses an acquisition challenge. Consequently, "Bandpass Sampling
Theory" has been investigated and applied in the literature. In this paper, we
consider the problem of modulo sampling of bandpass signals with the main goal
of sampling and recovery of high dynamic range inputs. Our work is inspired by
the Unlimited Sensing Framework (USF). In the USF, the modulo operation folds
high dynamic range inputs into low dynamic range, modulo samples. This
fundamentally avoids signal clipping. Given that the output of the modulo
nonlinearity is non-bandlimited, bandpass sampling conditions never hold true.
Yet, we show that bandpass signals can be recovered from a modulo
representation despite the inevitable aliasing. Our main contribution includes
proof of sampling theorems for recovery of bandpass signals from an
undersampled representation, reaching sub-Nyquist sampling rates. On the
recovery front, by considering both time-and frequency-domain perspectives, we
provide a holistic view of the modulo bandpass sampling problem. On the
hardware front, we include ideal, non-ideal and generalized modulo folding
architectures that arise in the hardware implementation of modulo
analog-to-digital converters. Numerical simulations corroborate our theoretical
results. Bridging the theory-practice gap, we validate our results using
hardware experiments, thus demonstrating the practical effectiveness of our
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 9 figures, revised manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast dynamic time warping and clustering in C++ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Volkan Kumtepeli, Rebecca Perriment, David A. Howey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for computationally efficient dynamic time warping
(DTW) and clustering of time-series data. The method frames the dynamic warping
of time series datasets as an optimisation problem solved using dynamic
programming, and then clusters time series data by solving a second
optimisation problem using mixed-integer programming (MIP). There is also an
option to use k-medoids clustering for increased speed, when a certificate for
global optimality is not essential. The improved efficiency of our approach is
due to task-level parallelisation of the clustering alongside DTW. Our approach
was tested using the UCR Time Series Archive, and was found to be, on average,
33% faster than the next fastest option when using the same clustering method.
This increases to 64% faster when considering only larger datasets (with more
than 1000 time series). The MIP clustering is most effective on small numbers
of longer time series, because the DTW computation is faster than other
approaches, but the clustering problem becomes increasingly computationally
expensive as the number of time series to be clustered increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Detection of Gait Events and Travel Distance Using Waist-worn
  Accelerometers Across a Typical Range of Walking and Running Speeds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albara Ah Ramli, Xin Liu, Kelly Berndt, Chen-Nee Chuah, Erica Goude, Lynea B. Kaethler, Amanda Lopez, Alina Nicorici, Corey Owens, David Rodriguez, Jane Wang, Daniel Aranki, Craig M. McDonald, Erik K. Henricson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Estimation of temporospatial clinical features of gait (CFs),
such as step count and length, step duration, step frequency, gait speed and
distance traveled is an important component of community-based mobility
evaluation using wearable accelerometers. However, challenges arising from
device complexity and availability, cost and analytical methodology have
limited widespread application of such tools. Research Question: Can
accelerometer data from commercially-available smartphones be used to extract
gait CFs across a broad range of attainable gait velocities in children with
Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using
machine learning (ML)-based methods Methods: Fifteen children with DMD and 15
TDs underwent supervised clinical testing across a range of gait speeds using
10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT)
and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer
at the waist near the body's center of mass. Gait CFs were extracted from the
accelerometer data using a multi-step machine learning-based process and
results were compared to ground-truth observation data. Results: Model
predictions vs. observed values for step counts, distance traveled, and step
length showed a strong correlation (Pearson's r = -0.9929 to 0.9986, p<0.0001).
The estimates demonstrated a mean (SD) percentage error of 1.49% (7.04%) for
step counts, 1.18% (9.91%) for distance traveled, and 0.37% (7.52%) for step
length compared to ground truth observations for the combined 6MWT, 100MRW, and
FW tasks. Significance: The study findings indicate that a single accelerometer
placed near the body's center of mass can accurately measure CFs across
different gait speeds in both TD and DMD peers, suggesting that there is
potential for accurately measuring CFs in the community with consumer-level
smartphones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the convergence of the distributed proximal point algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woocheol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we establish convergence results for the distributed proximal
point algorithm (DPPA) for distributed optimization problems. We consider the
problem on the whole domain Rd and find a general condition on the stepsize and
cost functions such that the DPPA is stable. We prove that the DPPA with
stepsize $\eta > 0$ exponentially converges to an $O(\eta)$-neighborhood of the
optimizer. Our result clearly explains the advantage of the DPPA with respect
to the convergence and stability in comparison with the distributed gradient
descent algorithm. We also provide numerical tests supporting the theoretical
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral and Energy Efficiency Maximization of MISO STAR-RIS-assisted
  URLLC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Soleymani, Ignacio Santamaria, Eduard Jorswieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a general optimization framework to improve the spectral
and energy efficiency (EE) of ultra-reliable low-latency communication (URLLC)
simultaneous-transfer-and-receive (STAR) reconfigurable intelligent surface
(RIS)-assisted interference-limited systems with finite block length (FBL).
This framework can solve a large variety of optimization problems in which the
objective and/or constraints are linear functions of the rates and/or EE of
users. Additionally, the framework can be applied to any interference-limited
system with treating interference as noise as the decoding strategy at
receivers. We consider a multi-cell broadcast channel as an example and show
how this framework can be specialized to solve the minimum-weighted rate,
weighted sum rate, global EE and weighted EE of the system. We make realistic
assumptions regarding the (STAR-)RIS by considering three different feasibility
sets for the components of either regular RIS or STAR-RIS. Our results show
that RIS can substantially increase the spectral and EE of URLLC systems if the
reflecting coefficients are properly optimized. Moreover, we consider three
different transmission strategies for STAR-RIS as energy splitting (ES), mode
switching (MS), and time switching (TS). We show that STAR-RIS can outperform a
regular RIS when the regular RIS cannot cover all the users. Furthermore, it is
shown that the ES scheme outperforms the MS and TS schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ACCESS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On The Effects Of Data Normalisation For Domain Adaptation On EEG Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Apicella, Francesco Isgrò, Andrea Pollastro, Roberto Prevete
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Machine Learning (ML) literature, a well-known problem is the Dataset
Shift problem where, differently from the ML standard hypothesis, the data in
the training and test sets can follow different probability distributions,
leading ML systems toward poor generalisation performances. This problem is
intensely felt in the Brain-Computer Interface (BCI) context, where bio-signals
as Electroencephalographic (EEG) are often used. In fact, EEG signals are
highly non-stationary both over time and between different subjects. To
overcome this problem, several proposed solutions are based on recent transfer
learning approaches such as Domain Adaption (DA). In several cases, however,
the actual causes of the improvements remain ambiguous. This paper focuses on
the impact of data normalisation, or standardisation strategies applied
together with DA methods. In particular, using \textit{SEED}, \textit{DEAP},
and \textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated
the impact of different normalization strategies applied with and without
several well-known DA methods, comparing the obtained performances. It results
that the choice of the normalisation strategy plays a key role on the
classifier performances in DA scenarios, and interestingly, in several cases,
the use of only an appropriate normalisation schema outperforms the DA
technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in its final version on Engineering Applications of
  Artificial Intelligence (EAAI) https://doi.org/10.1016/j.engappai.2023.106205</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grid-Free MIMO Beam Alignment through Site-Specific Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqiang Heng, Jeffrey G. Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beam alignment is a critical bottleneck in millimeter wave (mmWave)
communication. An ideal beam alignment technique should achieve high
beamforming (BF) gain with low latency, scale well to systems with higher
carrier frequencies, larger antenna arrays and multiple user equipments (UEs),
and not require hard-to-obtain context information (CI). These qualities are
collectively lacking in existing methods. We depart from the conventional
codebook-based (CB) approach where the optimal beam is chosen from quantized
codebooks and instead propose a grid-free (GF) beam alignment method that
directly synthesizes the transmit (Tx) and receive (Rx) beams from the
continuous search space using measurements from a few site-specific probing
beams that are found via a deep learning (DL) pipeline. In realistic settings,
the proposed method achieves a far superior signal-to-noise ratio (SNR)-latency
trade-off compared to the CB baselines: it aligns near-optimal beams 100x
faster or equivalently finds beams with 10-15 dB better average SNR in the same
number of searches, relative to an exhaustive search over a conventional
codebook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in IEEE Transactions on Wireless Communications,
  10.1109/TWC.2023.3283475</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a
  Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.06295v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.06295v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albara Ah Ramli, Xin Liu, Kelly Berndt, Erica Goude, Jiahui Hou, Lynea B. Kaethler, Rex Liu, Amanda Lopez, Alina Nicorici, Corey Owens, David Rodriguez, Jane Wang, Huanle Zhang, Daniel Aranki, Craig M. McDonald, Erik K. Henricson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differences in gait patterns of children with Duchenne muscular dystrophy
(DMD) and typically-developing (TD) peers are visible to the eye, but
quantifications of those differences outside of the gait laboratory have been
elusive. In this work, we measured vertical, mediolateral, and anteroposterior
acceleration using a waist-worn iPhone accelerometer during ambulation across a
typical range of velocities. Fifteen TD and fifteen DMD children from 3-16
years of age underwent eight walking/running activities, including five 25
meters walk/run speed-calibration tests at a slow walk to running speeds (SC-L1
to SC-L5), a 6-minute walk test (6MWT), a 100 meters fast-walk/jog/run
(100MRW), and a free walk (FW). For clinical anchoring purposes, participants
completed a Northstar Ambulatory Assessment (NSAA). We extracted temporospatial
gait clinical features (CFs) and applied multiple machine learning (ML)
approaches to differentiate between DMD and TD children using extracted
temporospatial gait CFs and raw data. Extracted temporospatial gait CFs showed
reduced step length and a greater mediolateral component of total power (TP)
consistent with shorter strides and Trendelenberg-like gait commonly observed
in DMD. ML approaches using temporospatial gait CFs and raw data varied in
effectiveness at differentiating between DMD and TD controls at different
speeds, with an accuracy of up to 100%. We demonstrate that by using ML with
accelerometer data from a consumer-grade smartphone, we can capture
DMD-associated gait characteristics in toddlers to teens.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deceptive Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajani Vithana, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the problem of deceptive information retrieval (DIR), in which a
user wishes to download a required file out of multiple independent files
stored in a system of databases while \emph{deceiving} the databases by making
the databases' predictions on the user-required file index incorrect with high
probability. Conceptually, DIR is an extension of private information retrieval
(PIR). In PIR, a user downloads a required file without revealing its index to
any of the databases. The metric of deception is defined as the probability of
error of databases' prediction on the user-required file, minus the
corresponding probability of error in PIR. The problem is defined on
time-sensitive data that keeps updating from time to time. In the proposed
scheme, the user deceives the databases by sending \emph{real} queries to
download the required file at the time of the requirement and \emph{dummy}
queries at multiple distinct future time instances to manipulate the
probabilities of sending each query for each file requirement, using which the
databases' make the predictions on the user-required file index. The proposed
DIR scheme is based on a capacity achieving probabilistic PIR scheme, and
achieves rates lower than the PIR capacity due to the additional downloads made
to deceive the databases. When the required level of deception is zero, the
proposed scheme achieves the PIR capacity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cobalt: Optimizing Mining Rewards in Proof-of-Work Network Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arti Vedula, Abhishek Gupta, Shaileshh Bojja Venkatakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mining in proof-of-work blockchains has become an expensive affair requiring
specialized hardware capable of executing several megahashes per second at huge
electricity costs. Miners earn a reward each time they mine a block within the
longest chain, which helps offset their mining costs. It is therefore of
interest to miners to maximize the number of mined blocks in the blockchain and
increase revenue. A key factor affecting mining rewards earned is the
connectivity between miners in the peer-to-peer network. To maximize rewards a
miner must choose its network connections carefully, ensuring existence of
paths to other miners that are on average of a lower latency compared to paths
between other miners. We formulate the problem of deciding whom to connect to
for miners as a combinatorial bandit problem. Each node picks its neighbors
strategically to minimize the latency to reach 90\% of the hash power of the
network relative to the 90-th percentile latency from other nodes. A key
contribution of our work is the use of a network coordinates based model for
learning the network structure within the bandit algorithm. Experimentally we
show our proposed algorithm outperforming or matching baselines on diverse
network settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Trustworthiness Model for DNN in Dedicated 6G Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anouar Nechi, Ahmed Mahmoudi, Christoph Herold, Daniel Widmer, Thomas Kürner, Mladen Berekovic, Saleh Mulhem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) is considered an efficient response to several
challenges facing 6G technology. However, AI still suffers from a huge trust
issue due to its ambiguous way of making predictions. Therefore, there is a
need for a method to evaluate the AI's trustworthiness in practice for future
6G applications. This paper presents a practical model to analyze the
trustworthiness of AI in a dedicated 6G application. In particular, we present
two customized Deep Neural Networks (DNNs) to solve the Automatic Modulation
Recognition (AMR) problem in Terahertz communications-based 6G technology.
Then, a specific trustworthiness model and its attributes, namely data
robustness, parameter sensitivity, and security covering adversarial examples,
are introduced. The evaluation results indicate that the proposed
trustworthiness attributes are crucial to evaluate the trustworthiness of DNN
for this 6G application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PREPRINT - accepted In Proceedings of the 19th International
  Conference on Wireless and Mobile Computing, Networking and Communications
  2023(STWiMob 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An End-To-End Analysis of Deep Learning-Based Remaining Useful Life
  Algorithms for Satefy-Critical 5G-Enabled IIoT Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Mario Amorosa, Nicolò Longhi, Giampaolo Cuozzo, Weronika Maria Bachan, Valerio Lieti, Enrico Buracchini, Roberto Verdone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remaining Useful Life (RUL) prediction is a critical task that aims to
estimate the amount of time until a system fails, where the latter is formed by
three main components, that is, the application, communication network, and RUL
logic. In this paper, we provide an end-to-end analysis of an entire RUL-based
chain. Specifically, we consider a factory floor where Automated Guided
Vehicles (AGVs) transport dangerous liquids whose fall may cause injuries to
workers. Regarding the communication infrastructure, the AGVs are equipped with
5G User Equipments (UEs) that collect real-time data of their movements and
send them to an application server. The RUL logic consists of a Deep Learning
(DL)-based pipeline that assesses if there will be liquid falls by analyzing
the collected data, and, eventually, sending commands to the AGVs to avoid such
a danger. According to this scenario, we performed End-to-End 5G NR-compliant
network simulations to study the Round-Trip Time (RTT) as a function of the
overall system bandwidth, subcarrier spacing, and modulation order. Then, via
real-world experiments, we collect data to train, test and compare 7 DL models
and 1 baseline threshold-based algorithm in terms of cost and average advance.
Finally, we assess whether or not the RTT provided by four different 5G NR
network architectures is compatible with the average advance provided by the
best-performing one-Dimensional Convolutional Neural Network (1D-CNN).
Numerical results show under which conditions the DL-based approach for RUL
estimation matches with the RTT performance provided by different 5G NR network
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures. This work has been accepted for publication at
  IEEE PIMRC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Layer Assisted Early Congestion Control for Cloud VR Services in
  5G Edge Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanghong Yang, Wenji Du, Baosen Zhao, Yongmao Ren, Jianan Sun, Xu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud virtual reality (VR) has emerged as a promising technology, offering
users a highly immersive and easily accessible experience. However, the current
5G radio access network faces challenges in accommodating the bursty traffic
generated by multiple cloudVR flows simultaneously, leading to congestion at
the 5G base station and increased delays. In this research, we present a
comprehensive quantitative analysis that highlights the underlying causes for
the poor delay performance of cloudVR flows within the existing 5G protocol
stack and network. To address these issues, we propose a novel cross-layer
informationassisted congestion control mechanism deployed in the 5G edge
network. Experiment results show that our mechanism enhances the number of
concurrent flows meeting delay standards by 1.5x to 2.5x, while maintaining a
smooth network load. These findings underscore the potential of leveraging 5G
edge nodes as a valuable resource to effectively meet the anticipated demands
of future services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>this paper is under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCS-based Quasi-Deterministic Ray Tracing for Statistical Channel
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javad Ebrahimizadeh, Evgenii Vinogradov, Guy A. E. Vandenbosch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a quasi-deterministic ray tracing (QD-RT) method for
analyzing the propagation of electromagnetic waves in street canyons. The
method uses a statistical bistatic distribution to model the Radar Cross
Section (RCS) of various irregular objects such as cars and pedestrians,
instead of relying on exact values as in a deterministic propagation model. The
performance of the QD-RT method is evaluated by comparing its generated path
loss distributions to those of the deterministic ray tracing (D-RT) model using
the Two-sample Cramer-von Mises test. The results indicate that the QD-RT
method generates the same path loss distributions as the D-RT model while
offering lower complexity. This study suggests that the QD-RT method has the
potential to be used for analyzing complicated scenarios such as street canyon
scenarios in mmWave wireless communication systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Bipartite Entanglement Capacity of Quantum Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayane Vardoyan, Emily van Milligen, Saikat Guha, Stephanie Wehner, Don Towsley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of multi-path entanglement distribution to a pair of
nodes in a quantum network consisting of devices with non-deterministic
entanglement swapping capabilities. Multi-path entanglement distribution
enables a network to establish end-to-end entangled links across any number of
available paths with pre-established link-level entanglement. Probabilistic
entanglement swapping, on the other hand, limits the amount of entanglement
that is shared between the nodes; this is especially the case when, due to
architectural and other practical constraints, swaps must be performed in
temporal proximity to each other. Limiting our focus to the case where only
bipartite entangled states are generated across the network, we cast the
problem as an instance of generalized flow maximization between two quantum end
nodes wishing to communicate. We propose a mixed-integer quadratically
constrained program (MIQCP) to solve this flow problem for networks with
arbitrary topology. We then compute the overall network capacity, defined as
the maximum number of EPR states distributed to users per time unit, by solving
the flow problem for all possible network states generated by probabilistic
entangled link presence and absence, and subsequently by averaging over all
network state capacities. The MIQCP can also be applied to networks with
multiplexed links. While our approach for computing the overall network
capacity has the undesirable property that the total number of states grows
exponentially with link multiplexing capability, it nevertheless yields an
exact solution that serves as an upper bound comparison basis for the
throughput performance of easily-implementable yet non-optimal entanglement
routing algorithms. We apply our capacity computation method to several
networks, including a topology based on SURFnet -- a backbone network used for
research purposes in the Netherlands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Kalman Filter based Low Complexity Throughput Prediction Algorithm for
  5G Cellular Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayukh Biswas, Ayan Chakraborty, Basabdatta Palit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Throughput Prediction is one of the primary preconditions for the
uninterrupted operation of several network-aware mobile applications, namely
video streaming. Recent works have advocated using Machine Learning (ML) and
Deep Learning (DL) for cellular network throughput prediction. In contrast,
this work has proposed a low computationally complex simple solution which
models the future throughput as a multiple linear regression of several present
network parameters and present throughput. It then feeds the variance of
prediction error and measurement error, which is inherent in any measurement
setup but unaccounted for in existing works, to a Kalman filter-based
prediction-correction approach to obtain the optimal estimates of the future
throughput. Extensive experiments across seven publicly available 5G throughput
datasets for different prediction window lengths have shown that the proposed
method outperforms the baseline ML and DL algorithms by delivering more
accurate results within a shorter timeframe for inferencing and retraining.
Furthermore, in comparison to its ML and DL counterparts, the proposed
throughput prediction method is also found to deliver higher QoE to both
streaming and live video users when used in conjunction with popular Model
Predictive Control (MPC) based adaptive bitrate streaming algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Network Embedding without Explicit Virtual Network Specification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangnan Cheng, Yingjie Bi, Ao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network virtualization enables Internet service providers to run multiple
heterogeneous and dedicated network architectures for different customers on a
shared substrate. In existing works on virtual network embedding (VNE), each
customer formulates a virtual network request (VNR) where a virtual network
(VN) is required. Motivated by a concrete example where VN is not a proper VNR
formulation to reflect the traffic demand of a customer, we propose a new VNR
formulation described by the traffic demand between several access node pairs
to complement the existing VNR formulation. Moreover, three different groups of
VNE variants are systematically examined. Simulations demonstrate that shared
channel embedding, as a new embedding variant under the proposed VNR
formulation, improves the acceptance rate and reduces cost and link utility
compared to traditional independent channel embedding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Predictive Latency for 5G: A Theoretical and Experimental
  Analysis Using Network Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Skocaj, Francesca Conserva, Nicol Sarcone Grande, Andrea Orsi, Davide Micheli, Giorgio Ghinamo, Simone Bizzarri, Roberto Verdone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of novel 5G services and applications with binding latency
requirements and guaranteed Quality of Service (QoS) hastened the need to
incorporate autonomous and proactive decision-making in network management
procedures. The objective of our study is to provide a thorough analysis of
predictive latency within 5G networks by utilizing real-world network data that
is accessible to mobile network operators (MNOs). In particular, (i) we present
an analytical formulation of the user-plane latency as a Hypoexponential
distribution, which is validated by means of a comparative analysis with
empirical measurements, and (ii) we conduct experimental results of
probabilistic regression, anomaly detection, and predictive forecasting
leveraging on emerging domains in Machine Learning (ML), such as Bayesian
Learning (BL) and Machine Learning on Graphs (GML). We test our predictive
framework using data gathered from scenarios of vehicular mobility, dense-urban
traffic, and social gathering events. Our results provide valuable insights
into the efficacy of predictive algorithms in practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeDrag: Point Tracking is Not You Need for Interactive Point-based
  Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To serve the intricate and varied demands of image editing, precise and
flexible manipulation of image content is indispensable. Recently, DragGAN has
achieved impressive editing results through point-based manipulation. However,
we have observed that DragGAN struggles with miss tracking, where DragGAN
encounters difficulty in effectively tracking the desired handle points, and
ambiguous tracking, where the tracked points are situated within other regions
that bear resemblance to the handle points. To deal with the above issues, we
propose FreeDrag, which adopts a feature-oriented approach to free the burden
on point tracking within the point-oriented methodology of DragGAN. The
FreeDrag incorporates adaptive template features, line search, and fuzzy
localization techniques to perform stable and efficient point-based image
editing. Extensive experiments demonstrate that our method is superior to the
DragGAN and enables stable point-based editing in challenging scenarios with
similar structures, fine details, or under multi-point targets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically detecting activities of daily living from in-home sensors
  as indicators of routine behaviour in an older population 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire M. Timon, Pamela Hussey, Hyowon Lee, Catriona Murphy, Harsh Vardan Rai, and Alan F. Smeaton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: The NEX project has developed an integrated Internet of Things
(IoT) system coupled with data analytics to offer unobtrusive health and
wellness monitoring supporting older adults living independently at home.
Monitoring {currently} involves visualising a set of automatically detected
activities of daily living (ADLs) for each participant. The detection of ADLs
is achieved {} to allow the incorporation of additional participants whose ADLs
are detected without re-training the system.
  Methods: Following an extensive User Needs and Requirements study involving
426 participants, a pilot trial and a friendly trial of the deployment, an
Action Research Cycle (ARC) trial was completed. This involved 23 participants
over a 10-week period each with c.20 IoT sensors in their homes. During the ARC
trial, participants each took part in two data-informed briefings which
presented visualisations of their own in-home activities. The briefings also
gathered training data on the accuracy of detected activities. Association rule
mining was then used on the combination of data from sensors and participant
feedback to improve the automatic detection of ADLs.
  Results: Association rule mining was used to detect a range of ADLs for each
participant independently of others and was then used to detect ADLs across
participants using a single set of rules {for each ADL}. This allows additional
participants to be added without the necessity of them providing training data.
  Conclusions: Additional participants can be added to the NEX system without
the necessity to re-train the system for automatic detection of the set of
their activities of daily living.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 Figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Needs, Passions and Loot Boxes -- Exploring Reasons for Problem
  Behaviour in Relation to Loot Box Engagement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Mercury Cooper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on the convergence of gaming and gambling has been around since the
1990s. The emergence of loot boxes in video games in the mid 2010s, a game
mechanic with a chance-based outcome that shares structural and psychological
similarities to gambling, caused public controversy and lead to the inception
of a new field of study, loot box research. Since then, various studies have
found a relationship between loot box engagement and problem gambling as well
as problem gaming. Due to the cross-sectional nature of this data, however,
inferences about causality are limited. While loot box research has extensively
investigated the relationship between loot box engagement and problem
behaviour, little research has been done to explain the underlying motivations
of players that drive them to interact with loot boxes. The goal of this thesis
is to provide possible explanations for the relationship between loot box
engagement and problem gamblers or problem gamers. In doing so, it draws upon
two prominent psychological theories. Self-Determination Theory and the
Dualistic Model of Passion. Self-Determination Theory's concept of
psychological needs and their satisfaction or frustration is hereby used to
explain the development of harmonious or obsessive passions, which are
introduced in the Dualistic Model of Passion. These obsessive passions have
been shown to be possible antecedents of behavioural addictions, such as
problem gambling or problem gaming. Thus, the interplay between needs, passions
and loot box opening could elucidate the aforementioned correlations between
loot box engagement and problem behaviour. However, further research,
especially utilising longitudinal data, is needed to better understand these
processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithm- Versus Human-Generated Academic Plans: Determining Optimality
  from Community College Articulation Agreements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Van Nguyen, Shayan Doroudi, Daniel A. Epstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We developed a low-fidelity prototype of a report that contains an
algorithmically-generated optimal academic plan. Optimal is defined as the
minimal set of community college courses that satisfy the transfer requirements
for multiple universities a student is preparing to apply to. We recruited 24
California community college transfer students to participate in a research
session, consisting of an experiment, survey, and interview. We experimentally
compared the prototype to ASSIST, California's official statewide database of
articulation agreement reports. Compared to students who used the prototype,
students assigned to use ASSIST reports to manually create an optimal academic
plan underperformed in optimality mistakes, time required, and usability
scores. Moving to our non-experimental results, a sizable minority of students
had a negative assessment of counselors' ability and willingness to manually
create optimal academic plans using ASSIST. Our last results revolved around
students' recommendations for supplemental software features to improve the
optimization prototype.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Modeling for Everyone: Exploring How Novices Approach
  Voice-Based 3D Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Desolda, Andrea Esposito, Florian Müller, Sebastian Feger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manufacturing tools like 3D printers have become accessible to the wider
society, making the promise of digital fabrication for everyone seemingly
reachable. While the actual manufacturing process is largely automated today,
users still require knowledge of complex design applications to produce
ready-designed objects and adapt them to their needs or design new objects from
scratch. To lower the barrier to the design and customization of personalized
3D models, we explored novice mental models in voice-based 3D modeling by
conducting a high-fidelity Wizard of Oz study with 22 participants. We
performed a thematic analysis of the collected data to understand how the
mental model of novices translates into voice-based 3D modeling. We conclude
with design implications for voice assistants. For example, they have to: deal
with vague, incomplete and wrong commands; provide a set of straightforward
commands to shape simple and composite objects; and offer different strategies
to select 3D objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at INTERACT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilising Explanations to Mitigate Robot Conversational Failures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimosthenis Kontogiorgos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an overview of robot failure detection work from HRI and
adjacent fields using failures as an opportunity to examine robot explanation
behaviours. As humanoid robots remain experimental tools in the early 2020s,
interactions with robots are situated overwhelmingly in controlled
environments, typically studying various interactional phenomena. Such
interactions suffer from real-world and large-scale experimentation and tend to
ignore the 'imperfectness' of the everyday user. Robot explanations can be used
to approach and mitigate failures, by expressing robot legibility and
incapability, and within the perspective of common-ground. In this paper, I
discuss how failures present opportunities for explanations in interactive
conversational robots and what the potentials are for the intersection of HRI
and explainability research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Synthesis Lab: Empowering Collaborative Learning in Higher Education
  through Knowledge Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Zhu, Hong Shui, Bodong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to synthesize information has emerged as a critical skill for
success across various fields. However, within the field of education, there is
a lack of systematic understanding and well-defined design infrastructures that
address the mechanisms and processes of knowledge synthesis in collaborative
learning settings. In this poster, we introduce a design innovation - The
Synthesis Lab, which aims to support students in synthesizing ideas from their
online discussions in higher education classrooms. The tool offers structured
work-spaces for students to decompose the synthesis process into intermediate
synthesis products and features two key iterative processes of knowledge
synthesis in collaborative settings: categorizing peers' ideas into conceptual
building blocks and developing a synthesis of the discussions. Future
implementation and evaluation of the design will make significant contributions
to both research and practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amadeus<span class="highlight-title">GPT</span>: a natural language interface for interactive animal
  behavioral analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Ye, Jessy Lauer, Mu Zhou, Alexander Mathis, Mackenzie W. Mathis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of quantifying and analyzing animal behavior involves translating
the naturally occurring descriptive language of their actions into
machine-readable code. Yet, codifying behavior analysis is often challenging
without deep understanding of animal behavior and technical machine learning
knowledge. To limit this gap, we introduce AmadeusGPT: a natural language
interface that turns natural language descriptions of behaviors into
machine-executable code. Large-language models (LLMs) such as GPT3.5 and GPT4
allow for interactive language-based queries that are potentially well suited
for making interactive behavior analysis. However, the comprehension capability
of these LLMs is limited by the context window size, which prevents it from
remembering distant conversations. To overcome the context window limitation,
we implement a novel dual-memory mechanism to allow communication between
short-term and long-term memory using symbols as context pointers for retrieval
and saving. Concretely, users directly use language-based definitions of
behavior and our augmented GPT develops code based on the core AmadeusGPT API,
which contains machine learning, computer vision, spatio-temporal reasoning,
and visualization modules. Users then can interactively refine results, and
seamlessly add new behavioral modules as needed. We benchmark AmadeusGPT and
show we can produce state-of-the-art performance on the MABE 2022 behavior
challenge tasks. Note, an end-user would not need to write any code to achieve
this. Thus, collectively AmadeusGPT presents a novel way to merge deep
biological knowledge, large-language models, and core computer vision modules
into a more naturally intelligent system. Code and demos can be found at:
https://github.com/AdaptiveMotorControlLab/AmadeusGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>demo available https://github.com/AdaptiveMotorControlLab/AmadeusGPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study of <span class="highlight-title">Self-Supervised</span> Speech Representations in Read
  and Spontaneous TTS <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has explored using self-supervised learning (SSL) speech
representations such as wav2vec2.0 as the representation medium in standard
two-stage TTS, in place of conventionally used mel-spectrograms. It is however
unclear which speech SSL is the better fit for TTS, and whether or not the
performance differs between read and spontaneous TTS, the later of which is
arguably more challenging. This study aims at addressing these questions by
testing several speech SSLs, including different layers of the same SSL, in
two-stage TTS on both read and spontaneous corpora, while maintaining constant
TTS model architecture and training settings. Results from listening tests show
that the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other
tested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our work
sheds light on both how speech SSL can readily improve current TTS systems, and
how SSLs compare in the challenging generative task of TTS. Audio examples can
be found at https://www.speech.kth.se/tts-demos/ssr_tts
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures. ICASSP Workshop SASB (Self-Supervision in Audio,
  Speech and Beyond)2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-TTSG: Denoising probabilistic integrated speech and gesture
  synthesis <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With read-aloud speech synthesis achieving high naturalness scores, there is
a growing research interest in synthesising spontaneous speech. However, human
spontaneous face-to-face conversation has both spoken and non-verbal aspects
(here, co-speech gestures). Only recently has research begun to explore the
benefits of jointly synthesising these two modalities in a single system. The
previous state of the art used non-probabilistic methods, which fail to capture
the variability of human speech and motion, and risk producing oversmoothing
artefacts and sub-optimal synthesis quality. We present the first
diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to
synthesise speech and gestures together. Our method can be trained on small
datasets from scratch. Furthermore, we describe a set of careful uni- and
multi-modal subjective tests for evaluating integrated speech and gesture
synthesis systems, and use them to validate our proposed approach. For
synthesised examples please see https://shivammehta25.github.io/Diff-TTSG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, Accepted at ISCA Speech Synthesis Workshop (SSW)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Chat<span class="highlight-title">GPT</span> pass the Vietnamese National High School Graduation
  Examination? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09170v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09170v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Quy Dao, Ngoc-Bich Le, Xuan-Dung Phan, Bac-Bien Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research article highlights the potential of AI-powered chatbots in
education and presents the results of using ChatGPT, a large language model, to
complete the Vietnamese National High School Graduation Examination (VNHSGE).
The study dataset included 30 essays in the literature test case and 1,700
multiple-choice questions designed for other subjects. The results showed that
ChatGPT was able to pass the examination with an average score of 6-7,
demonstrating the technology's potential to revolutionize the educational
landscape. The analysis of ChatGPT performance revealed its proficiency in a
range of subjects, including mathematics, English, physics, chemistry, biology,
history, geography, civic education, and literature, which suggests its
potential to provide effective support for learners. However, further research
is needed to assess ChatGPT performance on more complex exam questions and its
potential to support learners in different contexts. As technology continues to
evolve and improve, we can expect to see the use of AI tools like ChatGPT
become increasingly common in educational settings, ultimately enhancing the
educational experience for both students and educators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 13 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Continual Learning for Socially Aware Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Guerdan, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From learning assistance to companionship, social robots promise to enhance
many aspects of daily life. However, social robots have not seen widespread
adoption, in part because (1) they do not adapt their behavior to new users,
and (2) they do not provide sufficient privacy protections. Centralized
learning, whereby robots develop skills by gathering data on a server,
contributes to these limitations by preventing online learning of new
experiences and requiring storage of privacy-sensitive data. In this work, we
propose a decentralized learning alternative that improves the privacy and
personalization of social robots. We combine two machine learning approaches,
Federated Learning and Continual Learning, to capture interaction dynamics
distributed physically across robots and temporally across repeated robot
encounters. We define a set of criteria that should be balanced in
decentralized robot learning scenarios. We also develop a new algorithm --
Elastic Transfer -- that leverages importance-based regularization to preserve
relevant parameters across robots and interactions with multiple humans. We
show that decentralized learning is a viable alternative to centralized
learning in a proof-of-concept Socially-Aware Navigation domain, and
demonstrate how Elastic Transfer improves several of the proposed criteria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE RO-MAN 23'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span> detectors are biased against non-native English writers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of generative language models has brought about
substantial advancements in digital communication, while simultaneously raising
concerns regarding the potential misuse of AI-generated content. Although
numerous detection methods have been proposed to differentiate between AI and
human-generated content, the fairness and robustness of these detectors remain
underexplored. In this study, we evaluate the performance of several
widely-used GPT detectors using writing samples from native and non-native
English writers. Our findings reveal that these detectors consistently
misclassify non-native English writing samples as AI-generated, whereas native
writing samples are accurately identified. Furthermore, we demonstrate that
simple prompting strategies can not only mitigate this bias but also
effectively bypass GPT detectors, suggesting that GPT detectors may
unintentionally penalize writers with constrained linguistic expressions. Our
results call for a broader conversation about the ethical implications of
deploying ChatGPT content detectors and caution against their use in evaluative
or educational settings, particularly when they may inadvertently penalize or
exclude non-native English speakers from the global discourse. The published
version of this study can be accessed at:
www.cell.com/patterns/fulltext/S2666-3899(23)00130-7
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">81</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image
  Alignment with Iterative VQA Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaskirat Singh, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of text-conditioned image generation has made unparalleled progress
with the recent advent of latent diffusion models. While remarkable, as the
complexity of given text input increases, the state-of-the-art diffusion models
may still fail in generating images which accurately convey the semantics of
the given prompt. Furthermore, it has been observed that such misalignments are
often left undetected by pretrained multi-modal models such as CLIP. To address
these problems, in this paper we explore a simple yet effective decompositional
approach towards both evaluation and improvement of text-to-image alignment. In
particular, we first introduce a Decompositional-Alignment-Score which given a
complex prompt decomposes it into a set of disjoint assertions. The alignment
of each assertion with generated images is then measured using a VQA model.
Finally, alignment scores for different assertions are combined aposteriori to
give the final text-to-image alignment score. Experimental analysis reveals
that the proposed alignment metric shows significantly higher correlation with
human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also
find that the assertion level alignment scores provide a useful feedback which
can then be used in a simple iterative procedure to gradually increase the
expression of different assertions in the final image outputs. Human user
studies indicate that the proposed approach surpasses previous state-of-the-art
by 8.7% in overall text-to-image alignment accuracy. Project page for our paper
is available at https://1jsingh.github.io/divide-evaluate-and-refine
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoCo: Dialectic Multi-Robot Collaboration with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Mandi, Shreeya Jain, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to multi-robot collaboration that harnesses the
power of pre-trained large language models (LLMs) for both high-level
communication and low-level path planning. Robots are equipped with LLMs to
discuss and collectively reason task strategies. They then generate sub-task
plans and task space waypoint paths, which are used by a multi-arm motion
planner to accelerate trajectory planning. We also provide feedback from the
environment, such as collision checking, and prompt the LLM agents to improve
their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a
6-task benchmark covering a wide range of multi-robot collaboration scenarios,
accompanied by a text-only dataset for agent representation and reasoning. We
experimentally demonstrate the effectiveness of our approach -- it achieves
high success rates across all tasks in RoCoBench and adapts to variations in
task semantics. Our dialog setup offers high interpretability and flexibility
-- in real world experiments, we show RoCo easily incorporates
human-in-the-loop, where a user can communicate and collaborate with a robot
agent to complete tasks together. See project website
https://project-roco.github.io for videos and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Policies for Out-of-Distribution Generalization in Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suzan Ece Ada, Erhan Oztop, Emre Ugur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) methods leverage previous experiences to
learn better policies than the behavior policy used for experience collection.
In contrast to behavior cloning, which assumes the data is collected from
expert demonstrations, offline RL can work with non-expert data and multimodal
behavior policies. However, offline RL algorithms face challenges in handling
distribution shifts and effectively representing policies due to the lack of
online interaction during training. Prior work on offline RL uses conditional
diffusion models to obtain expressive policies to represent multimodal behavior
in the dataset. Nevertheless, they are not tailored toward alleviating the
out-of-distribution state generalization. We introduce a novel method
incorporating state reconstruction feature learning in the recent class of
diffusion policies to address the out-of-distribution generalization problem.
State reconstruction loss promotes more descriptive representation learning of
states to alleviate the distribution shift incurred by the out-of-distribution
states. We design a 2D Multimodal Contextual Bandit environment to demonstrate
and evaluate our proposed model. We assess the performance of our model not
only in this new environment but also on several D4RL benchmark tasks,
achieving state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as General Pattern Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe that pre-trained large language models (LLMs) are capable of
autoregressively completing complex token sequences -- from arbitrary ones
procedurally generated by probabilistic context-free grammars (PCFG), to more
rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general
AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern
completion proficiency can be partially retained even when the sequences are
expressed using tokens randomly sampled from the vocabulary. These results
suggest that without any additional training, LLMs can serve as general
sequence modelers, driven by in-context learning. In this work, we investigate
how these zero-shot capabilities may be applied to problems in robotics -- from
extrapolating sequences of numbers that represent states over time to complete
simple motions, to least-to-most prompting of reward-conditioned trajectories
that can discover and represent closed-loop policies (e.g., a stabilizing
controller for CartPole). While difficult to deploy today for real systems due
to latency, context size limitations, and compute costs, the approach of using
LLMs to drive low-level control may provide an exciting glimpse into how the
patterns among words could be transferred to actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Real-World AI Planning Domains: A Conceptual Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ebaa Alnazer, Ilche Georgievski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning is a pivotal ability of any intelligent system being developed for
real-world applications. AI planning is concerned with researching and
developing planning systems that automatically compute plans that satisfy some
user objective. Identifying and understanding the relevant and realistic
aspects that characterise real-world application domains are crucial to the
development of AI planning systems. This provides guidance to knowledge
engineers and software engineers in the process of designing, identifying, and
categorising resources required for the development process. To the best of our
knowledge, such support does not exist. We address this research gap by
developing a conceptual framework that identifies and categorises the aspects
of real-world planning domains in varying levels of granularity. Our framework
provides not only a common terminology but also a comprehensive overview of a
broad range of planning aspects exemplified using the domain of sustainable
buildings as a prominent application domain of AI planning. The framework has
the potential to impact the design, development, and applicability of AI
planning systems in real-world application domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures, 17th Symposium and Summer School (SummerSOC)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMEX: A Tool for Generating Customized Source Code Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debeshee Das, Noble Saji Mathews, Alex Mathai, Srikanth Tamilselvam, Kranthi Sedamaki, Sridhar Chimalakonda, Atul Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning effective representations of source code is critical for any Machine
Learning for Software Engineering (ML4SE) system. Inspired by natural language
processing, large language models (LLMs) like Codex and CodeGen treat code as
generic sequences of text and are trained on huge corpora of code data,
achieving state of the art performance on several software engineering (SE)
tasks. However, valid source code, unlike natural language, follows a strict
structure and pattern governed by the underlying grammar of the programming
language. Current LLMs do not exploit this property of the source code as they
treat code like a sequence of tokens and overlook key structural and semantic
properties of code that can be extracted from code-views like the Control Flow
Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc.
Unfortunately, the process of generating and integrating code-views for every
programming language is cumbersome and time consuming. To overcome this
barrier, we propose our tool COMEX - a framework that allows researchers and
developers to create and combine multiple code-views which can be used by
machine learning (ML) models for various SE tasks. Some salient features of our
tool are: (i) it works directly on source code (which need not be compilable),
(ii) it currently supports Java and C#, (iii) it can analyze both method-level
snippets and program-level snippets by using both intra-procedural and
inter-procedural analysis, and (iv) it is easily extendable to other languages
as it is built on tree-sitter - a widely used incremental parser that supports
over 40 languages. We believe this easy-to-use code-view generation and
customization tool will give impetus to research in source code representation
learning methods and ML4SE.
  Tool: https://pypi.org/project/comex - GitHub:
https://github.com/IBM/tree-sitter-codeviews - Demo:
https://youtu.be/GER6U87FVbU
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted for publication at ASE 2023 (Tool
  Demonstrations Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VampNet: Music Generation via Masked Acoustic Token Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VampNet, a masked acoustic token modeling approach to music
synthesis, compression, inpainting, and variation. We use a variable masking
schedule during training which allows us to sample coherent music from the
model by applying a variety of masking approaches (called prompts) during
inference. VampNet is non-autoregressive, leveraging a bidirectional
transformer architecture that attends to all tokens in a forward pass. With
just 36 sampling passes, VampNet can generate coherent high-fidelity musical
waveforms. We show that by prompting VampNet in various ways, we can apply it
to tasks like music compression, inpainting, outpainting, continuation, and
looping with variation (vamping). Appropriately prompted, VampNet is capable of
maintaining style, genre, instrumentation, and other high-level aspects of the
music. This flexible prompting capability makes VampNet a powerful music
co-creation tool. Code and audio samples are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Echo Chamber Effect: An Embedding Distance-based
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faisal Alatawi, Paras Sheth, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of social media platforms has facilitated the formation of echo
chambers, which are online spaces where users predominantly encounter
viewpoints that reinforce their existing beliefs while excluding dissenting
perspectives. This phenomenon significantly hinders information dissemination
across communities and fuels societal polarization. Therefore, it is crucial to
develop methods for quantifying echo chambers. In this paper, we present the
Echo Chamber Score (ECS), a novel metric that assesses the cohesion and
separation of user communities by measuring distances between users in the
embedding space. In contrast to existing approaches, ECS is able to function
without labels for user ideologies and makes no assumptions about the structure
of the interaction graph. To facilitate measuring distances between users, we
propose EchoGAE, a self-supervised graph autoencoder-based user embedding model
that leverages users' posts and the interaction graph to embed them in a manner
that reflects their ideological similarity. To assess the effectiveness of ECS,
we use a Twitter dataset consisting of four topics - two polarizing and two
non-polarizing. Our results showcase ECS's effectiveness as a tool for
quantifying echo chambers and shedding light on the dynamics of online
discourse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly-supervised positional contrastive learning: application to
  cirrhosis classification <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large medical imaging datasets can be cheaply and quickly annotated with
low-confidence, weak labels (e.g., radiological scores). Access to
high-confidence labels, such as histology-based diagnoses, is rare and costly.
Pretraining strategies, like contrastive learning (CL) methods, can leverage
unlabeled or weakly-annotated datasets. These methods typically require large
batch sizes, which poses a difficulty in the case of large 3D images at full
resolution, due to limited GPU memory. Nevertheless, volumetric positional
information about the spatial context of each 2D slice can be very important
for some medical applications. In this work, we propose an efficient
weakly-supervised positional (WSP) contrastive learning strategy where we
integrate both the spatial context of each 2D slice and a weak label via a
generic kernel-based loss function. We illustrate our method on cirrhosis
prediction using a large volume of weakly-labeled images, namely radiological
low-confidence annotations, and small strongly-labeled (i.e., high-confidence)
datasets. The proposed model improves the classification AUC by 5% with respect
to a baseline model on our internal dataset, and by 26% on the public LIHC
dataset from the Cancer Genome Atlas. The code is available at:
https://github.com/Guerbet-AI/wsp-contrastive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiVOLO: Multi-input <span class="highlight-title">Transformer</span> for Age and Gender Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Kuprashevich, Irina Tolstykh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Age and gender recognition in the wild is a highly challenging task: apart
from the variability of conditions, pose complexities, and varying image
quality, there are cases where the face is partially or completely occluded. We
present MiVOLO (Multi Input VOLO), a straightforward approach for age and
gender estimation using the latest vision transformer. Our method integrates
both tasks into a unified dual input/output model, leveraging not only facial
information but also person image data. This improves the generalization
ability of our model and enables it to deliver satisfactory results even when
the face is not visible in the image. To evaluate our proposed model, we
conduct experiments on four popular benchmarks and achieve state-of-the-art
performance, while demonstrating real-time processing capabilities.
Additionally, we introduce a novel benchmark based on images from the Open
Images Dataset. The ground truth annotations for this benchmark have been
meticulously generated by human annotators, resulting in high accuracy answers
due to the smart aggregation of votes. Furthermore, we compare our model's age
recognition performance with human-level accuracy and demonstrate that it
significantly outperforms humans across a majority of age ranges. Finally, we
grant public access to our models, along with the code for validation and
inference. In addition, we provide extra annotations for used datasets and
introduce our new benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For the project repository, please visit:
  https://github.com/WildChlamydia/MiVOLO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Interpretable Heuristics for WalkSAT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannet Interian, Sara Bernardini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local search algorithms are well-known methods for solving large, hard
instances of the satisfiability problem (SAT). The performance of these
algorithms crucially depends on heuristics for setting noise parameters and
scoring variables. The optimal setting for these heuristics varies for
different instance distributions. In this paper, we present an approach for
learning effective variable scoring functions and noise parameters by using
reinforcement learning. We consider satisfiability problems from different
instance distributions and learn specialized heuristics for each of them. Our
experimental results show improvements with respect to both a WalkSAT baseline
and another local search learned heuristic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Memristor-Inspired Computation for Epileptiform Signals in Spheroids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iván Díez de los Ríos, John Wesley Ephraim, Gemma Palazzolo, Teresa Serrano-Gotarredona, Gabriella Panuccio, Bernabé Linares-Barranco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a memristor-inspired computational method for
obtaining a type of running spectrogram or fingerprint of epileptiform activity
generated by rodent hippocampal spheroids. It can be used to compute on the fly
and with low computational cost an alert-level signal for epileptiform events
onset. Here, we describe the computational method behind this fingerprint
technique and illustrate it using epileptiform events recorded from hippocampal
spheroids using a microelectrode array system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in 2023 IEEE 5th International Conference on Artificial
  Intelligence Circuits and Systems (AICAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Driven Engineering for Artificial Intelligence -- A Systematic
  Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Raedler, Luca Berardinelli, Karolin Winter, Abbas Rahimi, Stefanie Rinderle-Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: This study aims to investigate the existing body of knowledge in
the field of Model-Driven Engineering MDE in support of AI (MDE4AI) to sharpen
future research further and define the current state of the art.
  Method: We conducted a Systemic Literature Review (SLR), collecting papers
from five major databases resulting in 703 candidate studies, eventually
retaining 15 primary studies. Each primary study will be evaluated and
discussed with respect to the adoption of (1) MDE principles and practices and
(2) the phases of AI development support aligned with the stages of the
CRISP-DM methodology.
  Results: The study's findings show that the pillar concepts of MDE
(metamodel, concrete syntax and model transformation), are leveraged to define
domain-specific languages (DSL) explicitly addressing AI concerns. Different
MDE technologies are used, leveraging different language workbenches. The most
prominent AI-related concerns are training and modeling of the AI algorithm,
while minor emphasis is given to the time-consuming preparation of the data
sets. Early project phases that support interdisciplinary communication of
requirements, such as the CRISP-DM \textit{Business Understanding} phase, are
rarely reflected.
  Conclusion: The study found that the use of MDE for AI is still in its early
stages, and there is no single tool or method that is widely used.
Additionally, current approaches tend to focus on specific stages of
development rather than providing support for the entire development process.
As a result, the study suggests several research directions to further improve
the use of MDE for AI and to guide future research in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 1 figure, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-Automated Solution Approach Selection Tool for Any Use Case via
  Scopus and OpenAI: a Case Study for AI/ML in Oncology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deniz Kenan Kılıç, Alex Elkjær Vasegaard, Aurélien Desoeuvres, Peter Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's vast literature landscape, a manual review is very time-consuming.
To address this challenge, this paper proposes a semi-automated tool for
solution method review and selection. It caters to researchers, practitioners,
and decision-makers while serving as a benchmark for future work. The tool
comprises three modules: (1) paper selection and scoring, using a keyword
selection scheme to query Scopus API and compute relevancy; (2) solution method
extraction in papers utilizing OpenAI API; (3) sensitivity analysis and
post-analyzes. It reveals trends, relevant papers, and methods. AI in the
oncology case study and several use cases are presented with promising results,
comparing the tool to manual ground truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is under review in Expert Systems with Applications,
  Elsevier</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Surgery for One-shot Unlearning on Generative Model <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seohui Bae, Seoyoon Kim, Hyemin Jung, Woohyung Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent regulation on right-to-be-forgotten emerges tons of interest in
unlearning pre-trained machine learning models. While approximating a
straightforward yet expensive approach of retrain-from-scratch, recent machine
unlearning methods unlearn a sample by updating weights to remove its influence
on the weight parameters. In this paper, we introduce a simple yet effective
approach to remove a data influence on the deep generative model. Inspired by
works in multi-task learning, we propose to manipulate gradients to regularize
the interplay of influence among samples by projecting gradients onto the
normal plane of the gradients to be retained. Our work is agnostic to
statistics of the removal samples, outperforming existing baselines while
providing theoretical analysis for the first time in unlearning a generative
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in GenLaw Workshop @ ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Liu, Lu Xu, Jicong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fueled by deep learning, computer-aided diagnosis achieves huge advances.
However, out of controlled lab environments, algorithms could face multiple
challenges. Open set recognition (OSR), as an important one, states that
categories unseen in training could appear in testing. In medical fields, it
could derive from incompletely collected training datasets and the constantly
emerging new or rare diseases. OSR requires an algorithm to not only correctly
classify known classes, but also recognize unknown classes and forward them to
experts for further diagnosis. To tackle OSR, we assume that known classes
could densely occupy small parts of the embedding space and the remaining
sparse regions could be recognized as unknowns. Following it, we propose Open
Margin Cosine Loss (OMCL) unifying two mechanisms. The former, called Margin
Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing
intra-class compactness and inter-class separability, together with an adaptive
scaling factor to strengthen the generalization capacity. The latter, called
Open-Space Suppression (OSS), opens the classifier by recognizing sparse
embedding space as unknowns using proposed feature space descriptors. Besides,
since medical OSR is still a nascent field, two publicly available benchmark
datasets are proposed for comparison. Extensive ablation studies and feature
visualization demonstrate the effectiveness of each design. Compared with
state-of-the-art methods, MLAS achieves superior performances, measured by ACC,
AUROC, and OSCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-YOLOP: Quantization-aware You Only Look Once for Panoptic Driving
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Chih Chang, Wei-Cheng Lin, Pei-Shuo Wang, Sheng-Feng Yu, Yu-Chen Lu, Kuan-Cheng Lin, Kai-Chiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present an efficient and quantization-aware panoptic driving
perception model (Q- YOLOP) for object detection, drivable area segmentation,
and lane line segmentation, in the context of autonomous driving. Our model
employs the Efficient Layer Aggregation Network (ELAN) as its backbone and
task-specific heads for each task. We employ a four-stage training process that
includes pretraining on the BDD100K dataset, finetuning on both the BDD100K and
iVS datasets, and quantization-aware training (QAT) on BDD100K. During the
training process, we use powerful data augmentation techniques, such as random
perspective and mosaic, and train the model on a combination of the BDD100K and
iVS datasets. Both strategies enhance the model's generalization capabilities.
The proposed model achieves state-of-the-art performance with an mAP@0.5 of
0.622 for object detection and an mIoU of 0.612 for segmentation, while
maintaining low computational and memory requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QBitOpt: Fast and Accurate Bitwidth Reallocation during Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorn Peters, Marios Fournarakis, Markus Nagel, Mart van Baalen, Tijmen Blankevoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantizing neural networks is one of the most effective methods for achieving
efficient inference on mobile and embedded devices. In particular, mixed
precision quantized (MPQ) networks, whose layers can be quantized to different
bitwidths, achieve better task performance for the same resource constraint
compared to networks with homogeneous bitwidths. However, finding the optimal
bitwidth allocation is a challenging problem as the search space grows
exponentially with the number of layers in the network. In this paper, we
propose QBitOpt, a novel algorithm for updating bitwidths during
quantization-aware training (QAT). We formulate the bitwidth allocation problem
as a constraint optimization problem. By combining fast-to-compute
sensitivities with efficient solvers during QAT, QBitOpt can produce
mixed-precision networks with high task performance guaranteed to satisfy
strict resource constraints. This contrasts with existing mixed-precision
methods that learn bitwidths using gradients and cannot provide such
guarantees. We evaluate QBitOpt on ImageNet and confirm that we outperform
existing fixed and mixed-precision methods under average bitwidth constraints
commonly found in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preventing Errors in Person Detection: A Part-Based Self-Monitoring
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franziska Schwaiger, Andrea Matic, Karsten Roscher, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect learned objects regardless of their appearance is
crucial for autonomous systems in real-world applications. Especially for
detecting humans, which is often a fundamental task in safety-critical
applications, it is vital to prevent errors. To address this challenge, we
propose a self-monitoring framework that allows for the perception system to
perform plausibility checks at runtime. We show that by incorporating an
additional component for detecting human body parts, we are able to
significantly reduce the number of missed human detections by factors of up to
9 when compared to a baseline setup, which was trained only on holistic person
objects. Additionally, we found that training a model jointly on humans and
their body parts leads to a substantial reduction in false positive detections
by up to 50% compared to training on humans alone. We performed comprehensive
experiments on the publicly available datasets DensePose and Pascal VOC in
order to demonstrate the effectiveness of our framework. Code is available at
https://github.com/ FraunhoferIKS/smf-object-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 35th IEEE Intelligent Vehicles Symposium (IV 2023),
  9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAGC-A68: a space access graph <span class="highlight-title">dataset</span> for the classification of spaces
  and space elements in apartment buildings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Ziaee, Georg Suter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of building models for usable area, building safety, and energy
use requires accurate classification data of spaces and space elements. To
reduce input model preparation effort and errors, automated classification of
spaces and space elements is desirable. A barrier hindering the utilization of
Graph Deep Learning (GDL) methods to space function and space element
classification is a lack of suitable datasets. To bridge this gap, we introduce
a dataset, SAGC-A68, which comprises access graphs automatically generated from
68 digital 3D models of space layouts of apartment buildings. This graph-based
dataset is well-suited for developing GDL models for space function and space
element classification. To demonstrate the potential of the dataset, we employ
it to train and evaluate a graph attention network (GAT) that predicts 22 space
function and 6 space element classes. The dataset and code used in the
experiment are available online. https://doi.org/10.5281/zenodo.7805872,
https://github.com/A2Amir/SAGC-A68.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in proceedings of the 30th International Workshop on
  Intelligent Computing in Engineering, EG-ICE 2023, London, England.
  https://www.ucl.ac.uk/bartlett/construction/sites/bartlett_construction/files/sagc-a68_a_space_access_graph_dataset_for_the_classification_of_spaces_and_space_elements_in_apartment_buildings.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature
  Product Manifold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuc Nguyen-Van, Dung D. Le, The-Anh Ta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In graph representation learning, it is important that the complex geometric
structure of the input graph, e.g. hidden relations among nodes, is well
captured in embedding space. However, standard Euclidean embedding spaces have
a limited capacity in representing graphs of varying structures. A promising
candidate for the faithful embedding of data with varying structure is product
manifolds of component spaces of different geometries (spherical, hyperbolic,
or euclidean). In this paper, we take a closer look at the structure of product
manifold embedding spaces and argue that each component space in a product
contributes differently to expressing structures in the input graph, hence
should be weighted accordingly. This is different from previous works which
consider the roles of different components equally. We then propose
WEIGHTED-PM, a data-driven method for learning embedding of heterogeneous
graphs in weighted product manifolds. Our method utilizes the topological
information of the input graph to automatically determine the weight of each
component in product spaces. Extensive experiments on synthetic and real-world
graph datasets demonstrate that WEIGHTED-PM is capable of learning better graph
representations with lower geometric distortion from input data, and performs
better on multiple downstream tasks, such as word similarity learning, top-$k$
recommendation, and knowledge graph embedding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Factuality of Abstractive Summarization via Contrastive Reward
  Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern abstractive summarization models often generate summaries that contain
hallucinated or contradictory information. In this paper, we propose a simple
but effective contrastive learning framework that incorporates recent
developments in reward learning and factuality metrics. Empirical studies
demonstrate that the proposed framework enables summarization models to learn
from feedback of factuality metrics using contrastive reward learning, leading
to more factual summaries by human evaluations. This suggests that further
advances in learning and evaluation algorithms can feed directly into providing
more factual summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TrustNLP @ ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deductive Controller Synthesis for Probabilistic Hyperproperties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Andriushchenko, Ezio Bartocci, Milan Ceska, Francesco Pontiggia, Sarah Sallinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic hyperproperties specify quantitative relations between the
probabilities of reaching different target sets of states from different
initial sets of states. This class of behavioral properties is suitable for
capturing important security, privacy, and system-level requirements. We
propose a new approach to solve the controller synthesis problem for Markov
decision processes (MDPs) and probabilistic hyperproperties. Our specification
language builds on top of the logic HyperPCTL and enhances it with structural
constraints over the synthesized controllers. Our approach starts from a family
of controllers represented symbolically and defined over the same copy of an
MDP. We then introduce an abstraction refinement strategy that can relate
multiple computation trees and that we employ to prune the search space
deductively. The experimental evaluation demonstrates that the proposed
approach considerably outperforms HyperProb, a state-of-the-art SMT-based model
checking tool for HyperPCTL. Moreover, our approach is the first one that is
able to effectively combine probabilistic hyperproperties with additional
intra-controller constraints (e.g. partial observability) as well as
inter-controller constraints (e.g. agreements on a common action).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Driven Engineering Method to Support the Formalization of Machine
  Learning using SysML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Raedler, Juergen Mangler, Stefanie Rinderle-Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods: This work introduces a method supporting the collaborative
definition of machine learning tasks by leveraging model-based engineering in
the formalization of the systems modeling language SysML. The method supports
the identification and integration of various data sources, the required
definition of semantic connections between data attributes, and the definition
of data processing steps within the machine learning support.
  Results: By consolidating the knowledge of domain and machine learning
experts, a powerful tool to describe machine learning tasks by formalizing
knowledge using the systems modeling language SysML is introduced. The method
is evaluated based on two use cases, i.e., a smart weather system that allows
to predict weather forecasts based on sensor data, and a waste prevention case
for 3D printer filament that cancels the printing if the intended result cannot
be achieved (image processing). Further, a user study is conducted to gather
insights of potential users regarding perceived workload and usability of the
elaborated method.
  Conclusion: Integrating machine learning-specific properties in systems
engineering techniques allows non-data scientists to understand formalized
knowledge and define specific aspects of a machine learning problem, document
knowledge on the data, and to further support data scientists to use the
formalized knowledge as input for an implementation using (semi-) automatic
code generation. In this respect, this work contributes by consolidating
knowledge from various domains and therefore, fosters the integration of
machine learning in industry by involving several stakeholders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 24 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Modeling for Everyone: Exploring How Novices Approach
  Voice-Based 3D Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Desolda, Andrea Esposito, Florian Müller, Sebastian Feger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manufacturing tools like 3D printers have become accessible to the wider
society, making the promise of digital fabrication for everyone seemingly
reachable. While the actual manufacturing process is largely automated today,
users still require knowledge of complex design applications to produce
ready-designed objects and adapt them to their needs or design new objects from
scratch. To lower the barrier to the design and customization of personalized
3D models, we explored novice mental models in voice-based 3D modeling by
conducting a high-fidelity Wizard of Oz study with 22 participants. We
performed a thematic analysis of the collected data to understand how the
mental model of novices translates into voice-based 3D modeling. We conclude
with design implications for voice assistants. For example, they have to: deal
with vague, incomplete and wrong commands; provide a set of straightforward
commands to shape simple and composite objects; and offer different strategies
to select 3D objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at INTERACT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Novel Cognitive Diagnosis Models via Evolutionary
  Multi-Objective Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangshang Yang, Haiping Ma, Cheng Zhen, Ye Tian, Limiao Zhang, Yaochu Jin, Xingyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive diagnosis plays a vital role in modern intelligent education
platforms to reveal students' proficiency in knowledge concepts for subsequent
adaptive tasks. However, due to the requirement of high model interpretability,
existing manually designed cognitive diagnosis models hold too simple
architectures to meet the demand of current intelligent education systems,
where the bias of human design also limits the emergence of effective cognitive
diagnosis models. In this paper, we propose to automatically design novel
cognitive diagnosis models by evolutionary multi-objective neural architecture
search (NAS). Specifically, we observe existing models can be represented by a
general model handling three given types of inputs and thus first design an
expressive search space for the NAS task in cognitive diagnosis. Then, we
propose multi-objective genetic programming (MOGP) to explore the NAS task's
search space by maximizing model performance and interpretability. In the MOGP
design, each architecture is transformed into a tree architecture and encoded
by a tree for easy optimization, and a tailored genetic operation based on four
sub-genetic operations is devised to generate offspring effectively. Besides,
an initialization strategy is also suggested to accelerate the convergence by
evolving half of the population from existing models' variants. Experiments on
two real-world datasets demonstrate that the cognitive diagnosis models
searched by the proposed approach exhibit significantly better performance than
existing models and also hold as good interpretability as human-designed
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedDCT: A Dynamic Cross-Tier Federated Learning Scheme in Wireless
  Communication Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Youquan Xian, Chuanjian Yao, Xiaoyun Gan, Lianghaojie Zhou, Jianyong Jiang, Dongcheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid proliferation of Internet of Things (IoT) devices and the
growing concern for data privacy among the public, Federated Learning (FL) has
gained significant attention as a privacy-preserving machine learning paradigm.
FL enables the training of a global model among clients without exposing local
data. However, when a federated learning system runs on wireless communication
networks, limited wireless resources, heterogeneity of clients, and network
transmission failures affect its performance and accuracy. In this study, we
propose a novel dynamic cross-tier FL scheme, named FedDCT to increase training
accuracy and performance in wireless communication networks. We utilize a
tiering algorithm that dynamically divides clients into different tiers
according to specific indicators and assigns specific timeout thresholds to
each tier to reduce the training time required. To improve the accuracy of the
model without increasing the training time, we introduce a cross-tier client
selection algorithm that can effectively select the tiers and participants.
Simulation experiments show that our scheme can make the model converge faster
and achieve a higher accuracy in wireless communication networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft
  <span class="highlight-title">Prompt</span>ing and Calibrated Confidence Estimation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexin Zhang, Jiaxin Wen, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained language models achieve impressive results across many
tasks. However, recent works point out that pre-trained language models may
memorize a considerable fraction of their training data, leading to the privacy
risk of information leakage. In this paper, we propose a method named Ethicist
for targeted training data extraction through loss smoothed soft prompting and
calibrated confidence estimation, investigating how to recover the suffix in
the training data when given a prefix. To elicit memorization in the attacked
model, we tune soft prompt embeddings while keeping the model fixed. We further
propose a smoothing loss that smooths the loss distribution of the suffix
tokens to make it easier to sample the correct suffix. In order to select the
most probable suffix from a collection of sampled suffixes and estimate the
prediction confidence, we propose a calibrated confidence estimation method,
which normalizes the confidence of the generated suffixes with a local
estimation. We show that Ethicist significantly improves the extraction
performance on a recently proposed public benchmark. We also investigate
several factors influencing the data extraction performance, including decoding
strategy, model scale, prefix length, and suffix length. Our code is available
at https://github.com/thu-coai/Targeted-Data-Extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Long Paper (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advancements in End-to-End Autonomous Driving using Deep
  Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh Chib, Pravendra Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-End driving is a promising paradigm as it circumvents the drawbacks
associated with modular systems, such as their overwhelming complexity and
propensity for error propagation. Autonomous driving transcends conventional
traffic patterns by proactively recognizing critical events in advance,
ensuring passengers' safety and providing them with comfortable transportation,
particularly in highly stochastic and variable traffic settings. This paper
presents a comprehensive review of the End-to-End autonomous driving stack. It
provides a taxonomy of automated driving tasks wherein neural networks have
been employed in an End-to-End manner, encompassing the entire driving process
from perception to control, while addressing key challenges encountered in
real-world applications. Recent developments in End-to-End autonomous driving
are analyzed, and research is categorized based on underlying principles,
methodologies, and core functionality. These categories encompass sensorial
input, main and auxiliary output, learning approaches ranging from imitation to
reinforcement learning, and model evaluation techniques. The survey
incorporates a detailed discussion of the explainability and safety aspects.
Furthermore, it assesses the state-of-the-art, identifies challenges, and
explores future possibilities. We maintained the latest advancements and their
corresponding open-source implementations at
https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECS -- an Interactive Tool for Data Quality Assurance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Sieberichs, Simon Geerkens, Alexander Braun, Thomas Waschulzik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing capabilities of machine learning systems and their
potential use in safety-critical systems, ensuring high-quality data is
becoming increasingly important. In this paper we present a novel approach for
the assurance of data quality. For this purpose, the mathematical basics are
first discussed and the approach is presented using multiple examples. This
results in the detection of data points with potentially harmful properties for
the use in safety-critical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RLTF: Reinforcement Learning from Unit Test Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, these RL methods have only used
offline frameworks, limiting their exploration of new sample spaces.
Additionally, current approaches that utilize unit test signals are rather
simple, not accounting for specific error locations within the code. To address
these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test
Feedback, a novel online RL framework with unit test feedback of
multi-granularity for refining code LLMs. Our approach generates data in
real-time during training and simultaneously utilizes fine-grained feedback
signals to guide the model towards producing higher-quality code. Extensive
experiments show that RLTF achieves state-of-the-art performance on the APPS
and the MBPP benchmarks. Our code can be found at:
https://github.com/Zyq-scut/RLTF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Injecting Logical Constraints into Neural Networks via Straight-Through
  Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhun Yang, Joohyung Lee, Chiyoun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Injecting discrete logical constraints into neural network learning is one of
the main challenges in neuro-symbolic AI. We find that a
straight-through-estimator, a method introduced to train binary neural
networks, could effectively be applied to incorporate logical constraints into
neural network learning. More specifically, we design a systematic way to
represent discrete logical constraints as a loss function; minimizing this loss
using gradient descent via a straight-through-estimator updates the neural
network's weights in the direction that the binarized outputs satisfy the
logical constraints. The experimental results show that by leveraging GPUs and
batch training, this method scales significantly better than existing
neuro-symbolic methods that require heavy symbolic computation for computing
gradients. Also, we demonstrate that our method applies to different types of
neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer
labeled data by learning directly from known constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning as Computationally Constrained Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, Benjamin Van Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An agent that efficiently accumulates knowledge to develop increasingly
sophisticated skills over a long lifetime could advance the frontier of
artificial intelligence capabilities. The design of such agents, which remains
a long-standing challenge of artificial intelligence, is addressed by the
subject of continual learning. This monograph clarifies and formalizes concepts
of continual learning, introducing a framework and set of tools to stimulate
further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stroke Extraction of Chinese Character Based on Deep Structure
  Deformable Image Registration <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Li, Yahan Yu, Yi Yang, Guanghao Ren, Jian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke extraction of Chinese characters plays an important role in the field
of character recognition and generation. The most existing character stroke
extraction methods focus on image morphological features. These methods usually
lead to errors of cross strokes extraction and stroke matching due to rarely
using stroke semantics and prior information. In this paper, we propose a deep
learning-based character stroke extraction method that takes semantic features
and prior information of strokes into consideration. This method consists of
three parts: image registration-based stroke registration that establishes the
rough registration of the reference strokes and the target as prior
information; image semantic segmentation-based stroke segmentation that
preliminarily separates target strokes into seven categories; and
high-precision extraction of single strokes. In the stroke registration, we
propose a structure deformable image registration network to achieve
structure-deformable transformation while maintaining the stable morphology of
single strokes for character images with complex structures. In order to verify
the effectiveness of the method, we construct two datasets respectively for
calligraphy characters and regular handwriting characters. The experimental
results show that our method strongly outperforms the baselines. Code is
available at https://github.com/MengLi-l1/StrokeExtraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, published to AAAI-23 (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Miriam: Exploiting Elastic Kernels for Real-time Multi-DNN Inference on
  Edge GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihe Zhao, Neiwen Ling, Nan Guan, Guoliang Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications such as autonomous driving and augmented reality, require
the concurrent running of multiple deep neural networks (DNN) that poses
different levels of real-time performance requirements. However, coordinating
multiple DNN tasks with varying levels of criticality on edge GPUs remains an
area of limited study. Unlike server-level GPUs, edge GPUs are resource-limited
and lack hardware-level resource management mechanisms for avoiding resource
contention. Therefore, we propose Miriam, a contention-aware task coordination
framework for multi-DNN inference on edge GPU. Miriam consolidates two main
components, an elastic-kernel generator, and a runtime dynamic kernel
coordinator, to support mixed critical DNN inference. To evaluate Miriam, we
build a new DNN inference benchmark based on CUDA with diverse representative
DNN workloads. Experiments on two edge GPU platforms show that Miriam can
increase system throughput by 92% while only incurring less than 10\% latency
overhead for critical tasks, compared to state of art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Source-Aware Embedding Training on Heterogeneous Information Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsai Hor Chan, Chi Ho Wong, Jiajun Shen, Guosheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous information networks (HINs) have been extensively applied to
real-world tasks, such as recommendation systems, social networks, and citation
networks. While existing HIN representation learning methods can effectively
learn the semantic and structural features in the network, little awareness was
given to the distribution discrepancy of subgraphs within a single HIN.
However, we find that ignoring such distribution discrepancy among subgraphs
from multiple sources would hinder the effectiveness of graph embedding
learning algorithms. This motivates us to propose SUMSHINE (Scalable
Unsupervised Multi-Source Heterogeneous Information Network Embedding) -- a
scalable unsupervised framework to align the embedding distributions among
multiple sources of an HIN. Experimental results on real-world datasets in a
variety of downstream tasks validate the performance of our method over the
state-of-the-art heterogeneous information network embedding algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Data Intelligence 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Adversarial Robustness via Score-Based Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boya Zhang, Weijian Luo, Zhihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks have the potential to mislead deep neural network
classifiers by introducing slight perturbations. Developing algorithms that can
mitigate the effects of these attacks is crucial for ensuring the safe use of
artificial intelligence. Recent studies have suggested that score-based
diffusion models are effective in adversarial defenses. However, existing
diffusion-based defenses rely on the sequential simulation of the reversed
stochastic differential equations of diffusion models, which are
computationally inefficient and yield suboptimal results. In this paper, we
introduce a novel adversarial defense scheme named ScoreOpt, which optimizes
adversarial samples at test-time, towards original clean data in the direction
guided by score-based priors. We conduct comprehensive experiments on multiple
datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results
demonstrate that our approach outperforms existing adversarial defenses in
terms of both robustness performance and inference speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Generate Equitable Text in Dialogue from Biased Training
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Sicilia, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ingrained principles of fairness in a dialogue system's decision-making
process and generated responses are crucial for user engagement, satisfaction,
and task achievement. Absence of equitable and inclusive principles can hinder
the formation of common ground, which in turn negatively impacts the overall
performance of the system. For example, misusing pronouns in a user interaction
may cause ambiguity about the intended subject. Yet, there is no comprehensive
study of equitable text generation in dialogue. Aptly, in this work, we use
theories of computational learning to study this problem. We provide formal
definitions of equity in text generation, and further, prove formal connections
between learning human-likeness and learning equity: algorithms for improving
equity ultimately reduce to algorithms for improving human-likeness (on
augmented data). With this insight, we also formulate reasonable conditions
under which text generation algorithms can learn to generate equitable text
without any modifications to the biased training data on which they learn. To
exemplify our theory in practice, we look at a group of algorithms for the
GuessWhat?! visual dialogue game and, using this example, test our theory
empirically. Our theory accurately predicts relative-performance of multiple
algorithms in generating equitable text as measured by both human and automated
evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Demand-Driven Perspective on Generative Audio AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangshin Oh, Minsung Kang, Hyeongi Moon, Keunwoo Choi, Ben Sangbae Chon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve successful deployment of AI research, it is crucial to understand
the demands of the industry. In this paper, we present the results of a survey
conducted with professional audio engineers, in order to determine research
priorities and define various research tasks. We also summarize the current
challenges in audio quality and controllability based on the survey. Our
analysis emphasizes that the availability of datasets is currently the main
bottleneck for achieving high-quality audio generation. Finally, we suggest
potential solutions for some revealed issues with empirical evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing Graph ODE for Learning Complex System Dynamics across
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Huang, Yizhou Sun, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning multi-agent system dynamics has been extensively studied for various
real-world applications, such as molecular dynamics in biology. Most of the
existing models are built to learn single system dynamics from observed
historical data and predict the future trajectory. In practice, however, we
might observe multiple systems that are generated across different
environments, which differ in latent exogenous factors such as temperature and
gravity. One simple solution is to learn multiple environment-specific models,
but it fails to exploit the potential commonalities among the dynamics across
environments and offers poor prediction results where per-environment data is
sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary
Differential Equations), a machine learning framework for learning continuous
multi-agent system dynamics across environments. Our model learns system
dynamics using neural ordinary differential equations (ODE) parameterized by
Graph Neural Networks (GNNs) to capture the continuous interaction among
agents. We achieve the model generalization by assuming the dynamics across
different environments are governed by common physics laws that can be captured
via learning a shared ODE function. The distinct latent exogenous factors
learned for each environment are incorporated into the ODE function to account
for their differences. To improve model performance, we additionally design two
regularization losses to (1) enforce the orthogonality between the learned
initial states and exogenous factors via mutual information minimization; and
(2) reduce the temporal variance of learned exogenous factors within the same
system via contrastive learning. Experiments over various physical simulations
show that our model can accurately predict system dynamics, especially in the
long range, and can generalize well to new systems with few observations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Solve Constraint Satisfaction Problems with Recurrent
  <span class="highlight-title">Transformer</span> <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhun Yang, Adam Ishay, Joohyung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constraint satisfaction problems (CSPs) are about finding values of variables
that satisfy the given constraints. We show that Transformer extended with
recurrence is a viable approach to learning to solve CSPs in an end-to-end
manner, having clear advantages over state-of-the-art methods such as Graph
Neural Networks, SATNet, and some neuro-symbolic models. With the ability of
Transformer to handle visual input, the proposed Recurrent Transformer can
straightforwardly be applied to visual constraint reasoning problems while
successfully addressing the symbol grounding problem. We also show how to
leverage deductive knowledge of discrete constraints in the Transformer's
inductive learning to achieve sample-efficient learning and semi-supervised
learning for CSPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages. The Eleventh International Conference on Learning
  Representations (ICLR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic
  Strategies <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubens O. Moraes, David S. Aleixo, Lucas N. Ferreira, Levi H. S. Lelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Local Learner (2L), an algorithm for providing a set of
reference strategies to guide the search for programmatic strategies in
two-player zero-sum games. Previous learning algorithms, such as Iterated Best
Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be
computationally expensive or miss important information for guiding search
algorithms. 2L actively selects a set of reference strategies to improve the
search signal. We empirically demonstrate the advantages of our approach while
guiding a local search algorithm for synthesizing strategies in three games,
including MicroRTS, a challenging real-time strategy game. Results show that 2L
learns reference strategies that provide a stronger search signal than IBR, FP,
and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L
outperformed the winners of the two latest MicroRTS competitions, which were
programmatic strategies written by human programmers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Joint Conference on Artificial Intelligence (IJCAI)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring and Mitigating Interference in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Liu, Han Wang, Ruo Yu Tao, Khurram Javed, Adam White, Martha White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic interference is common in many network-based learning systems,
and many proposals exist for mitigating it. Before overcoming interference we
must understand it better. In this work, we provide a definition and novel
measure of interference for value-based reinforcement learning methods such as
Fitted Q-Iteration and DQN. We systematically evaluate our measure of
interference, showing that it correlates with instability in control
performance, across a variety of network architectures. Our new interference
measure allows us to ask novel scientific questions about commonly used deep
learning architectures and study learning algorithms which mitigate
interference. Lastly, we outline a class of algorithms which we call
online-aware that are designed to mitigate interference, and show they do
reduce interference according to our measure and that they improve stability
and performance in several classic control environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Conference on Lifelong Learning Agents (CoLLAs) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fed-C<span class="highlight-title">Prompt</span>: Contrastive <span class="highlight-title">Prompt</span> for Rehearsal-Free Federated Continual
  Learning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Bagwe, Xiaoyong Yuan, Miao Pan, Lan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated continual learning (FCL) learns incremental tasks over time from
confidential datasets distributed across clients. This paper focuses on
rehearsal-free FCL, which has severe forgetting issues when learning new tasks
due to the lack of access to historical task data. To address this issue, we
propose Fed-CPrompt based on prompt learning techniques to obtain task-specific
prompts in a communication-efficient way. Fed-CPrompt introduces two key
components, asynchronous prompt learning, and contrastive continual loss, to
handle asynchronous task arrival and heterogeneous data distributions in FCL,
respectively. Extensive experiments demonstrate the effectiveness of
Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by FL-ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Detection of Gait Events and Travel Distance Using Waist-worn
  Accelerometers Across a Typical Range of Walking and Running Speeds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albara Ah Ramli, Xin Liu, Kelly Berndt, Chen-Nee Chuah, Erica Goude, Lynea B. Kaethler, Amanda Lopez, Alina Nicorici, Corey Owens, David Rodriguez, Jane Wang, Daniel Aranki, Craig M. McDonald, Erik K. Henricson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Estimation of temporospatial clinical features of gait (CFs),
such as step count and length, step duration, step frequency, gait speed and
distance traveled is an important component of community-based mobility
evaluation using wearable accelerometers. However, challenges arising from
device complexity and availability, cost and analytical methodology have
limited widespread application of such tools. Research Question: Can
accelerometer data from commercially-available smartphones be used to extract
gait CFs across a broad range of attainable gait velocities in children with
Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using
machine learning (ML)-based methods Methods: Fifteen children with DMD and 15
TDs underwent supervised clinical testing across a range of gait speeds using
10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT)
and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer
at the waist near the body's center of mass. Gait CFs were extracted from the
accelerometer data using a multi-step machine learning-based process and
results were compared to ground-truth observation data. Results: Model
predictions vs. observed values for step counts, distance traveled, and step
length showed a strong correlation (Pearson's r = -0.9929 to 0.9986, p<0.0001).
The estimates demonstrated a mean (SD) percentage error of 1.49% (7.04%) for
step counts, 1.18% (9.91%) for distance traveled, and 0.37% (7.52%) for step
length compared to ground truth observations for the combined 6MWT, 100MRW, and
FW tasks. Significance: The study findings indicate that a single accelerometer
placed near the body's center of mass can accurately measure CFs across
different gait speeds in both TD and DMD peers, suggesting that there is
potential for accurately measuring CFs in the community with consumer-level
smartphones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification
  of Top-k Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjay Kariyappa, Leonidas Tsepenekas, Freddy Lécué, Daniele Magazzeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The SHAP framework provides a principled method to explain the predictions of
a model by computing feature importance. Motivated by applications in finance,
we introduce the Top-k Identification Problem (TkIP), where the objective is to
identify the k features with the highest SHAP values. While any method to
compute SHAP values with uncertainty estimates (such as KernelSHAP and
SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample
inefficient. The goal of our work is to improve the sample efficiency of
existing methods in the context of solving TkIP. Our key insight is that TkIP
can be framed as an Explore-m problem--a well-studied problem related to
multi-armed bandits (MAB). This connection enables us to improve sample
efficiency by leveraging two techniques from the MAB literature: (1) a better
stopping-condition (to stop sampling) that identifies when PAC (Probably
Approximately Correct) guarantees have been met and (2) a greedy sampling
scheme that judiciously allocates samples between different features. By
adopting these methods we develop KernelSHAP@k and SamplingSHAP@k to
efficiently solve TkIP, offering an average improvement of $5\times$ in
sample-efficiency and runtime across most common credit related datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksei Sorokin, Xinran Zhu, Eric Hans Lee, Bolong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient boosted trees (GBTs) are ubiquitous models used by researchers,
machine learning (ML) practitioners, and data scientists because of their
robust performance, interpretable behavior, and ease-of-use. One critical
challenge in training GBTs is the tuning of their hyperparameters. In practice,
selecting these hyperparameters is often done manually. Recently, the ML
community has advocated for tuning hyperparameters through black-box
optimization and developed state-of-the-art systems to do so. However, applying
such systems to tune GBTs suffers from two drawbacks. First, these systems are
not \textit{model-aware}, rather they are designed to apply to a
\textit{generic} model; this leaves significant optimization performance on the
table. Second, using these systems requires \textit{domain knowledge} such as
the choice of hyperparameter search space, which is an antithesis to the
automatic experimentation that black-box optimization aims to provide. In this
paper, we present SigOpt Mulch, a model-aware hyperparameter tuning system
specifically designed for automated tuning of GBTs that provides two
improvements over existing systems. First, Mulch leverages powerful techniques
in metalearning and multifidelity optimization to perform model-aware
hyperparameter optimization. Second, it automates the process of learning
performant hyperparameters by making intelligent decisions about the
optimization search space, thus reducing the need for user domain knowledge.
These innovations allow Mulch to identify good GBT hyperparameters far more
efficiently -- and in a more seamless and user-friendly way -- than existing
black-box hyperparameter tuning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamics of Temporal Difference Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blake Bordelon, Paul Masset, Henry Kuo, Cengiz Pehlevan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning has been successful across several applications in
which agents have to learn to act in environments with sparse feedback.
However, despite this empirical success there is still a lack of theoretical
understanding of how the parameters of reinforcement learning models and the
features used to represent states interact to control the dynamics of learning.
In this work, we use concepts from statistical physics, to study the typical
case learning curves for temporal difference learning of a value function with
linear function approximators. Our theory is derived under a Gaussian
equivalence hypothesis where averages over the random trajectories are replaced
with temporally correlated Gaussian feature averages and we validate our
assumptions on small scale Markov Decision Processes. We find that the
stochastic semi-gradient noise due to subsampling the space of possible
episodes leads to significant plateaus in the value error, unlike in
traditional gradient descent dynamics. We study how learning dynamics and
plateaus depend on feature structure, learning rate, discount factor, and
reward function. We then analyze how strategies like learning rate annealing
and reward shaping can favorably alter learning dynamics and plateaus. To
conclude, our work introduces new tools to open a new direction towards
developing a theory of learning dynamics in reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unmasking the giant: A comprehensive evaluation of Chat<span class="highlight-title">GPT</span>'s proficiency
  in coding algorithms and data structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayed Erfan Arefin, Tasnia Ashrafi Heya, Hasan Al-Qudah, Ynes Ineza, Abdul Serwadda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformative influence of Large Language Models (LLMs) is profoundly
reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT
distinguishes itself within these models, demonstrating remarkable performance
in multi-turn conversations and exhibiting code proficiency across an array of
languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's
coding capabilities based on what is to date the largest catalog of coding
challenges. Our focus is on the python programming language and problems
centered on data structures and algorithms, two topics at the very foundations
of Computer Science. We evaluate ChatGPT for its ability to generate correct
solutions to the problems fed to it, its code quality, and nature of run-time
errors thrown by its code. Where ChatGPT code successfully executes, but fails
to solve the problem at hand, we look into patterns in the test cases passed in
order to gain some insights into how wrong ChatGPT code is in these kinds of
situations. To infer whether ChatGPT might have directly memorized some of the
data that was used to train it, we methodically design an experiment to
investigate this phenomena. Making comparisons with human performance whenever
feasible, we investigate all the above questions from the context of both its
underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics
within the main topics, and on problems having varying degrees of difficulty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimedia Generative Script Learning for Task Planning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyun Wang, Manling Li, Hou Pong Chan, Lifu Huang, Julia Hockenmaier, Girish Chowdhary, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-oriented generative script learning aims to generate subsequent steps to
reach a particular goal, which is an essential task to assist robots or humans
in performing stereotypical activities. An important aspect of this process is
the ability to capture historical states visually, which provides detailed
information that is not covered by text and will guide subsequent steps.
Therefore, we propose a new task, Multimedia Generative Script Learning, to
generate subsequent steps by tracking historical states in both text and vision
modalities, as well as presenting the first benchmark containing 5,652 tasks
and 79,089 multimedia steps. This task is challenging in three aspects: the
multimedia challenge of capturing the visual states in images, the induction
challenge of performing unseen tasks, and the diversity challenge of covering
different information in individual steps. We propose to encode visual state
changes through a selective multimedia encoder to address the multimedia
challenge, transfer knowledge from previously observed tasks using a
retrieval-augmented decoder to overcome the induction challenge, and further
present distinct information at each step by optimizing a diversity-oriented
contrastive learning objective. We define metrics to evaluate both generation
and inductive quality. Experiment results demonstrate that our approach
significantly outperforms strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, Accepted by Findings of the Association for Computational
  Linguistics: ACL 2023, Code and Resources at
  https://github.com/EagleW/Multimedia-Generative-Script-Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Testing of Detection Tools for AI-Generated Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tomáš Foltýnek, Jean Guerrero-Dib, Olumide Popoola, Petr Šigut, Lorna Waddington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative pre-trained transformer large language models
have emphasised the potential risks of unfair use of artificial intelligence
(AI) generated content in an academic environment and intensified efforts in
searching for solutions to detect such content. The paper examines the general
functionality of detection tools for artificial intelligence generated text and
evaluates them based on accuracy and error type analysis. Specifically, the
study seeks to answer research questions about whether existing detection tools
can reliably differentiate between human-written text and ChatGPT-generated
text, and whether machine translation and content obfuscation techniques affect
the detection of AI-generated text. The research covers 12 publicly available
tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely
used in the academic setting. The researchers conclude that the available
detection tools are neither accurate nor reliable and have a main bias towards
classifying the output as human-written rather than detecting AI-generated
text. Furthermore, content obfuscation techniques significantly worsen the
performance of tools. The study makes several significant contributions. First,
it summarises up-to-date similar scientific and non-scientific efforts in the
field. Second, it presents the result of one of the most comprehensive tests
conducted so far, based on a rigorous research methodology, an original
document set, and a broad coverage of tools. Third, it discusses the
implications and drawbacks of using detection tools for AI-generated text in
academic settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 13 figures and 10 tables, and an appendix with 18 figures.
  Submitted to the International Journal for Educational Integrity</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GFlowNet Foundations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.09266v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.09266v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Yoshua Bengio</span>, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, Emmanuel Bengio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (GFlowNets) have been introduced as a method to
sample a diverse set of candidates in an active learning context, with a
training objective that makes them approximately sample in proportion to a
given reward function. In this paper, we show a number of additional
theoretical properties of GFlowNets. They can be used to estimate joint
probability distributions and the corresponding marginal distributions where
some variables are unspecified and, of particular interest, can represent
distributions over composite objects like sets and graphs. GFlowNets amortize
the work typically done by computationally expensive MCMC methods in a single
but trained generative pass. They could also be used to estimate partition
functions and free energies, conditional probabilities of supersets
(supergraphs) given a subset (subgraph), as well as marginal distributions over
all supersets (supergraphs) of a given set (graph). We introduce variations
enabling the estimation of entropy and mutual information, sampling from a
Pareto frontier, connections to reward-maximizing policies, and extensions to
stochastic environments, continuous actions and modular energy functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DORA: Exploring Outlier Representations in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04530v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04530v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Bykov, Mayukh Deb, Dennis Grinwald, Klaus-Robert Müller, Marina M. -C. Höhne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) excel at learning complex abstractions within
their internal representations. However, the concepts they learn remain opaque,
a problem that becomes particularly acute when models unintentionally learn
spurious correlations. In this work, we present DORA (Data-agnOstic
Representation Analysis), the first data-agnostic framework for analyzing the
representational space of DNNs. Central to our framework is the proposed
Extreme-Activation (EA) distance measure, which assesses similarities between
representations by analyzing their activation patterns on data points that
cause the highest level of activation. As spurious correlations often manifest
in features of data that are anomalous to the desired task, such as watermarks
or artifacts, we demonstrate that internal representations capable of detecting
such artifactual concepts can be found by analyzing relationships within neural
representations. We validate the EA metric quantitatively, demonstrating its
effectiveness both in controlled scenarios and real-world applications.
Finally, we provide practical examples from popular Computer Vision models to
illustrate that representations identified as outliers using the EA metric
often correspond to undesired and spurious concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Bayes Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Emtiyaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational Bayes is a popular method for approximate inference but its
derivation can be cumbersome. To simplify the process, we give a 3-step recipe
to identify the posterior form by explicitly looking for linearity with respect
to expectations of well-known distributions. We can then directly write the
update by simply ``reading-off'' the terms in front of those expectations. The
recipe makes the derivation easier, faster, shorter, and more general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models, Natural Language Processing, Domain
  Specialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18703v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18703v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Jian Pei, Carl Yang, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced the field of natural
language processing (NLP), providing a highly useful, task-agnostic foundation
for a wide range of applications. However, directly applying LLMs to solve
sophisticated problems in specific domains meets many hurdles, caused by the
heterogeneity of domain data, the sophistication of domain knowledge, the
uniqueness of domain objectives, and the diversity of the constraints (e.g.,
various social norms, cultural conformity, religious beliefs, and ethical
standards in the domain applications). Domain specification techniques are key
to make large language models disruptive in many applications. Specifically, to
solve these hurdles, there has been a notable increase in research and
practices conducted in recent years on the domain specialization of LLMs. This
emerging field of study, with its substantial potential for impact,
necessitates a comprehensive and systematic review to better summarize and
guide ongoing work in this area. In this article, we present a comprehensive
survey on domain specification techniques for large language models, an
emerging direction critical for large language model applications. First, we
propose a systematic taxonomy that categorizes the LLM domain-specialization
techniques based on the accessibility to LLMs and summarizes the framework for
all the subcategories as well as their relations and differences to each other.
Second, we present an extensive taxonomy of critical application domains that
can benefit dramatically from specialized LLMs, discussing their practical
significance and open challenges. Last, we offer our insights into the current
research status and future trends in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Noise as a Resource for Computation and Learning in Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16044v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16044v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gehua Ma, Rui Yan, Huajin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  -- A theoretical framework that subsumes conventional deterministic spiking
neural networks and surrogate gradients, facilitating more efficient and
effective employment of various neuromorphic hardware developments in
real-world applications.
  -- Scalable spiking neural models that incorporate noisy neuronal dynamics
for implicit regularization, improved robustness, and computational accounts of
biological neural computation, revealing that unreliable neural substrates
yield reliable computation and learning.
  Networks of spiking neurons underpin the extraordinary information-processing
capabilities of the brain and have emerged as pillar models in neuromorphic
intelligence. Despite extensive research on spiking neural networks (SNNs),
most are established on deterministic models. Integrating noise into SNNs leads
to biophysically more realistic neural dynamics and may benefit model
performance. This work presents the noisy spiking neural network (NSNN) and the
noise-driven learning rule (NDL) by introducing a spiking neuron model
incorporating noisy neuronal dynamics. Our approach shows how noise may serve
as a resource for computation and learning and theoretically provides a
framework for general SNNs. We show that our method exhibits competitive
performance and improved robustness against challenging perturbations than
deterministic SNNs and better reproduces probabilistic neural computation in
neural coding. This study offers a powerful and easy-to-use tool for machine
learning, neuromorphic intelligence practitioners, and computational
neuroscience researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Visual <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.12556v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.12556v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer, first applied to the field of natural language processing, is a
type of deep neural network mainly based on the self-attention mechanism.
Thanks to its strong representation capabilities, researchers are looking at
ways to apply transformer to computer vision tasks. In a variety of visual
benchmarks, transformer-based models perform similar to or better than other
types of networks such as convolutional and recurrent neural networks. Given
its high performance and less need for vision-specific inductive bias,
transformer is receiving more and more attention from the computer vision
community. In this paper, we review these vision transformer models by
categorizing them in different tasks and analyzing their advantages and
disadvantages. The main categories we explore include the backbone network,
high/mid-level vision, low-level vision, and video processing. We also include
efficient transformer methods for pushing transformer into real device-based
applications. Furthermore, we also take a brief look at the self-attention
mechanism in computer vision, as it is the base component in transformer.
Toward the end of this paper, we discuss the challenges and provide several
further research directions for vision transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TPAMI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Art Authentication with Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovica Schaerf, Carina Popovici, Eric Postma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Transformers, initially developed for language, have been
successfully applied to visual tasks. Vision Transformers have been shown to
push the state-of-the-art in a wide range of tasks, including image
classification, object detection, and semantic segmentation. While ample
research has shown promising results in art attribution and art authentication
tasks using Convolutional Neural Networks, this paper examines if the
superiority of Vision Transformers extends to art authentication, improving,
thus, the reliability of computer-based authentication of artworks. Using a
carefully compiled dataset of authentic paintings by Vincent van Gogh and two
contrast datasets, we compare the art authentication performances of Swin
Transformers with those of EfficientNet. Using a standard contrast set
containing imitations and proxies (works by painters with styles closely
related to van Gogh), we find that EfficientNet achieves the best performance
overall. With a contrast set that only consists of imitations, we find the Swin
Transformer to be superior to EfficientNet by achieving an authentication
accuracy of over 85%. These results lead us to conclude that Vision
Transformers represent a strong and promising contender in art authentication,
particularly in enhancing the computer-based ability to detect artistic
imitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Neural Computing and Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ (Un)reasonable Allure of Ante-hoc Interpretability for High-stakes
  Domains: Transparency Is Necessary but Insufficient for Comprehensibility <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kacper Sokol, Julia E. Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ante-hoc interpretability has become the holy grail of explainable artificial
intelligence for high-stakes domains such as healthcare; however, this notion
is elusive, lacks a widely-accepted definition and depends on the operational
context. It can refer to predictive models whose structure adheres to
domain-specific constraints, or ones that are inherently transparent. The
latter conceptualisation assumes observers who judge this quality, whereas the
former presupposes them to have technical and domain expertise (thus alienating
other groups of explainees). Additionally, the distinction between ante-hoc
interpretability and the less desirable post-hoc explainability, which refers
to methods that construct a separate explanatory model, is vague given that
transparent predictive models may still require (post-)processing to yield
suitable explanatory insights. Ante-hoc interpretability is thus an overloaded
concept that comprises a range of implicit properties, which we unpack in this
paper to better understand what is needed for its safe adoption across
high-stakes domains. To this end, we outline modelling and explaining
desiderata that allow us to navigate its distinct realisations in view of the
envisaged application and audience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Interpretable ML in Healthcare at 2023 International
  Conference on Machine Learning (ICML)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-TTSG: Denoising probabilistic integrated speech and gesture
  synthesis <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With read-aloud speech synthesis achieving high naturalness scores, there is
a growing research interest in synthesising spontaneous speech. However, human
spontaneous face-to-face conversation has both spoken and non-verbal aspects
(here, co-speech gestures). Only recently has research begun to explore the
benefits of jointly synthesising these two modalities in a single system. The
previous state of the art used non-probabilistic methods, which fail to capture
the variability of human speech and motion, and risk producing oversmoothing
artefacts and sub-optimal synthesis quality. We present the first
diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to
synthesise speech and gestures together. Our method can be trained on small
datasets from scratch. Furthermore, we describe a set of careful uni- and
multi-modal subjective tests for evaluating integrated speech and gesture
synthesis systems, and use them to validate our proposed approach. For
synthesised examples please see https://shivammehta25.github.io/Diff-TTSG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, Accepted at ISCA Speech Synthesis Workshop (SSW)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial
  Understanding with Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Fu, Yilun Du, Kurran Singh, Joshua B. Tenenbaum, John J. Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and
illustrate how it supports object SLAM for consistent spatial understanding
with long-term scene changes. NeuSE is a set of latent object embeddings
created from partial object observations. It serves as a compact point cloud
surrogate for complete object models, encoding full shape information while
transforming SE(3)-equivariantly in tandem with the object in the physical
world. With NeuSE, relative frame transforms can be directly derived from
inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape
and pose characterization, can operate independently or in conjunction with
typical SLAM systems. It directly infers SE(3) camera pose constraints that are
compatible with general SLAM pose graph optimization, while also maintaining a
lightweight object-centric map that adapts to real-world changes. Our approach
is evaluated on synthetic and real-world sequences featuring changed objects
and shows improved localization accuracy and change-aware mapping capability,
when working either standalone or jointly with a common SLAM pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages and 12 figures. Accepted to RSS 2023. Project webpage:
  https://neuse-slam.github.io/neuse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Learning and Solving Infinite Games with an ERM Oracle <span class="chip">COLT2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelos Assos, Idan Attias, Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While ERM suffices to attain near-optimal generalization error in the
stochastic learning setting, this is not known to be the case in the online
learning setting, where algorithms for general concept classes rely on
computationally inefficient oracles such as the Standard Optimal Algorithm
(SOA). In this work, we propose an algorithm for online binary classification
setting that relies solely on ERM oracle calls, and show that it has finite
regret in the realizable setting and sublinearly growing regret in the agnostic
setting. We bound the regret in terms of the Littlestone and threshold
dimensions of the underlying concept class.
  We obtain similar results for nonparametric games, where the ERM oracle can
be interpreted as a best response oracle, finding the best response of a player
to a given history of play of the other players. In this setting, we provide
learning algorithms that only rely on best response oracles and converge to
approximate-minimax equilibria in two-player zero-sum games and approximate
coarse correlated equilibria in multi-player general-sum games, as long as the
game has a bounded fat-threshold dimension. Our algorithms apply to both
binary-valued and real-valued games and can be viewed as providing
justification for the wide use of double oracle and multiple oracle algorithms
in the practice of solving large games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In COLT2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Query Approximation Algorithms for Non-monotone Submodular
  Maximization under Knapsack Constraint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canh V. Pham, Tan D. Tran, Dung T. K. Ha, My T. Thai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work, for the first time, introduces two constant factor approximation
algorithms with linear query complexity for non-monotone submodular
maximization over a ground set of size $n$ subject to a knapsack constraint,
$\mathsf{DLA}$ and $\mathsf{RLA}$. $\mathsf{DLA}$ is a deterministic algorithm
that provides an approximation factor of $6+\epsilon$ while $\mathsf{RLA}$ is a
randomized algorithm with an approximation factor of $4+\epsilon$. Both run in
$O(n \log(1/\epsilon)/\epsilon)$ query complexity. The key idea to obtain a
constant approximation ratio with linear query lies in: (1) dividing the ground
set into two appropriate subsets to find the near-optimal solution over these
subsets with linear queries, and (2) combining a threshold greedy with
properties of two disjoint sets or a random selection process to improve
solution quality. In addition to the theoretical analysis, we have evaluated
our proposed solutions with three applications: Revenue Maximization, Image
Summarization, and Maximum Weighted Cut, showing that our algorithms not only
return comparative results to state-of-the-art algorithms but also require
significantly fewer queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inductive Relation Prediction from Relational Paths and Context with
  Hierarchical <span class="highlight-title">Transformer</span>s <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00215v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00215v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaang Li, Quan Wang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation prediction on knowledge graphs (KGs) is a key research topic.
Dominant embedding-based methods mainly focus on the transductive setting and
lack the inductive ability to generalize to new entities for inference.
Existing methods for inductive reasoning mostly mine the connections between
entities, i.e., relational paths, without considering the nature of head and
tail entities contained in the relational context. This paper proposes a novel
method that captures both connections between entities and the intrinsic nature
of entities, by simultaneously aggregating RElational Paths and cOntext with a
unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely
on relation semantics and can naturally generalize to the fully-inductive
setting, where KGs for training and inference have no common entities. In the
experiments, REPORT performs consistently better than all baselines on almost
all the eight version subsets of two fully-inductive datasets. Moreover. REPORT
is interpretable by providing each element's contribution to the prediction
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023 (Oral). The code is available at:
  https://github.com/JiaangL/REPORT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Value of Out-of-Distribution Data <span class="chip">ECCV
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10967v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10967v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin De Silva, Rahul Ramesh, Carey E. Priebe, Pratik Chaudhari, Joshua T. Vogelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We expect the generalization error to improve with more samples from a
similar task, and to deteriorate with more samples from an out-of-distribution
(OOD) task. In this work, we show a counter-intuitive phenomenon: the
generalization error of a task can be a non-monotonic function of the number of
OOD samples. As the number of OOD samples increases, the generalization error
on the target task improves before deteriorating beyond a threshold. In other
words, there is value in training on small amounts of OOD data. We use Fisher's
Linear Discriminant on synthetic datasets and deep networks on computer vision
benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate
and analyze this phenomenon. In the idealistic setting where we know which
samples are OOD, we show that these non-monotonic trends can be exploited using
an appropriately weighted objective of the target and OOD empirical risk. While
its practical utility is limited, this does suggest that if we can detect OOD
samples, then there may be ways to benefit from them. When we do not know which
samples are OOD, we show how a number of go-to strategies such as
data-augmentation, hyper-parameter optimization, and pre-training are not
enough to ensure that the target generalization error does not deteriorate with
the number of OOD samples in the dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Previous versions of this work have been presented at the
  Out-of-Distribution Generalization in Computer Vision (OOD-CV) Workshop (ECCV
  2022) and the Workshop on Distribution Shifts (NeurIPS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Robustness of Bayesian Neural Networks to Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Bortolussi, Ginevra Carbone, Luca Laurenti, Andrea Patane, Guido Sanguinetti, Matthew Wicker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vulnerability to adversarial attacks is one of the principal hurdles to the
adoption of deep learning in safety-critical applications. Despite significant
efforts, both practical and theoretical, training deep learning models robust
to adversarial attacks is still an open problem. In this paper, we analyse the
geometry of adversarial attacks in the large-data, overparameterized limit for
Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to
gradient-based attacks arises as a result of degeneracy in the data
distribution, i.e., when the data lies on a lower-dimensional submanifold of
the ambient space. As a direct consequence, we demonstrate that in this limit
BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we
prove that the expected gradient of the loss with respect to the BNN posterior
distribution is vanishing, even when each neural network sampled from the
posterior is vulnerable to gradient-based attacks. Experimental results on the
MNIST, Fashion MNIST, and half moons datasets, representing the finite data
regime, with BNNs trained with Hamiltonian Monte Carlo and Variational
Inference, support this line of arguments, showing that BNNs can display both
high accuracy on clean data and robustness to both gradient-based and
gradient-free based adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2002.04359</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Conditional Generative Modeling all you need for Decision-Making? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15657v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15657v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://anuragajay.github.io/decision-diffuser/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to extrapolate using continued fractions: Predicting the
  critical temperature of superconductor materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.03774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.03774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Moscato, Mohammad Nazmul Haque, Kevin Huang, Julia Sloan, Jon C. de Oliveira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of Artificial Intelligence (AI) and Machine Learning (ML), the
approximation of unknown target functions $y=f(\mathbf{x})$ using limited
instances $S={(\mathbf{x^{(i)}},y^{(i)})}$, where $\mathbf{x^{(i)}} \in D$ and
$D$ represents the domain of interest, is a common objective. We refer to $S$
as the training set and aim to identify a low-complexity mathematical model
that can effectively approximate this target function for new instances
$\mathbf{x}$. Consequently, the model's generalization ability is evaluated on
a separate set $T=\{\mathbf{x^{(j)}}\} \subset D$, where $T \neq S$, frequently
with $T \cap S = \emptyset$, to assess its performance beyond the training set.
However, certain applications require accurate approximation not only within
the original domain $D$ but also in an extended domain $D'$ that encompasses
$D$. This becomes particularly relevant in scenarios involving the design of
new structures, where minimizing errors in approximations is crucial. For
example, when developing new materials through data-driven approaches, the
AI/ML system can provide valuable insights to guide the design process by
serving as a surrogate function. Consequently, the learned model can be
employed to facilitate the design of new laboratory experiments. In this paper,
we propose a method for multivariate regression based on iterative fitting of a
continued fraction, incorporating additive spline models. We compare the
performance of our method with established techniques, including AdaBoost,
Kernel Ridge, Linear Regression, Lasso Lars, Linear Support Vector Regression,
Multi-Layer Perceptrons, Random Forests, Stochastic Gradient Descent, and
XGBoost. To evaluate these methods, we focus on an important problem in the
field: predicting the critical temperature of superconductors based on
physical-chemical characteristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Algorithms ( ISSN: 1999-4893 )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Potential of Large Language Models (LLMs) in Learning on
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fix some minor typos and errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QI2 -- an Interactive Tool for Data Quality Assurance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Geerkens, Christian Sieberichs, Alexander Braun, Thomas Waschulzik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of high data quality is increasing with the growing impact and
distribution of ML systems and big data. Also the planned AI Act from the
European commission defines challenging legal requirements for data quality
especially for the market introduction of safety relevant ML systems. In this
paper we introduce a novel approach that supports the data quality assurance
process of multiple data quality aspects. This approach enables the
verification of quantitative data quality requirements. The concept and
benefits are introduced and explained on small example data sets. How the
method is applied is demonstrated on the well known MNIST data set based an
handwritten digits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Bias: Enhancing Image Classification by Improving Model
  Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raha Ahmadi, Mohammad Javad Rajabi, Mohammad Khalooie, Mohammad Sabokrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have demonstrated remarkable capabilities in learning
complex patterns and concepts from training data. However, recent findings
indicate that these models tend to rely heavily on simple and easily
discernible features present in the background of images rather than the main
concepts or objects they are intended to classify. This phenomenon poses a
challenge to image classifiers as the crucial elements of interest in images
may be overshadowed. In this paper, we propose a novel approach to address this
issue and improve the learning of main concepts by image classifiers. Our
central idea revolves around concurrently guiding the model's attention toward
the foreground during the classification task. By emphasizing the foreground,
which encapsulates the primary objects of interest, we aim to shift the focus
of the model away from the dominant influence of the background. To accomplish
this, we introduce a mechanism that encourages the model to allocate sufficient
attention to the foreground. We investigate various strategies, including
modifying the loss function or incorporating additional architectural
components, to enable the classifier to effectively capture the primary concept
within an image. Additionally, we explore the impact of different foreground
attention mechanisms on model performance and provide insights into their
effectiveness. Through extensive experimentation on benchmark datasets, we
demonstrate the efficacy of our proposed approach in improving the
classification accuracy of image classifiers. Our findings highlight the
importance of foreground attention in enhancing model understanding and
representation of the main concepts within images. The results of this study
contribute to advancing the field of image classification and provide valuable
insights for developing more robust and accurate deep-learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyang Xie, Jinbao Wang, Jiaqi Liu, Jiayi Lyu, Yong Liu, Chengjie Wang, Feng Zheng, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently many advanced algorithms have been
published, but their performance deviates greatly. We realize that the lack of
actual IM settings most probably hinders the development and usage of these
methods in real-world applications. As far as we know, IAD methods are not
evaluated systematically. As a result, this makes it difficult for researchers
to analyze them because they are designed for different or special cases. To
solve this problem, we first propose a uniform IM setting to assess how well
these algorithms perform, which includes several aspects, i.e., various levels
of supervision (unsupervised vs. semi-supervised), few-shot learning, continual
learning, noisy labels, memory usage, and inference speed. Moreover, we
skillfully build a comprehensive image anomaly detection benchmark (IM-IAD)
that includes 16 algorithms on 7 mainstream datasets with uniform settings. Our
extensive experiments (17,017 in total) provide in-depth insights for IAD
algorithm redesign or selection under the IM setting. Next, the proposed
benchmark IM-IAD gives challenges as well as directions for the future. To
foster reproducibility and accessibility, the source code of IM-IAD is uploaded
on the website, https://github.com/M-3LAB/IM-IAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Code Example Recommendations on Informal Documentation Using
  <span class="highlight-title">BERT</span> and Query-Aware LSH: A Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajjad Rahmani, AmirHossein Naghshzan, Latifa Guerrouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our research investigates the recommendation of code examples to aid software
developers, a practice that saves developers significant time by providing
ready-to-use code snippets. The focus of our study is Stack Overflow, a
commonly used resource for coding discussions and solutions, particularly in
the context of the Java programming language.
  We applied BERT, a powerful Large Language Model (LLM) that enables us to
transform code examples into numerical vectors by extracting their semantic
information. Once these numerical representations are prepared, we identify
Approximate Nearest Neighbors (ANN) using Locality-Sensitive Hashing (LSH). Our
research employed two variants of LSH: Random Hyperplane-based LSH and
Query-Aware LSH. We rigorously compared these two approaches across four
parameters: HitRate, Mean Reciprocal Rank (MRR), Average Execution Time, and
Relevance.
  Our study revealed that the Query-Aware (QA) approach showed superior
performance over the Random Hyperplane-based (RH) method. Specifically, it
exhibited a notable improvement of 20% to 35% in HitRate for query pairs
compared to the RH approach. Furthermore, the QA approach proved significantly
more time-efficient, with its speed in creating hashing tables and assigning
data samples to buckets being at least four times faster. It can return code
examples within milliseconds, whereas the RH approach typically requires
several seconds to recommend code examples. Due to the superior performance of
the QA approach, we tested it against PostFinder and FaCoY, the
state-of-the-art baselines. Our QA method showed comparable efficiency proving
its potential for effective code recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BayesFlow: Amortized Bayesian Workflows With Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan T Radev, Marvin Schmitt, Lukas Schumacher, Lasse Elsemüller, Valentin Pratz, Yannik Schälte, Ullrich Köthe, Paul-Christian Bürkner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Bayesian inference involves a mixture of computational techniques for
estimating, validating, and drawing conclusions from probabilistic models as
part of principled workflows for data analysis. Typical problems in Bayesian
workflows are the approximation of intractable posterior distributions for
diverse model types and the comparison of competing models of the same process
in terms of their complexity and predictive performance. This manuscript
introduces the Python library BayesFlow for simulation-based training of
established neural network architectures for amortized data compression and
inference. Amortized Bayesian inference, as implemented in BayesFlow, enables
users to train custom neural networks on model simulations and re-use these
networks for any subsequent application of the models. Since the trained
networks can perform inference almost instantaneously, the upfront neural
network training is quickly amortized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collective Privacy Recovery: Data-sharing Coordination via Decentralized
  Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Pournaras, Mark Christopher Ballandies, Stefano Bennati, Chien-fei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collective privacy loss becomes a colossal problem, an emergency for personal
freedoms and democracy. But, are we prepared to handle personal data as scarce
resource and collectively share data under the doctrine: as little as possible,
as much as necessary? We hypothesize a significant privacy recovery if a
population of individuals, the data collective, coordinates to share minimum
data for running online services with the required quality. Here we show how to
automate and scale-up complex collective arrangements for privacy recovery
using decentralized artificial intelligence. For this, we compare for first
time attitudinal, intrinsic, rewarded and coordinated data sharing in a
rigorous living-lab experiment of high realism involving >27,000 real data
disclosures. Using causal inference and cluster analysis, we differentiate
criteria predicting privacy and five key data-sharing behaviors. Strikingly,
data-sharing coordination proves to be a win-win for all: remarkable privacy
recovery for people with evident costs reduction for service providers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contains Supplementary Information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forming Trees with Treeformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nilay Patel, Jeffrey Flanigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human language is known to exhibit a nested, hierarchical structure, allowing
us to form complex sentences out of smaller pieces. However, many
state-of-the-art neural networks models such as Transformers have no explicit
hierarchical structure in its architecture -- that is, they don't have an
inductive bias toward hierarchical structure. Additionally, Transformers are
known to perform poorly on compositional generalization tasks which require
such structures. In this paper, we introduce Treeformer, a general-purpose
encoder module inspired by the CKY algorithm which learns a composition
operator and pooling function to construct hierarchical encodings for phrases
and sentences. Our extensive experiments demonstrate the benefits of
incorporating hierarchical structure into the Transformer and show significant
improvements in compositional generalization as well as in downstream tasks
such as machine translation, abstractive summarization, and various natural
language understanding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RANLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I2I: Initializing Adapters with Improvised Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas Srinivasan, Furong Jia, Mohammad Rostami, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2nd Conference on Lifelong Learning Agents (CoLLAs), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecallM: An Architecture for Temporal Context Understanding and Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Kynoch, Hugo Latapie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ideal long-term memory mechanism for Large Language Model (LLM) based
chatbots, would lay the foundation for continual learning, complex reasoning
and allow sequential and temporal dependencies to be learnt. Creating this type
of memory mechanism is an extremely challenging problem. In this paper we
explore different methods of achieving the effect of long-term memory. We
propose a new architecture focused on creating adaptable and updatable
long-term memory for AGI systems. We demonstrate through various experiments
the benefits of the RecallM architecture, particularly the improved temporal
understanding of knowledge it provides.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, Our code is publicly available online at:
  https://github.com/cisco-open/DeepVision/tree/main/recallm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClimaX: A foundation model for weather and climate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10343v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10343v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K. Gupta, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most state-of-the-art approaches for weather and climate modeling are based
on physics-informed numerical models of the atmosphere. These approaches aim to
model the non-linear dynamics and complex interactions between multiple
variables, which are challenging to approximate. Additionally, many such
numerical models are computationally intensive, especially when modeling the
atmospheric phenomenon at a fine-grained spatial and temporal resolution.
Recent data-driven approaches based on machine learning instead aim to directly
solve a downstream forecasting or projection task by learning a data-driven
functional mapping using deep neural networks. However, these networks are
trained using curated and homogeneous climate datasets for specific
spatiotemporal tasks, and thus lack the generality of numerical models. We
develop and demonstrate ClimaX, a flexible and generalizable deep learning
model for weather and climate science that can be trained using heterogeneous
datasets spanning different variables, spatio-temporal coverage, and physical
groundings. ClimaX extends the Transformer architecture with novel encoding and
aggregation blocks that allow effective use of available compute while
maintaining general utility. ClimaX is pre-trained with a self-supervised
learning objective on climate datasets derived from CMIP6. The pre-trained
ClimaX can then be fine-tuned to address a breadth of climate and weather
tasks, including those that involve atmospheric variables and spatio-temporal
scales unseen during pretraining. Compared to existing data-driven baselines,
we show that this generality in ClimaX results in superior performance on
benchmarks for weather forecasting and climate projections, even when
pretrained at lower resolutions and compute budgets. The source code is
available at https://github.com/microsoft/ClimaX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Image Augmentations for Siamese Representation Learning with
  Chest X-Rays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rogier van der Sluijs, Nandita Bhaskhar, Daniel Rubin, Curtis Langlotz, Akshay Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image augmentations are quintessential for effective visual representation
learning across self-supervised learning techniques. While augmentation
strategies for natural imaging have been studied extensively, medical images
are vastly different from their natural counterparts. Thus, it is unknown
whether common augmentation strategies employed in Siamese representation
learning generalize to medical images and to what extent. To address this
challenge, in this study, we systematically assess the effect of various
augmentations on the quality and robustness of the learned representations. We
train and evaluate Siamese Networks for abnormality detection on chest X-Rays
across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate
the efficacy of the learned representations through experiments involving
linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally,
we identify a set of augmentations that yield robust representations that
generalize well to both out-of-distribution data and diseases, while
outperforming supervised baselines using just zero-shot transfer and linear
probes by up to 20%. Our code is available at
https://github.com/StanfordMIMI/siaug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contributions. Oral paper at MIDL 2023. Additional experiments
  in appendix in V2. Keywords: Data Augmentations, Self-Supervised Learning,
  Medical Imaging, Chest X-rays, Siamese Representation Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Less Intelligent the Elements, the More Intelligent the Whole. Or,
  Possibly Not? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.12689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.12689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Fioretti, Andrea Policarpi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a Leviathan analogy between neurons in a brain and human beings in
society, asking ourselves whether individual intelligence is necessary for
collective intelligence to emerge and, most importantly, what sort of
individual intelligence is conducive of greater collective intelligence. We
first review disparate insights from connectionist cognitive science,
agent-based modeling, group psychology, economics and physics. Subsequently, we
apply these insights to the sort and degrees of intelligence that in the
Lotka-Volterra model lead to either co-existence or global extinction of
predators and preys.
  We find several individual behaviors -- particularly of predators -- that are
conducive to co-existence, eventually with oscillations around an equilibrium.
However, we also find that if both preys and predators are sufficiently
intelligent to extrapolate one other's behavior, co-existence comes along with
indefinite growth of both populations. Since the Lotka-Volterra model is also
interpreted to represent the business cycle, we understand this finding as a
condition for economic growth around oscillations. Specifically, we hypothesize
that pre-modern societies may not have exhibited limitless growth also because
capitalistic future-oriented thinking based on saving and investing concerned
at most a fraction of the population.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-09T00:00:00Z">2023-07-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Generative Large Language Models Perform ASR Error Correction? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, Kate Knill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ASR error correction continues to serve as an important part of
post-processing for speech recognition systems. Traditionally, these models are
trained with supervised training using the decoding results of the underlying
ASR system and the reference text. This approach is computationally intensive
and the model needs to be re-trained when switching the underlying ASR model.
Recent years have seen the development of large language models and their
ability to perform natural language processing tasks in a zero-shot manner. In
this paper, we take ChatGPT as an example to examine its ability to perform ASR
error correction in the zero-shot or 1-shot settings. We use the ASR N-best
list as model input and propose unconstrained error correction and N-best
constrained error correction methods. Results on a Conformer-Transducer model
and the pre-trained Whisper model show that we can largely improve the ASR
system performance with error correction using the powerful ChatGPT model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Agnostic Structured Pruning of Speech Representation Models <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Siyuan Wang, Wei-Qiang Zhang, Hongbin Suo, Yulong Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have
been shown to significantly improve many speech tasks. However, their large
memory and strong computational requirements hinder their industrial
applicability. Structured pruning is a hardware-friendly model compression
technique but usually results in a larger loss of accuracy. In this paper, we
propose a fine-grained attention head pruning method to compensate for the
performance degradation. In addition, we also introduce the straight through
estimator into the L0 regularization to further accelerate the pruned model.
Experiments on the SUPERB benchmark show that our model can achieve comparable
performance to the dense model in multiple tasks and outperforms the Wav2vec
2.0 base model on average, with 72% fewer parameters and 2 times faster
inference speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio
  <span class="highlight-title">Pretrain</span>ing for Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yanni Hu, Yuguang Yang, Jixun Yao, Wen Fei, Lei Ma, Heng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning based pretraining methods have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) model for speech emotion recognition. To be specific, we
first build an effective emotion CLAP model Emo-CLAP for emotion recognition,
utilizing various self-supervised learning based pre-trained models. Then,
considering the importance of the gender attribute in speech emotion modeling,
two GEmo-CLAP approaches are further proposed to integrate the emotion and
gender information of speech signals, forming more reasonable objectives.
Extensive experiments on the IEMOCAP corpus demonstrate that our proposed two
GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving superior recognition
performance compared with other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IANS: Intelligibility-aware Null-steering Beamforming for
  Dual-Microphone Arrays <span class="chip">SP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Yuan Ting, Syu-Siang Wang, Yu Tsao, Borching Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beamforming techniques are popular in speech-related applications due to
their effective spatial filtering capabilities. Nonetheless, conventional
beamforming techniques generally depend heavily on either the target's
direction-of-arrival (DOA), relative transfer function (RTF) or covariance
matrix. This paper presents a new approach, the intelligibility-aware
null-steering (IANS) beamforming framework, which uses the STOI-Net
intelligibility prediction model to improve speech intelligibility without
prior knowledge of the speech signal parameters mentioned earlier. The IANS
framework combines a null-steering beamformer (NSBF) to generate a set of
beamformed outputs, and STOI-Net, to determine the optimal result. Experimental
results indicate that IANS can produce intelligibility-enhanced signals using a
small dual-microphone array. The results are comparable to those obtained by
null-steering beamformers with given knowledge of DOAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to IEEE MLSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Generative Large Language Models Perform ASR Error Correction? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, Kate Knill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ASR error correction continues to serve as an important part of
post-processing for speech recognition systems. Traditionally, these models are
trained with supervised training using the decoding results of the underlying
ASR system and the reference text. This approach is computationally intensive
and the model needs to be re-trained when switching the underlying ASR model.
Recent years have seen the development of large language models and their
ability to perform natural language processing tasks in a zero-shot manner. In
this paper, we take ChatGPT as an example to examine its ability to perform ASR
error correction in the zero-shot or 1-shot settings. We use the ASR N-best
list as model input and propose unconstrained error correction and N-best
constrained error correction methods. Results on a Conformer-Transducer model
and the pre-trained Whisper model show that we can largely improve the ASR
system performance with error correction using the powerful ChatGPT model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Agnostic Structured Pruning of Speech Representation Models <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Siyuan Wang, Wei-Qiang Zhang, Hongbin Suo, Yulong Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have
been shown to significantly improve many speech tasks. However, their large
memory and strong computational requirements hinder their industrial
applicability. Structured pruning is a hardware-friendly model compression
technique but usually results in a larger loss of accuracy. In this paper, we
propose a fine-grained attention head pruning method to compensate for the
performance degradation. In addition, we also introduce the straight through
estimator into the L0 regularization to further accelerate the pruned model.
Experiments on the SUPERB benchmark show that our model can achieve comparable
performance to the dense model in multiple tasks and outperforms the Wav2vec
2.0 base model on average, with 72% fewer parameters and 2 times faster
inference speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio
  <span class="highlight-title">Pretrain</span>ing for Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yanni Hu, Yuguang Yang, Jixun Yao, Wen Fei, Lei Ma, Heng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning based pretraining methods have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) model for speech emotion recognition. To be specific, we
first build an effective emotion CLAP model Emo-CLAP for emotion recognition,
utilizing various self-supervised learning based pre-trained models. Then,
considering the importance of the gender attribute in speech emotion modeling,
two GEmo-CLAP approaches are further proposed to integrate the emotion and
gender information of speech signals, forming more reasonable objectives.
Extensive experiments on the IEMOCAP corpus demonstrate that our proposed two
GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving superior recognition
performance compared with other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IANS: Intelligibility-aware Null-steering Beamforming for
  Dual-Microphone Arrays <span class="chip">SP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Yuan Ting, Syu-Siang Wang, Yu Tsao, Borching Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beamforming techniques are popular in speech-related applications due to
their effective spatial filtering capabilities. Nonetheless, conventional
beamforming techniques generally depend heavily on either the target's
direction-of-arrival (DOA), relative transfer function (RTF) or covariance
matrix. This paper presents a new approach, the intelligibility-aware
null-steering (IANS) beamforming framework, which uses the STOI-Net
intelligibility prediction model to improve speech intelligibility without
prior knowledge of the speech signal parameters mentioned earlier. The IANS
framework combines a null-steering beamformer (NSBF) to generate a set of
beamformed outputs, and STOI-Net, to determine the optimal result. Experimental
results indicate that IANS can produce intelligibility-enhanced signals using a
small dual-microphone array. The results are comparable to those obtained by
null-steering beamformers with given knowledge of DOAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to IEEE MLSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The WQN algorithm for EEG artifact removal in the absence of scale
  invariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Dora, Stéphane Jaffard, David Holcman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) signals reflect brain activity across different
brain states, characterized by distinct frequency distributions. Through
multifractal analysis tools, we investigate the scaling behaviour of different
classes of EEG signals and artifacts. We show that brain states associated to
sleep and general anaesthesia are not in general characterized by scale
invariance. The lack of scale invariance motivates the development of artifact
removal algorithms capable of operating independently at each scale. We examine
here the properties of the wavelet quantile normalization algorithm, a recently
introduced adaptive method for real-time correction of transient artifacts in
EEG signals. We establish general results regarding the regularization
properties of the WQN algorithm, showing how it can eliminate singularities
introduced by artefacts, and we compare it to traditional thresholding
algorithms. Furthermore, we show that the algorithm performance is independent
of the wavelet basis. We finally examine its continuity and boundedness
properties and illustrate its distinctive non-local action on the wavelet
coefficients through pathological examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Based End-to-End Learning for Multi-Target Integrated Sensing and
  Communication <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Miguel Mateos-Ramos, Christian Häger, Musa Furkan Keskin, Luc Le Magoarou, Henk Wymeersch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study model-based end-to-end learning in the context of integrated sensing
and communication (ISAC) under hardware impairments. A monostatic orthogonal
frequency-division multiplexing (OFDM) sensing and multiple-input single-output
(MISO) communication scenario is considered, incorporating hardware
imperfections at the ISAC transceiver antenna array. To enable end-to-end
learning of the ISAC transmitter and sensing receiver, we propose a novel
differentiable version of the orthogonal matching pursuit (OMP) algorithm that
is suitable for multi-target sensing. Based on the differentiable OMP, we
devise two model-based parameterization strategies to account for hardware
impairments: (i) learning a dictionary of steering vectors for different
angles, and (ii) learning the parameterized hardware impairments. For the
single-target case, we carry out a comprehensive performance analysis of the
proposed model-based learning approaches, a neural-network-based learning
approach and a strong baseline consisting of least-squares beamforming,
conventional OMP, and maximum-likelihood symbol detection for communication.
Results show that learning the parameterized hardware impairments offers higher
detection probability, better angle and range estimation accuracy, lower
communication symbol error rate (SER), and exhibits the lowest complexity among
all learning methods. Lastly, we demonstrate that learning the parameterized
hardware impairments is scalable also to multiple targets, revealing
significant improvements in terms of ISAC performance over the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, submitted to JSTSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Analysis on EEG Signal Using Machine Learning and Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M. Masrur Ahmed, Eshaan Tanzim Sabur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion has a significant influence on how one thinks and interacts with
others. It serves as a link between how a person feels and the actions one
takes, or it could be said that it influences one's life decisions on occasion.
Since the patterns of emotions and their reflections vary from person to
person, their inquiry must be based on approaches that are effective over a
wide range of population regions. To extract features and enhance accuracy,
emotion recognition using brain waves or EEG signals requires the
implementation of efficient signal processing techniques. Various approaches to
human-machine interaction technologies have been ongoing for a long time, and
in recent years, researchers have had great success in automatically
understanding emotion using brain signals. In our research, several emotional
states were classified and tested on EEG signals collected from a well-known
publicly available dataset, the DEAP Dataset, using SVM (Support Vector
Machine), KNN (K-Nearest Neighbor), and an advanced neural network model, RNN
(Recurrent Neural Network), trained with LSTM (Long Short Term Memory). The
main purpose of this study is to improve ways to improve emotion recognition
performance using brain signals. Emotions, on the other hand, can change with
time. As a result, the changes in emotion over time are also examined in our
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta Federated Reinforcement Learning for Distributed Resource
  Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Ji, Zhijin Qin, Xiaoming Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cellular networks, resource allocation is usually performed in a
centralized way, which brings huge computation complexity to the base station
(BS) and high transmission overhead. This paper explores a distributed resource
allocation method that aims to maximize energy efficiency (EE) while ensuring
the quality of service (QoS) for users. Specifically, in order to address
wireless channel conditions, we propose a robust meta federated reinforcement
learning (\textit{MFRL}) framework that allows local users to optimize transmit
power and assign channels using locally trained neural network models, so as to
offload computational burden from the cloud server to the local users, reducing
transmission overhead associated with local channel state information. The BS
performs the meta learning procedure to initialize a general global model,
enabling rapid adaptation to different environments with improved EE
performance. The federated learning technique, based on decentralized
reinforcement learning, promotes collaboration and mutual benefits among users.
Analysis and numerical results demonstrate that the proposed \textit{MFRL}
framework accelerates the reinforcement learning process, decreases
transmission overhead, and offloads computation, while outperforming the
conventional decentralized reinforcement learning algorithm in terms of
convergence speed and EE performance across various scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TWC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Tomography for Structured Quantum States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Qin, Casey Jameson, Zhexuan Gong, Michael B. Wakin, Zhihui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reconstruction of quantum states from experimental measurements, often
achieved using quantum state tomography (QST), is crucial for the verification
and benchmarking of quantum devices. However, performing QST for a generic
unstructured quantum state requires an enormous number of state copies that
grows \emph{exponentially} with the number of individual quanta in the system,
even for the most optimal measurement settings. Fortunately, many physical
quantum states, such as states generated by noisy, intermediate-scale quantum
computers, are usually structured. In one dimension, such states are expected
to be well approximated by matrix product operators (MPOs) with a finite
matrix/bond dimension independent of the number of qubits, therefore enabling
efficient state representation. Nevertheless, it is still unclear whether
efficient QST can be performed for these states in general.
  In this paper, we attempt to bridge this gap and establish theoretical
guarantees for the stable recovery of MPOs using tools from compressive sensing
and the theory of empirical processes. We begin by studying two types of random
measurement settings: Gaussian measurements and Haar random rank-one Positive
Operator Valued Measures (POVMs). We show that the information contained in an
MPO with a finite bond dimension can be preserved using a number of random
measurements that depends only \emph{linearly} on the number of qubits,
assuming no statistical error of the measurements. We then study MPO-based QST
with physical quantum measurements through Haar random rank-one POVMs that can
be implemented on quantum computers. We prove that only a \emph{polynomial}
number of state copies in the number of qubits is required to guarantee bounded
recovery error of an MPO state.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Saving Precoder Design for Narrowband and Wideband Massive MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01661v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01661v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Peschiera, François Rottenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study massive multiple-input multiple-output (MIMO)
precoders optimizing power consumption while achieving the users' rate
requirements. We first characterize analytically the solutions for narrowband
and wideband systems minimizing the power amplifiers (PAs) consumption in low
system load, where the per-antenna power constraints are not binding. After, we
focus on the asymptotic wideband regime. The power consumed by the whole base
station (BS) and the high-load scenario are then also investigated. We obtain
simple solutions, and the optimal strategy in the asymptotic case reduces to
finding the optimal number of active antennas, relying on known precoders among
the active antennas. Numerical results show that large savings in power
consumption are achievable in the narrowband system by employing antenna
selection, while all antennas need to be activated in the wideband system when
considering only the PAs consumption, and this implies lower savings. When
considering the overall BS power consumption and a large number of subcarriers,
we show that significant savings are achievable in the low-load regime by using
a subset of the BS antennas. While optimization based on transmit power pushes
to activate all antennas, optimization based on consumed power activates a
number of antennas proportional to the load.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Transactions on Green Communications and Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Complexity Multi-Antenna Coded Caching Using Location-Aware
  Placement Delivery Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Bakhshzad Mahmoodi, MohammadJavad Salehi, Antti Tolli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A location-aware multi-antenna coded caching scheme is proposed for
applications with location-dependent data requests, such as wireless immersive
experience, where users are immersed in a three-dimensional virtual world. The
wireless connectivity conditions vary as the users move within the application
area motivating the use of a non-uniform cache memory allocation process to
avoid excessive delivery time for users located in wireless bottleneck areas.
To this end, a location-aware placement and delivery array (LAPDA) is designed
for cache-aided multiantenna data delivery with a fast converging, iterative
linear beamforming process. The underlying weighted max-min transmit precoder
design enables the proposed scheme to serve users in poor connectivity areas
with smaller amounts of data while simultaneously delivering larger amounts to
other users. Our new scheme is suitable for large networks due to its linear
transceiver structure and it is not constrained by the number of users, cache
size, or the number of antennas at the transmitter, unlike the existing
schemes. Despite non-uniform cache placement, the proposed scheme still
achieves a significant degree of coded caching gain that is additive to the
multiplexing gain and greatly outperforms the conventional symmetric CC schemes
in terms of both average and 95-percentile delivery time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages and 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrusion Resilience Systems for Modern Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Shoker, Vincent Rahli, Jeremie Decouchant, Paulo Esteves-Verissimo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vehicular Intrusion Detection and Prevention Systems either incur
high false-positive rates or do not capture zero-day vulnerabilities, leading
to safety-critical risks. In addition, prevention is limited to few primitive
options like dropping network packets or extreme options, e.g., ECU Bus-off
state. To fill this gap, we introduce the concept of vehicular Intrusion
Resilience Systems (IRS) that ensures the resilience of critical applications
despite assumed faults or zero-day attacks, as long as threat assumptions are
met. IRS enables running a vehicular application in a replicated way, i.e., as
a Replicated State Machine, over several ECUs, and then requiring the
replicated processes to reach a form of Byzantine agreement before changing
their local state. Our study rides the mutation of modern vehicular
environments, which are closing the gap between simple and resource-constrained
"real-time and embedded systems", and complex and powerful "information
technology" ones. It shows that current vehicle (e.g., Zonal) architectures and
networks are becoming plausible for such modular fault and intrusion tolerance
solutions,deemed too heavy in the past. Our evaluation on a simulated
Automotive Ethernet network running two state-of-the-art agreement protocols
(Damysus and Hotstuff) shows that the achieved latency and throughout are
feasible for many Automotive applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerard Calvo Bartra, Filip Lemic, Sergi Abadal, Xavier Costa Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific advancements in nanotechnology and advanced materials are paving
the way toward nanoscale devices for in-body precision medicine; comprising
integrated sensing, computing, communication, data and energy storage
capabilities. In the human cardiovascular system, such devices are envisioned
to be passively flowing and continuously sensing for detecting events of
diagnostic interest. The diagnostic value of detecting such events can be
enhanced by assigning to them their physical locations (e.g., body region),
which is the main proposition of flow-guided localization. Current flow-guided
localization approaches suffer from low localization accuracy and they are
by-design unable to localize events within the entire cardiovascular system.
Toward addressing this issue, we propose the utilization of Graph Neural
Networks (GNNs) for this purpose, and demonstrate localization accuracy and
coverage enhancements of our proposal over the existing State of the Art (SotA)
approaches. Based on our evaluation, we provide several design guidelines for
GNN-enabled flow-guided localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 1 table, 15 references. arXiv admin note: text
  overlap with arXiv:2305.18493</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shaping the Emerging Norms of Using Large Language Models in Social
  Computing Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Shen, Tianshi Li, Toby Jia-Jun Li, Joon Sung Park, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) has brought both excitement and
concerns to social computing research. On the one hand, LLMs offer
unprecedented capabilities in analyzing vast amounts of textual data and
generating human-like responses, enabling researchers to delve into complex
social phenomena. On the other hand, concerns are emerging regarding the
validity, privacy, and ethics of the research when LLMs are involved. This SIG
aims at offering an open space for social computing researchers who are
interested in understanding the impacts of LLMs to discuss their current
practices, perspectives, challenges when engaging with LLMs in their everyday
work and collectively shaping the emerging norms of using LLMs in social
computing research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VR Job Interview Using a Gender-Swapped Avatar <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieun Kim, Hauke Sandhaus, Susan R. Fussell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Reality (VR) has emerged as a potential solution for mitigating bias
in a job interview by hiding the applicants' demographic features. The current
study examines the use of a gender-swapped avatar in a virtual job interview
that affects the applicants' perceptions and their performance evaluated by
recruiters. With a mixed-method approach, we first conducted a lab experiment
(N=8) exploring how using a gender-swapped avatar in a virtual job interview
impacts perceived anxiety, confidence, competence, and ability to perform.
Then, a semi-structured interview investigated the participants' VR interview
experiences using an avatar. Our findings suggest that using gender-swapped
avatars may reduce the anxiety that job applicants will experience during the
interview. Also, the affinity diagram produced seven key themes highlighting
the advantages and limitations of VR as an interview platform. These findings
contribute to the emerging field of VR-based recruitment and have practical
implications for promoting diversity and inclusion in the hiring process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CSCW 2022 Posters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Instructions for Intuitive Human Interaction with
  Robotic Assistants in Field Construction Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somin Park, Xi Wang, Carol C. Menassa, Vineet R. Kamat, Joyce Y. Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of robots is widely considered to have significant potential
of alleviating the issues of worker shortage and stagnant productivity that
afflict the construction industry. However, it is challenging to use fully
automated robots in complex and unstructured construction sites. Human-Robot
Collaboration (HRC) has shown promise of combining human workers' flexibility
and robot assistants' physical abilities to jointly address the uncertainties
inherent in construction work. When introducing HRC in construction, it is
critical to recognize the importance of teamwork and supervision in field
construction and establish a natural and intuitive communication system for the
human workers and robotic assistants. Natural language-based interaction can
enable intuitive and familiar communication with robots for human workers who
are non-experts in robot programming. However, limited research has been
conducted on this topic in construction. This paper proposes a framework to
allow human workers to interact with construction robots based on natural
language instructions. The proposed method consists of three stages: Natural
Language Understanding (NLU), Information Mapping (IM), and Robot Control (RC).
Natural language instructions are input to a language model to predict a tag
for each word in the NLU module. The IM module uses the result of the NLU
module and building component information to generate the final instructional
output essential for a robot to acknowledge and perform the construction task.
A case study for drywall installation is conducted to evaluate the proposed
approach. The obtained results highlight the potential of using natural
language-based interaction to replicate the communication that occurs between
human workers within the context of human-robot teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Analysis on EEG Signal Using Machine Learning and Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M. Masrur Ahmed, Eshaan Tanzim Sabur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion has a significant influence on how one thinks and interacts with
others. It serves as a link between how a person feels and the actions one
takes, or it could be said that it influences one's life decisions on occasion.
Since the patterns of emotions and their reflections vary from person to
person, their inquiry must be based on approaches that are effective over a
wide range of population regions. To extract features and enhance accuracy,
emotion recognition using brain waves or EEG signals requires the
implementation of efficient signal processing techniques. Various approaches to
human-machine interaction technologies have been ongoing for a long time, and
in recent years, researchers have had great success in automatically
understanding emotion using brain signals. In our research, several emotional
states were classified and tested on EEG signals collected from a well-known
publicly available dataset, the DEAP Dataset, using SVM (Support Vector
Machine), KNN (K-Nearest Neighbor), and an advanced neural network model, RNN
(Recurrent Neural Network), trained with LSTM (Long Short Term Memory). The
main purpose of this study is to improve ways to improve emotion recognition
performance using brain signals. Emotions, on the other hand, can change with
time. As a result, the changes in emotion over time are also examined in our
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smart Dimming Sunglasses for Photophobia Using Spatial Light Modulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07013v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07013v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodan Hu, Yan Zhang, Naoya Isoyama, Hideaki Uchiyama, Nobuchika Sakata, Kiyoshi Kiyokawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a smart dimming sunglasses system designed for photophobia
sufferers, particularly those highly sensitive to light intensity. The system
incorporates a spatial light modulator (SLM) to filter light based on
camera-detected scenes, controlling pixel transmittance via a modulation
function for automated non-linear field of view dimming, thus offering flexible
light modulation to meet the visual needs of photophobic users. However, a
conventional occlusion mask on the SLM, aimed at blocking incoming light,
appears blurred and insufficient due to a misaligned focal plane. Previous
attempts to remedy this with an aperture-based expanded mask led to
over-blocking (occlusion leak), due to an excessively large expansion radius.
Our work, therefore, focuses on developing an optimization model that simulates
a defocused occlusion mask and determines the degraded pixels' effective
contribution by studying pixel transmittance occlusion efficiency. This
optimized mask successfully attenuates bright areas to appropriate brightness
levels without unnecessary attenuation of areas that do not require modulation,
overcoming the limitations of both the unprocessed and aperture-based expanded
masks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Activity Behavioural Pattern Recognition in Smarthome with
  Long-hour Data Collection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjit Kolkar, Geetha V
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research on human activity recognition has provided novel solutions to
many applications like healthcare, sports, and user profiling. Considering the
complex nature of human activities, it is still challenging even after
effective and efficient sensors are available. The existing works on human
activity recognition using smartphone sensors focus on recognizing basic human
activities like sitting, sleeping, standing, stair up and down and running.
However, more than these basic activities is needed to analyze human
behavioural pattern. The proposed framework recognizes basic human activities
using deep learning models. Also, ambient sensors like PIR, pressure sensors,
and smartphone-based sensors like accelerometers and gyroscopes are combined to
make it hybrid-sensor-based human activity recognition. The hybrid approach
helped derive more activities than the basic ones, which also helped derive
human activity patterns or user profiling. User profiling provides sufficient
information to identify daily living activity patterns and predict whether any
anomaly exists. The framework provides the base for applications such as
elderly monitoring when they are alone at home. The GRU model's accuracy of
95\% is observed to recognize the basic activities. Finally, Human activity
patterns over time are recognized based on the duration and frequency of the
activities. It is observed that human activity pattern, like, morning walking
duration, varies depending on the day of the week.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">35</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> in the Age of Generative AI and Large Language Models: A Concise
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salman Mohamadi, Ghulam Mujtaba, Ngan Le, Gianfranco Doretto, Donald A. Adjeroh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Pipeline for Improving Optical Character Recognition through
  Post-processing Using Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishik Rakshit, Samyak Mehta, Anirban Dasgupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Character Recognition (OCR) technology finds applications in
digitizing books and unstructured documents, along with applications in other
domains such as mobility statistics, law enforcement, traffic, security
systems, etc. The state-of-the-art methods work well with the OCR with printed
text on license plates, shop names, etc. However, applications such as printed
textbooks and handwritten texts have limited accuracy with existing techniques.
The reason may be attributed to similar-looking characters and variations in
handwritten characters. Since these issues are challenging to address with OCR
technologies exclusively, we propose a post-processing approach using Natural
Language Processing (NLP) tools. This work presents an end-to-end pipeline that
first performs OCR on the handwritten or printed text and then improves its
accuracy using NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE GCON (IEEE Guwahati Subsection Conference) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Human Detection in Fire Scenarios using Infrared and Thermal
  Imaging Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Truong-Dong Do, Nghe-Nhan Truong, My-Ha Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fire is considered one of the most serious threats to human lives which
results in a high probability of fatalities. Those severe consequences stem
from the heavy smoke emitted from a fire that mostly restricts the visibility
of escaping victims and rescuing squad. In such hazardous circumstances, the
use of a vision-based human detection system is able to improve the ability to
save more lives. To this end, a thermal and infrared imaging fusion strategy
based on multiple cameras for human detection in low-visibility scenarios
caused by smoke is proposed in this paper. By processing with multiple cameras,
vital information can be gathered to generate more useful features for human
detection. Firstly, the cameras are calibrated using a Light Heating
Chessboard. Afterward, the features extracted from the input images are merged
prior to being passed through a lightweight deep neural network to perform the
human detection task. The experiments conducted on an NVIDIA Jetson Nano
computer demonstrated that the proposed method can process with reasonable
speed and can achieve favorable performance with a mAP@0.5 of 95%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LakeBench: Benchmarks for Data Discovery over Data Lakes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavitha Srinivas, Julian Dolby, Ibrahim Abdelaziz, Oktie Hassanzadeh, Harsha Kokel, Aamod Khatiwada, Tejaswini Pedapati, Subhajit Chaudhury, Horst Samulowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within enterprises, there is a growing need to intelligently navigate data
lakes, specifically focusing on data discovery. Of particular importance to
enterprises is the ability to find related tables in data repositories. These
tables can be unionable, joinable, or subsets of each other. There is a dearth
of benchmarks for these tasks in the public domain, with related work targeting
private datasets. In LakeBench, we develop multiple benchmarks for these tasks
by using the tables that are drawn from a diverse set of data sources such as
government data from CKAN, Socrata, and the European Central Bank. We compare
the performance of 4 publicly available tabular foundational models on these
tasks. None of the existing models had been trained on the data discovery tasks
that we developed for this benchmark; not surprisingly, their performance shows
significant room for improvement. The results suggest that the establishment of
such benchmarks may be useful to the community to build tabular models usable
for data discovery in data lakes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Autoencoder-based Lossy Compression for Large-scale
  High-resolution Scientific Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu Le, Hernan Santos, Jian Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lossy compression has become an important technique to reduce data size in
many domains. This type of compression is especially valuable for large-scale
scientific data, whose size ranges up to several petabytes. Although
Autoencoder-based models have been successfully leveraged to compress images
and videos, such neural networks have not widely gained attention in the
scientific data domain. Our work presents a neural network that not only
significantly compresses large-scale scientific data but also maintains high
reconstruction quality. The proposed model is tested with scientific benchmark
data available publicly and applied to a large-scale high-resolution climate
modeling data set. Our model achieves a compression ratio of 140 on several
benchmark data sets without compromising the reconstruction quality. Simulation
data from the High-Resolution Community Earth System Model (CESM) Version 1.3
over 500 years are also being compressed with a compression ratio of 200 while
the reconstruction error is negligible for scientific analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Challenges of Deploying Privacy-Preserving Synthetic Data in the
  Enterprise <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauren Arthur, Jason Costello, Jonathan Hardy, Will O'Brien, James Rea, Gareth Rees, Georgi Ganev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI technologies are gaining unprecedented popularity, causing a
mix of excitement and apprehension through their remarkable capabilities. In
this paper, we study the challenges associated with deploying synthetic data, a
subfield of Generative AI. Our focus centers on enterprise deployment, with an
emphasis on privacy concerns caused by the vast amount of personal and highly
sensitive data. We identify 40+ challenges and systematize them into five main
groups -- i) generation, ii) infrastructure & architecture, iii) governance,
iv) compliance & regulation, and v) adoption. Additionally, we discuss a
strategic and systematic approach that enterprises can employ to effectively
address the challenges and achieve their goals by establishing trust in the
implemented solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 1st Workshop on Challenges in Deployable Generative
  AI, part of ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Instructions for Intuitive Human Interaction with
  Robotic Assistants in Field Construction Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somin Park, Xi Wang, Carol C. Menassa, Vineet R. Kamat, Joyce Y. Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of robots is widely considered to have significant potential
of alleviating the issues of worker shortage and stagnant productivity that
afflict the construction industry. However, it is challenging to use fully
automated robots in complex and unstructured construction sites. Human-Robot
Collaboration (HRC) has shown promise of combining human workers' flexibility
and robot assistants' physical abilities to jointly address the uncertainties
inherent in construction work. When introducing HRC in construction, it is
critical to recognize the importance of teamwork and supervision in field
construction and establish a natural and intuitive communication system for the
human workers and robotic assistants. Natural language-based interaction can
enable intuitive and familiar communication with robots for human workers who
are non-experts in robot programming. However, limited research has been
conducted on this topic in construction. This paper proposes a framework to
allow human workers to interact with construction robots based on natural
language instructions. The proposed method consists of three stages: Natural
Language Understanding (NLU), Information Mapping (IM), and Robot Control (RC).
Natural language instructions are input to a language model to predict a tag
for each word in the NLU module. The IM module uses the result of the NLU
module and building component information to generate the final instructional
output essential for a robot to acknowledge and perform the construction task.
A case study for drywall installation is conducted to evaluate the proposed
approach. The obtained results highlight the potential of using natural
language-based interaction to replicate the communication that occurs between
human workers within the context of human-robot teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAS Video-QA: Self-Adaptive Sampling for Efficient Video
  Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Graph Attention for Enhanced Spatial Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Singh, Yash Bhambhu, Himanshu Buckchash, Deepak K. Gupta, Dilip K. Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> and Approach to Chart Classification <span class="chip">ICDAR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Dhote, Mohammed Javed, David S Doermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts represent an essential source of visual information in documents and
facilitate a deep understanding and interpretation of information typically
conveyed numerically. In the scientific literature, there are many charts, each
with its stylistic differences. Recently the document understanding community
has begun to address the problem of automatic chart understanding, which begins
with chart classification. In this paper, we present a survey of the current
state-of-the-art techniques for chart classification and discuss the available
datasets and their supported chart types. We broadly classify these
contributions as traditional approaches based on ML, CNN, and Transformers.
Furthermore, we carry out an extensive comparative performance analysis of
CNN-based and transformer-based approaches on the recently published CHARTINFO
UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The
data set includes 15 different chart categories, including 22,923 training
images and 13,260 test images. We have implemented a vision-based transformer
model that produces state-of-the-art results in chart classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 15th IAPR Workshop on Graphics Recognition (GREC) 2023 in
  conjunction with 17th International Conference on Document Analysis and
  Recognition (ICDAR) 2023, August 21-26, 2023 San Jose, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Explainable Artificial Intelligence Model in Image
  Classification problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quoc Hung Cao, Truong Thanh Hung Nguyen, Vo Thanh Khang Nguyen, Xuan Phong Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, artificial intelligence is increasingly being applied widely
in many different fields and has a profound and direct impact on human life.
Following this is the need to understand the principles of the model making
predictions. Since most of the current high-precision models are black boxes,
neither the AI scientist nor the end-user deeply understands what's going on
inside these models. Therefore, many algorithms are studied for the purpose of
explaining AI models, especially those in the problem of image classification
in the field of computer vision such as LIME, CAM, GradCAM. However, these
algorithms still have limitations such as LIME's long execution time and CAM's
confusing interpretation of concreteness and clarity. Therefore, in this paper,
we propose a new method called Segmentation - Class Activation Mapping (SeCAM)
that combines the advantages of these algorithms above, while at the same time
overcoming their disadvantages. We tested this algorithm with various models,
including ResNet50, Inception-v3, VGG16 from ImageNet Large Scale Visual
Recognition Challenge (ILSVRC) data set. Outstanding results when the algorithm
has met all the requirements for a specific explanation in a remarkably concise
time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Proceedings of FAIC 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrit Diggavi Seshadri, Alessandra Russo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, following the intuition that adverbs describing scene-sequences
are best identified by reasoning over high-level concepts of object-behavior,
we propose the design of a new framework that reasons over object-behaviours
extracted from raw-video-clips to recognize the clip's corresponding
adverb-types. Importantly, while previous works for general scene
adverb-recognition assume knowledge of the clips underlying action-types, our
method is directly applicable in the more general problem setting where the
action-type of a video-clip is unknown. Specifically, we propose a novel
pipeline that extracts human-interpretable object-behaviour-facts from raw
video clips and propose novel symbolic and transformer based reasoning methods
that operate over these extracted facts to identify adverb-types. Experiment
results demonstrate that our proposed methods perform favourably against the
previous state-of-the-art. Additionally, to support efforts in symbolic
video-processing, we release two new datasets of object-behaviour-facts
extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Carbon-Efficient Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Zhao, Tian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a novel approach to neural architecture search (NAS) that
aims to reduce energy costs and increase carbon efficiency during the model
design process. The proposed framework, called carbon-efficient NAS (CE-NAS),
consists of NAS evaluation algorithms with different energy requirements, a
multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS
dynamically balances energy-efficient sampling and energy-consuming evaluation
tasks based on current carbon emissions. Using a recent NAS benchmark dataset
and two carbon traces, our trace-driven simulations demonstrate that CE-NAS
achieves better carbon and search efficiency than the three baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FILM: How can Few-Shot Image Classification Benefit from <span class="highlight-title">Pre-Train</span>ed
  Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Jiang, Yunkai Dang, Dong Pang, Huishuai Zhang, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot learning aims to train models that can be generalized to novel
classes with only a few samples. Recently, a line of works are proposed to
enhance few-shot learning with accessible semantic information from class
names. However, these works focus on improving existing modules such as visual
prototypes and feature extractors of the standard few-shot learning framework.
This limits the full potential use of semantic information. In this paper, we
propose a novel few-shot learning framework that uses pre-trained language
models based on contrastive learning. To address the challenge of alignment
between visual features and textual embeddings obtained from text-based
pre-trained language model, we carefully design the textual branch of our
framework and introduce a metric module to generalize the cosine similarity.
For better transferability, we let the metric module adapt to different
few-shot tasks and adopt MAML to train the model via bi-level optimization.
Moreover, we conduct extensive experiments on multiple benchmarks to
demonstrate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A User Study on Explainable Online Reinforcement Learning for Adaptive
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Metzger, Jan Laufer, Felix Feit, Klaus Pohl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online reinforcement learning (RL) is increasingly used for realizing
adaptive systems in the presence of design time uncertainty. Online RL
facilitates learning from actual operational data and thereby leverages
feedback only available at runtime. However, Online RL requires the definition
of an effective and correct reward function, which quantifies the feedback to
the RL algorithm and thereby guides learning. With Deep RL gaining interest,
the learned knowledge is no longer explicitly represented, but is represented
as a neural network. For a human, it becomes practically impossible to relate
the parametrization of the neural network to concrete RL decisions. Deep RL
thus essentially appears as a black box, which severely limits the debugging of
adaptive systems. We previously introduced the explainable RL technique
XRL-DINE, which provides visual insights into why certain decisions were made
at important time points. Here, we introduce an empirical user study involving
54 software engineers from academia and industry to assess (1) the performance
of software engineers when performing different tasks using XRL-DINE and (2)
the perceived usefulness and ease of use of XRL-DINE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2210.05931</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge
  Graphs <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Roush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work within the Argument Mining community has shown the applicability
of Natural Language Processing systems for solving problems found within
competitive debate. One of the most important tasks within competitive debate
is for debaters to create high quality debate cases. We show that effective
debate cases can be constructed using constrained shortest path traversals on
Argumentative Semantic Knowledge Graphs. We study this potential in the context
of a type of American Competitive Debate, called Policy Debate, which already
has a large scale dataset targeting it called DebateSum. We significantly
improve upon DebateSum by introducing 53180 new examples, as well as further
useful metadata for every example, to the dataset. We leverage the txtai
semantic search and knowledge graph toolchain to produce and contribute 9
semantic knowledge graphs built on this dataset. We create a unique method for
evaluating which knowledge graphs are better in the context of producing policy
debate cases. A demo which automatically generates debate cases, along with all
other code and the Knowledge Graphs, are open-sourced and made available to the
public here: https://github.com/Hellisotherpeople/DebateKG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, knife-edge reject from EACL 2023 and workshops, System
  Demonstration paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Head Attention Mechanism Learning for Cancer New Subtypes and
  Treatment Based on Cancer Multi-Omics Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangrui Pan, Dazhen Liu, Yutao Dou, Lian Wang, Zhichao Feng, Pengfei Rong, Liwen Xu, Shaoliang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the high heterogeneity and clinical characteristics of cancer, there
are significant differences in multi-omics data and clinical features among
subtypes of different cancers. Therefore, the identification and discovery of
cancer subtypes are crucial for the diagnosis, treatment, and prognosis of
cancer. In this study, we proposed a generalization framework based on
attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze
cancer multi-omics data for the identification and characterization of cancer
subtypes. AMUCL framework includes a unsupervised multi-head attention
mechanism, which deeply extracts multi-omics data features. Importantly, a
decoupled contrastive learning model (DMACL) based on a multi-head attention
mechanism is proposed to learn multi-omics data features and clusters and
identify new cancer subtypes. This unsupervised contrastive learning method
clusters subtypes by calculating the similarity between samples in the feature
space and sample space of multi-omics data. Compared to 11 other deep learning
models, the DMACL model achieved a C-index of 0.002, a Silhouette score of
0.801, and a Davies Bouldin Score of 0.38 on a single-cell multi-omics dataset.
On a cancer multi-omics dataset, the DMACL model obtained a C-index of 0.016, a
Silhouette score of 0.688, and a Davies Bouldin Score of 0.46, and obtained the
most reliable cancer subtype clustering results for each type of cancer.
Finally, we used the DMACL model in the AMUCL framework to reveal six cancer
subtypes of AML. By analyzing the GO functional enrichment, subtype-specific
biological functions, and GSEA of AML, we further enhanced the interpretability
of cancer subtype analysis based on the generalizable AMUCL framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shift-Robust Molecular Relational Learning with Causal Substructure <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namkyeong Lee, Kanghoon Yoon, Gyoung S. Na, Sein Kim, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, molecular relational learning, whose goal is to predict the
interaction behavior between molecular pairs, got a surge of interest in
molecular sciences due to its wide range of applications. In this work, we
propose CMRL that is robust to the distributional shift in molecular relational
learning by detecting the core substructure that is causally related to
chemical reactions. To do so, we first assume a causal relationship based on
the domain knowledge of molecular sciences and construct a structural causal
model (SCM) that reveals the relationship between variables. Based on the SCM,
we introduce a novel conditional intervention framework whose intervention is
conditioned on the paired molecule. With the conditional intervention
framework, our model successfully learns from the causal substructure and
alleviates the confounding effect of shortcut substructures that are spuriously
correlated to chemical reactions. Extensive experiments on various tasks with
real-world and synthetic datasets demonstrate the superiority of CMRL over
state-of-the-art baseline models. Our code is available at
https://github.com/Namkyeong/CMRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Adversarial Contrastive Learning for Emotion Recognition in
  Conversations <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting generalized and robust representations is a major challenge in
emotion recognition in conversations (ERC). To address this, we propose a
supervised adversarial contrastive learning (SACL) framework for learning
class-spread structured representations in a supervised manner. SACL applies
contrast-aware adversarial training to generate worst-case samples and uses
joint class-spread contrastive learning to extract structured representations.
It can effectively utilize label-level feature consistency and retain
fine-grained intra-class features. To avoid the negative impact of adversarial
perturbations on context-dependent data, we design a contextual adversarial
training (CAT) strategy to learn more diverse features from context and enhance
the model's context robustness. Under the framework with CAT, we develop a
sequence-based SACL-LSTM to learn label-consistent and context-robust features
for ERC. Experiments on three datasets show that SACL-LSTM achieves
state-of-the-art performance on ERC. Extended experiments prove the
effectiveness of SACL and CAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, accepted by ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Capture <span class="highlight-title">Dataset</span> for Practical Use of AI-based Motion Editing and
  Stylization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makito Kobayashi, Chen-Chieh Liao, Keito Inoue, Sentaro Yojima, Masafumi Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we proposed a new style-diverse dataset for the domain of
motion style transfer. The motion dataset uses an industrial-standard human
bone structure and thus is industry-ready to be plugged into 3D characters for
many projects. We claim the challenges in motion style transfer and encourage
future work in this domain by releasing the proposed motion dataset both to the
public and the market. We conduct a comprehensive study on motion style
transfer in the experiment using the state-of-the-art method, and the results
show the proposed dataset's validity for the motion style transfer task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Sequential Bayesian Inference for Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Kessler, Adam Cobb, Tim G. J. Rudner, Stefan Zohren, Stephen J. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Bayesian inference can be used for continual learning to prevent
catastrophic forgetting of past tasks and provide an informative prior when
learning new tasks. We revisit sequential Bayesian inference and test whether
having access to the true posterior is guaranteed to prevent catastrophic
forgetting in Bayesian neural networks. To do this we perform sequential
Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as
a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo
samples. We find that this approach fails to prevent catastrophic forgetting
demonstrating the difficulty in performing sequential Bayesian inference in
neural networks. From there we study simple analytical examples of sequential
Bayesian inference and CL and highlight the issue of model misspecification
which can lead to sub-optimal continual learning performance despite exact
inference. Furthermore, we discuss how task data imbalances can cause
forgetting. From these limitations, we argue that we need probabilistic models
of the continual learning generative process rather than relying on sequential
Bayesian inference over Bayesian neural network weights. In this vein, we also
propose a simple baseline called Prototypical Bayesian Continual Learning,
which is competitive with state-of-the-art Bayesian continual learning methods
on class incremental continual learning vision benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Entropy, 24 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From $O(\sqrt{n})$ to $O(\log(n))$ in Quadratic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A "dark cloud" hangs over numerical optimization theory for decades, namely,
whether an optimization algorithm $O(\log(n))$ iteration complexity exists.
"Yes", this paper answers, with a new optimization algorithm and strict theory
proof. It starts with box-constrained quadratic programming (Box-QP), and many
practical optimization problems fall into Box-QP. General smooth quadratic
programming (QP), nonsmooth Lasso, and support vector machine (or regression)
can be reformulated as Box-QP via duality theory. It is the first time to
present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which
behaves like a "direct" method: the required number of iterations is
deterministic with exact value
$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$.
This significant breakthrough enables us to transition from the $O(\sqrt{n})$
to the $O(\log(n))$ optimization algorithm, whose amazing scalability is
particularly relevant in today's era of big data and artificial intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monocular Road Planar Parallax Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Yuan, Teng Chen, Wei Sui, Jiafeng Xie, Lefei Zhang, Yuan Li, Qian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the 3D structure of the drivable surface and surrounding
environment is a crucial task for assisted and autonomous driving. It is
commonly solved either by using 3D sensors such as LiDAR or directly predicting
the depth of points via deep learning. However, the former is expensive, and
the latter lacks the use of geometry information for the scene. In this paper,
instead of following existing methodologies, we propose Road Planar Parallax
Attention Network (RPANet), a new deep neural network for 3D sensing from
monocular image sequences based on planar parallax, which takes full advantage
of the omnipresent road plane geometry in driving scenes. RPANet takes a pair
of images aligned by the homography of the road plane as input and outputs a
$\gamma$ map (the ratio of height to depth) for 3D reconstruction. The $\gamma$
map has the potential to construct a two-dimensional transformation between two
consecutive frames. It implies planar parallax and can be combined with the
road plane serving as a reference to estimate the 3D structure by warping the
consecutive frames. Furthermore, we introduce a novel cross-attention module to
make the network better perceive the displacements caused by planar parallax.
To verify the effectiveness of our method, we sample data from the Waymo Open
Dataset and construct annotations related to planar parallax. Comprehensive
experiments are conducted on the sampled dataset to demonstrate the 3D
reconstruction accuracy of our approach in challenging scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Large Language Models to Simulate Multiple Humans and Replicate
  Human Subject Studies <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10264v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10264v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gati Aher, Rosa I. Arriaga, Adam Tauman Kalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new type of test, called a Turing Experiment (TE), for
evaluating to what extent a given language model, such as GPT models, can
simulate different aspects of human behavior. A TE can also reveal consistent
distortions in a language model's simulation of a specific human behavior.
Unlike the Turing Test, which involves simulating a single arbitrary
individual, a TE requires simulating a representative sample of participants in
human subject research. We carry out TEs that attempt to replicate
well-established findings from prior studies. We design a methodology for
simulating TEs and illustrate its use to compare how well different language
models are able to reproduce classic economic, psycholinguistic, and social
psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock
Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings
were replicated using recent models, while the last TE reveals a
"hyper-accuracy distortion" present in some language models (including ChatGPT
and GPT-4), which could affect downstream applications in education and the
arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at International Conference on Machine
  Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Creativity of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00008v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00008v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are revolutionizing several areas of Artificial
Intelligence. One of the most remarkable applications is creative writing,
e.g., poetry or storytelling: the generated outputs are often of astonishing
quality. However, a natural question arises: can LLMs be really considered
creative? In this article we firstly analyze the development of LLMs under the
lens of creativity theories, investigating the key open questions and
challenges. In particular, we focus our discussion around the dimensions of
value, novelty and surprise as proposed by Margaret Boden in her work. Then, we
consider different classic perspectives, namely product, process, press and
person. We discuss a set of ``easy'' and ``hard'' problems in machine
creativity, presenting them in relation to LLMs. Finally, we examine the
societal impact of these technologies with a particular focus on the creative
industries, analyzing the opportunities offered by them, the challenges arising
by them and the potential associated risks, from both legal and ethical points
of view.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Improving Safety Performance of Reinforcement Learning Based
  Driving with Black-Box Verification Algorithms <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16575v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16575v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Resul Dagdanov, Halil Durmus, Nazim Kemal Ure
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a self-improving artificial intelligence system to
enhance the safety performance of reinforcement learning (RL)-based autonomous
driving (AD) agents using black-box verification methods. RL algorithms have
become popular in AD applications in recent years. However, the performance of
existing RL algorithms heavily depends on the diversity of training scenarios.
A lack of safety-critical scenarios during the training phase could result in
poor generalization performance in real-world driving applications. We propose
a novel framework in which the weaknesses of the training set are explored
through black-box verification methods. After discovering AD failure scenarios,
the RL agent's training is re-initiated via transfer learning to improve the
performance of previously unsafe scenarios. Simulation results demonstrate that
our approach efficiently discovers safety failures of action decisions in
RL-based adaptive cruise control (ACC) applications and significantly reduces
the number of vehicle collisions through iterative applications of our method.
The source code is publicly available at
https://github.com/data-and-decision-lab/self-improving-RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 2 tables, published in IEEE International
  Conference on Robotics and Automation (ICRA), June 2, 2023, London, UK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCLIP: Fast Generalization and Personalization for CLIP in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Lu, Xixu Hu, Jindong Wang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has emerged as a new paradigm for privacy-preserving
computation in recent years. Unfortunately, FL faces two critical challenges
that hinder its actual performance: data distribution heterogeneity and high
resource costs brought by large foundation models. Specifically, the non-IID
data in different clients make existing FL algorithms hard to converge while
the high resource costs, including computational and communication costs that
increase the deployment difficulty in real-world scenarios. In this paper, we
propose an effective yet simple method, named FedCLIP, to achieve fast
generalization and personalization for CLIP in federated learning. Concretely,
we design an attention-based adapter for the large model, CLIP, and the rest
operations merely depend on adapters. Lightweight adapters can make the most
use of pretrained model information and ensure models be adaptive for clients
in specific tasks. Simultaneously, small-scale operations can mitigate the
computational burden and communication burden caused by large models. Extensive
experiments are conducted on three datasets with distribution shifts.
Qualitative and quantitative results demonstrate that FedCLIP significantly
outperforms other baselines (9% overall improvements on PACS) and effectively
reduces computational and communication costs (283x faster than FedAVG). Our
code will be available at: https://github.com/microsoft/PersonalizedFL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Data Engineering Bulletin; code is at:
  https://github.com/microsoft/PersonalizedFL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic
  Algorithm II (NSGA-II) <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08581v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08581v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Zheng, Benjamin Doerr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The non-dominated sorting genetic algorithm II (NSGA-II) is the most
intensively used multi-objective evolutionary algorithm (MOEA) in real-world
applications. However, in contrast to several simple MOEAs analyzed also via
mathematical means, no such study exists for the NSGA-II so far. In this work,
we show that mathematical runtime analyses are feasible also for the NSGA-II.
As particular results, we prove that with a population size four times larger
than the size of the Pareto front, the NSGA-II with two classic mutation
operators and four different ways to select the parents satisfies the same
asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic
OneMinMax and LeadingOnesTrailingZeros benchmarks. However, if the population
size is only equal to the size of the Pareto front, then the NSGA-II cannot
efficiently compute the full Pareto front: for an exponential number of
iterations, the population will always miss a constant fraction of the Pareto
front. Our experiments confirm the above findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the journal version of the paper "Weijie Zheng, Yufei Liu,
  Benjamin Doerr: A First Mathematical Runtime Analysis of the Non-Dominated
  Sorting Genetic Algorithm II (NSGA-II). AAAI 2022. arXiv:2112.08581v3"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.12468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.12468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihong Deng, Zuyue Fu, Lingxiao Wang, Zhuoran Yang, Chenjia Bai, Tianyi Zhou, Zhaoran Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) harnesses the power of massive datasets
for resolving sequential decision problems. Most existing papers only discuss
defending against out-of-distribution (OOD) actions while we investigate a
broader issue, the spurious correlations between epistemic uncertainty and
decision-making, an essential factor that causes suboptimality. In this paper,
we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically
effective and theoretically provable algorithm. We empirically show that SCORE
achieves the SoTA performance with 3.1x acceleration on various tasks in a
standard benchmark (D4RL). The proposed algorithm introduces an annealing
behavior cloning regularizer to help produce a high-quality estimation of
uncertainty which is critical for eliminating spurious correlations from
suboptimality. Theoretically, we justify the rationality of the proposed method
and prove its convergence to the optimal policy with a sublinear rate under
mild assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning, Fast and Slow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02370v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02370v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang Pham, Chenghao Liu, Steven C. H. Hoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2110.00175</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An <span class="highlight-title">Overview</span> on Generative AI at Scale with Edge-Cloud Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Cheng Wang, Jintang Xue, Chengwei Wei, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a specific category of artificial intelligence (AI), generative artificial
intelligence (GenAI) generates new content that resembles what is created by
humans. The rapid development of GenAI systems has created a huge amount of new
data on the Internet, posing new challenges to current computing and
communication frameworks. Currently, GenAI services rely on the traditional
cloud computing framework due to the need for large computation resources.
However, such services will encounter high latency because of data transmission
and a high volume of requests. On the other hand, edge-cloud computing can
provide adequate computation power and low latency at the same time through the
collaboration between edges and the cloud. Thus, it is attractive to build
GenAI systems at scale by leveraging the edge-cloud computing paradigm. In this
overview paper, we review recent developments in GenAI and edge-cloud
computing, respectively. Then, we use two exemplary GenAI applications to
discuss technical challenges in scaling up their solutions using edge-cloud
collaborative systems. Finally, we list design considerations for training and
deploying GenAI systems at scale and point out future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control
  for Empathetic Response Generation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqun Bi, Lei Shen, Yanan Cao, Meng Chen, Yuqiang Xie, Zheng Lin, Xiaodong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathy is a crucial factor in open-domain conversations, which naturally
shows one's caring and understanding to others. Though several methods have
been proposed to generate empathetic responses, existing works often lead to
monotonous empathy that refers to generic and safe expressions. In this paper,
we propose to use explicit control to guide the empathy expression and design a
framework DiffusEmp based on conditional diffusion language model to unify the
utilization of dialogue context and attribute-oriented control signals.
Specifically, communication mechanism, intent, and semantic frame are imported
as multi-grained signals that control the empathy realization from coarse to
fine levels. We then design a specific masking strategy to reflect the
relationship between multi-grained signals and response tokens, and integrate
it into the diffusion model to influence the generative process. Experimental
results on a benchmark dataset EmpatheticDialogue show that our framework
outperforms competitive baselines in terms of controllability, informativeness,
and diversity without the loss of context-relatedness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACL 2023 main conference (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Vocabulary Affordance Detection in 3D Point Clouds <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toan Nguyen, Minh Nhat Vu, An Vuong, Dzung Nguyen, Thieu Vo, Ngan Le, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affordance detection is a challenging problem with a wide variety of robotic
applications. Traditional affordance detection methods are limited to a
predefined set of affordance labels, hence potentially restricting the
adaptability of intelligent robots in complex and dynamic environments. In this
paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method,
which is capable of detecting an unbounded number of affordances in 3D point
clouds. By simultaneously learning the affordance text and the point feature,
OpenAD successfully exploits the semantic relationships between affordances.
Therefore, our proposed method enables zero-shot detection and can be able to
detect previously unseen affordances without a single annotation example.
Intensive experimental results show that OpenAD works effectively on a wide
range of affordance detection setups and outperforms other baselines by a large
margin. Additionally, we demonstrate the practicality of the proposed OpenAD in
real-world robotic applications with a fast inference speed (~100ms). Our
project is available at https://openad2023.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The 2023 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Indecision Trees: Learning Argument-Based Reasoning under Quantified
  Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan S. Kent, David H. Menager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using Machine Learning systems in the real world can often be problematic,
with inexplicable black-box models, the assumed certainty of imperfect
measurements, or providing a single classification instead of a probability
distribution.
  This paper introduces Indecision Trees, a modification to Decision Trees
which learn under uncertainty, can perform inference under uncertainty, provide
a robust distribution over the possible labels, and can be disassembled into a
set of logical arguments for use in other reasoning systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Coreference Resolution in Multiparty Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.01307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.01307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Patrick Xia, Mahsa Yarmohammadi, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multiparty dialogue datasets for entity coreference resolution are
nascent, and many challenges are still unaddressed. We create a large-scale
dataset, Multilingual Multiparty Coref (MMC), for this task based on TV
transcripts. Due to the availability of gold-quality subtitles in multiple
languages, we propose reusing the annotations to create silver coreference
resolution data in other languages (Chinese and Farsi) via annotation
projection. On the gold (English) data, off-the-shelf models perform relatively
poorly on MMC, suggesting that MMC has broader coverage of multiparty
coreference than prior datasets. On the silver data, we find success both using
it for data augmentation and training from scratch, which effectively simulates
the zero-shot cross-lingual setting.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-08T00:00:00Z">2023-07-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion-Guided Music Accompaniment Generation Based on Variational
  Autoencoder <span class="chip">IJCNN2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Wang, Shubing Zhang, Li Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music accompaniment generation is a crucial aspect in the composition
process. Deep neural networks have made significant strides in this field, but
it remains a challenge for AI to effectively incorporate human emotions to
create beautiful accompaniments. Existing models struggle to effectively
characterize human emotions within neural network models while composing music.
To address this issue, we propose the use of an easy-to-represent emotion flow
model, the Valence/Arousal Curve, which allows for the compatibility of
emotional information within the model through data transformation and enhances
interpretability of emotional factors by utilizing a Variational Autoencoder as
the model structure. Further, we used relative self-attention to maintain the
structure of the music at music phrase level and to generate a richer
accompaniment when combined with the rules of music theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted By International Joint Conference on Neural Networks
  2023(IJCNN2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On decoder-only architecture for speech-to-text and large language model
  integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success in the field of
natural language processing, enabling better human-computer interaction using
natural language. However, the seamless integration of speech signals into LLMs
has not been explored well. The "decoder-only" architecture has also not been
well studied for speech processing tasks. In this research, we introduce
Speech-LLaMA, a novel approach that effectively incorporates acoustic
information into text-based large language models. Our method leverages
Connectionist Temporal Classification and a simple audio encoder to map the
compressed acoustic features to the continuous semantic space of the LLM. In
addition, we further probe the decoder-only architecture for speech-to-text
tasks by training a smaller scale randomly initialized speech-LLaMA model from
speech-text paired data alone. We conduct experiments on multilingual
speech-to-text translation tasks and demonstrate a significant improvement over
strong baselines, highlighting the potential advantages of decoder-only models
for speech-to-text conversion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anti-noise window: Subjective perception of active noise reduction and
  effect of informational masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhan Lam, Kelvin Chee Quan Lim, Kenneth Ooi, Zhen-Ting Ong, Dongyuan Shi, Woon-Seng Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reviving natural ventilation (NV) for urban sustainability presents
challenges for indoor acoustic comfort. Active control and interference-based
noise mitigation strategies, such as the use of loudspeakers, offer potential
solutions to achieve acoustic comfort while maintaining NV. However, these
approaches are not commonly integrated or evaluated from a perceptual
standpoint. This study examines the perceptual and objective aspects of an
active-noise-control (ANC)-based "anti-noise" window (ANW) and its integration
with informational masking (IM) in a model bedroom. Forty participants assessed
the ANW in a three-way interaction involving noise types (traffic, train, and
aircraft), maskers (bird, water), and ANC (on, off). The evaluation focused on
perceived annoyance (PAY; ISO/TS 15666), perceived affective quality (ISO/TS
12913-2), loudness (PLN), and included an open-ended qualitative assessment.
Despite minimal objective reduction in decibel-based indicators and a slight
increase in psychoacoustic sharpness, the ANW alone demonstrated significant
reductions in PAY and PLN, as well as an improvement in ISO pleasantness across
all noise types. The addition of maskers generally enhanced overall acoustic
comfort, although water masking led to increased PLN. Furthermore, the
combination of ANC with maskers showed interaction effects, with both maskers
significantly reducing PAY compared to ANC alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript submitted to Sustainable Cities and Society</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling sounds emitted from physical object interactions is critical for
immersive perceptual experiences in real and virtual worlds. Traditional
methods of impact sound synthesis use physics simulation to obtain a set of
physics parameters that could represent and synthesize the sound. However, they
require fine details of both the object geometries and impact locations, which
are rarely available in the real world and can not be applied to synthesize
impact sounds from common videos. On the other hand, existing video-driven deep
learning-based approaches could only capture the weak correspondence between
visual content and impact sounds since they lack of physics knowledge. In this
work, we propose a physics-driven diffusion model that can synthesize
high-fidelity impact sound for a silent video clip. In addition to the video
content, we propose to use additional physics priors to guide the impact sound
synthesis procedure. The physics priors include both physics parameters that
are directly estimated from noisy real-world impact sound examples without
sophisticated setup and learned residual parameters that interpret the sound
environment via neural networks. We further implement a novel diffusion model
with specific training and inference strategies to combine physics priors and
visual information for impact sound synthesis. Experimental results show that
our model outperforms several existing systems in generating realistic impact
sounds. More importantly, the physics-based representations are fully
interpretable and transparent, thus enabling us to perform sound editing
flexibly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page:
  https://sukun1045.github.io/video-physics-sound-diffusion/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion-Guided Music Accompaniment Generation Based on Variational
  Autoencoder <span class="chip">IJCNN2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Wang, Shubing Zhang, Li Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music accompaniment generation is a crucial aspect in the composition
process. Deep neural networks have made significant strides in this field, but
it remains a challenge for AI to effectively incorporate human emotions to
create beautiful accompaniments. Existing models struggle to effectively
characterize human emotions within neural network models while composing music.
To address this issue, we propose the use of an easy-to-represent emotion flow
model, the Valence/Arousal Curve, which allows for the compatibility of
emotional information within the model through data transformation and enhances
interpretability of emotional factors by utilizing a Variational Autoencoder as
the model structure. Further, we used relative self-attention to maintain the
structure of the music at music phrase level and to generate a richer
accompaniment when combined with the rules of music theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted By International Joint Conference on Neural Networks
  2023(IJCNN2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On decoder-only architecture for speech-to-text and large language model
  integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success in the field of
natural language processing, enabling better human-computer interaction using
natural language. However, the seamless integration of speech signals into LLMs
has not been explored well. The "decoder-only" architecture has also not been
well studied for speech processing tasks. In this research, we introduce
Speech-LLaMA, a novel approach that effectively incorporates acoustic
information into text-based large language models. Our method leverages
Connectionist Temporal Classification and a simple audio encoder to map the
compressed acoustic features to the continuous semantic space of the LLM. In
addition, we further probe the decoder-only architecture for speech-to-text
tasks by training a smaller scale randomly initialized speech-LLaMA model from
speech-text paired data alone. We conduct experiments on multilingual
speech-to-text translation tasks and demonstrate a significant improvement over
strong baselines, highlighting the potential advantages of decoder-only models
for speech-to-text conversion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anti-noise window: Subjective perception of active noise reduction and
  effect of informational masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhan Lam, Kelvin Chee Quan Lim, Kenneth Ooi, Zhen-Ting Ong, Dongyuan Shi, Woon-Seng Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reviving natural ventilation (NV) for urban sustainability presents
challenges for indoor acoustic comfort. Active control and interference-based
noise mitigation strategies, such as the use of loudspeakers, offer potential
solutions to achieve acoustic comfort while maintaining NV. However, these
approaches are not commonly integrated or evaluated from a perceptual
standpoint. This study examines the perceptual and objective aspects of an
active-noise-control (ANC)-based "anti-noise" window (ANW) and its integration
with informational masking (IM) in a model bedroom. Forty participants assessed
the ANW in a three-way interaction involving noise types (traffic, train, and
aircraft), maskers (bird, water), and ANC (on, off). The evaluation focused on
perceived annoyance (PAY; ISO/TS 15666), perceived affective quality (ISO/TS
12913-2), loudness (PLN), and included an open-ended qualitative assessment.
Despite minimal objective reduction in decibel-based indicators and a slight
increase in psychoacoustic sharpness, the ANW alone demonstrated significant
reductions in PAY and PLN, as well as an improvement in ISO pleasantness across
all noise types. The addition of maskers generally enhanced overall acoustic
comfort, although water masking led to increased PLN. Furthermore, the
combination of ANC with maskers showed interaction effects, with both maskers
significantly reducing PAY compared to ANC alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript submitted to Sustainable Cities and Society</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Textless Spoken Language Understanding with Discrete Units as
  Intermediate Target 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan-Wei Wu, Guan-Ting Lin, Shang-Wen Li, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken Language Understanding (SLU) is a task that aims to extract semantic
information from spoken utterances. Previous research has made progress in
end-to-end SLU by using paired speech-text data, such as pre-trained Automatic
Speech Recognition (ASR) models or paired text as intermediate targets.
However, acquiring paired transcripts is expensive and impractical for
unwritten languages. On the other hand, Textless SLU extracts semantic
information from speech without utilizing paired transcripts. However, the
absence of intermediate targets and training guidance for textless SLU often
results in suboptimal performance. In this work, inspired by the
content-disentangled discrete units from self-supervised speech models, we
proposed to use discrete units as intermediate guidance to improve textless SLU
performance. Our method surpasses the baseline method on five SLU benchmark
corpora. Additionally, we find that unit guidance facilitates few-shot learning
and enhances the model's ability to handle noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2023. *Equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling sounds emitted from physical object interactions is critical for
immersive perceptual experiences in real and virtual worlds. Traditional
methods of impact sound synthesis use physics simulation to obtain a set of
physics parameters that could represent and synthesize the sound. However, they
require fine details of both the object geometries and impact locations, which
are rarely available in the real world and can not be applied to synthesize
impact sounds from common videos. On the other hand, existing video-driven deep
learning-based approaches could only capture the weak correspondence between
visual content and impact sounds since they lack of physics knowledge. In this
work, we propose a physics-driven diffusion model that can synthesize
high-fidelity impact sound for a silent video clip. In addition to the video
content, we propose to use additional physics priors to guide the impact sound
synthesis procedure. The physics priors include both physics parameters that
are directly estimated from noisy real-world impact sound examples without
sophisticated setup and learned residual parameters that interpret the sound
environment via neural networks. We further implement a novel diffusion model
with specific training and inference strategies to combine physics priors and
visual information for impact sound synthesis. Experimental results show that
our model outperforms several existing systems in generating realistic impact
sounds. More importantly, the physics-based representations are fully
interpretable and transparent, thus enabling us to perform sound editing
flexibly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page:
  https://sukun1045.github.io/video-physics-sound-diffusion/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold Filter-Combine Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joyce Chew, Edward De Brouwer, Smita Krishnaswamy, Deanna Needell, Michael Perlmutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a large class of manifold neural networks (MNNs) which we call
Manifold Filter-Combine Networks. This class includes as special cases, the
MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold
scattering transform (a wavelet-based model of neural networks), and other
interesting examples not previously considered in the literature such as the
manifold equivalent of Kipf and Welling's graph convolutional network. We then
consider a method, based on building a data-driven graph, for implementing such
networks when one does not have global knowledge of the manifold, but merely
has access to finitely many sample points. We provide sufficient conditions for
the network to provably converge to its continuum limit as the number of sample
points tends to infinity. Unlike previous work (which focused on specific MNN
architectures and graph constructions), our rate of convergence does not
explicitly depend on the number of filters used. Moreover, it exhibits linear
dependence on the depth of the network rather than the exponential dependence
obtained previously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Efficient Beamforming Design for Integrated Sensing and
  Communications Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Zou, Songlin Sun, Christos Masouros, Yuanhao Cui, Yafeng Liu, Derrick Wing Kwan Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the design of energy-efficient beamforming for
an ISAC system, where the transmitted waveform is optimized for joint
multi-user communication and target estimation simultaneously. We aim to
maximize the system energy efficiency (EE), taking into account the constraints
of a maximum transmit power budget, a minimum required
signal-to-interference-plus-noise ratio (SINR) for communication, and a maximum
tolerable Cramer-Rao bound (CRB) for target estimation. We first consider
communication-centric EE maximization. To handle the non-convex fractional
objective function, we propose an iterative quadratic-transform-Dinkelbach
method, where Schur complement and semi-definite relaxation (SDR) techniques
are leveraged to solve the subproblem in each iteration. For the scenarios
where sensing is critical, we propose a novel performance metric for
characterizing the sensing-centric EE and optimize the metric adopted in the
scenario of sensing a point-like target and an extended target. To handle the
nonconvexity, we employ the successive convex approximation (SCA) technique to
develop an efficient algorithm for approximating the nonconvex problem as a
sequence of convex ones. Furthermore, we adopt a Pareto optimization mechanism
to articulate the tradeoff between the communication-centric EE and
sensing-centric EE. We formulate the search of the Pareto boundary as a
constrained optimization problem and propose a computationally efficient
algorithm to handle it. Numerical results validate the effectiveness of our
proposed algorithms compared with the baseline schemes and the obtained
approximate Pareto boundary shows that there is a non-trivial tradeoff between
communication-centric EE and sensing-centric EE, where the number of
communication users and EE requirements have serious effects on the achievable
tradeoff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault Monitoring in Passive Optical Networks using Machine Learning
  Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khouloud Abdelli, Carsten Tropschug, Helmut Griesser, Stephan Pachnicke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive optical network (PON) systems are vulnerable to a variety of
failures, including fiber cuts and optical network unit (ONU)
transmitter/receiver failures. Any service interruption caused by a fiber cut
can result in huge financial losses for service providers or operators.
Identifying the faulty ONU becomes difficult in the case of nearly equidistant
branch terminations because the reflections from the branches overlap, making
it difficult to distinguish the faulty branch given the global backscattering
signal. With increasing network size, the complexity of fault monitoring in PON
systems increases, resulting in less reliable monitoring. To address these
challenges, we propose in this paper various machine learning (ML) approaches
for fault monitoring in PON systems, and we validate them using experimental
optical time domain reflectometry (OTDR) data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICTON 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Methods for MLE of Toeplitz Structured Covariance Matrices with
  Applications to RADAR Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Augusto Aubry, Prabhu Babu, Antonio De Maio, Massimo Rosamilia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work considers Maximum Likelihood Estimation (MLE) of a Toeplitz
structured covariance matrix. In this regard, an equivalent reformulation of
the MLE problem is introduced and two iterative algorithms are proposed for the
optimization of the equivalent statistical learning framework. Both the
strategies are based on the Majorization Minimization (MM) paradigm and hence
enjoy nice properties such as monotonicity and ensured convergence to a
stationary point of the equivalent MLE problem. The proposed framework is also
extended to deal with MLE of other practically relevant covariance structures,
namely, the banded Toeplitz, block Toeplitz, and Toeplitz-block-Toeplitz.
Through numerical simulations, it is shown that the new methods provide
excellent performance levels in terms of both mean square estimation error
(which is very close to the benchmark Cram\'er-Rao Bound (CRB)) and
signal-to-interference-plus-noise ratio, especially in comparison with state of
the art strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Transactions on Signal Processing. arXiv admin
  note: substantial text overlap with arXiv:2110.12176</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social-Mobility-Aware Joint Communication and Computation Resource
  Management in NOMA-Enabled Vehicular Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Xue, Haixia Zhang, Hui Ding, Dongfeng Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing computation and communication (2C) optimization schemes for
vehicular edge computing (VEC) networks mainly focus on the physical domain
without considering the influence from the social domain. This may greatly
limit the potential of task offloading, making it difficult to fully boom the
task offloading rate with given power, resulting in low energy efficiency (EE).
To address the issue, this letter devotes itself to investigate
social-mobility-aware VEC framework and proposes a novel EE-oriented 2C
assignment scheme. In doing so, we assume that the task vehicular user (T-VU)
can offload computation tasks to the service vehicular user (S-VU) and the road
side unit (RSU) by non-orthogonal multiple access (NOMA). An optimization
problem is formulated to jointly assign the 2C resources to maximize the system
EE, which turns out to be a mixed integer non-convex objective function. To
solve the problem, we transform it into separated computation and communication
resource allocation subproblems. Dealing with the first subproblem, we propose
a social-mobility-aware edge server selection and task splitting algorithm
(SM-SSTSA) to achieve edge server selection and task splitting. Then, by
solving the second subproblem, the power allocation and spectrum assignment
solutions are obtained utilizing a tightening lower bound method and a
Kuhn-Munkres algorithm. Finally, we solve the original problem through an
iterative method. Simulation results demonstrate the superior EE performance of
the proposed scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle
  Force and Joint Kinematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Shi, Shuhao Ma, Yihui Zhao, Zhiqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Muscle force and joint kinematics estimation from surface electromyography
(sEMG) are essential for real-time biomechanical analysis of the dynamic
interplay among neural muscle stimulation, muscle dynamics, and kinetics.
Recent advances in deep neural networks (DNNs) have shown the potential to
improve biomechanical analysis in a fully automated and reproducible manner.
However, the small sample nature and physical interpretability of biomechanical
analysis limit the applications of DNNs. This paper presents a novel
physics-informed low-shot learning method for sEMG-based estimation of muscle
force and joint kinematics. This method seamlessly integrates Lagrange's
equation of motion and inverse dynamic muscle model into the generative
adversarial network (GAN) framework for structured feature decoding and
extrapolated estimation from the small sample data. Specifically, Lagrange's
equation of motion is introduced into the generative model to restrain the
structured decoding of the high-level features following the laws of physics.
And a physics-informed policy gradient is designed to improve the adversarial
learning efficiency by rewarding the consistent physical representation of the
extrapolated estimations and the physical references. Experimental validations
are conducted on two scenarios (i.e. the walking trials and wrist motion
trials). Results indicate that the estimations of the muscle forces and joint
kinematics are unbiased compared to the physics-based inverse dynamics, which
outperforms the selected benchmark methods, including physics-informed
convolution neural network (PI-CNN), vallina generative adversarial network
(GAN), and multi-layer extreme learning machine (ML-ELM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV Trajectory Optimization for Directional THz Links Using Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Taghi Dabiri, Mazen Hasn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an alternative solution for quick disaster recovery of backhaul/fronthaul
links, in this paper, a dynamic unmanned aerial vehicles (UAV)-assisted
heterogeneous (HetNet) network equipped with directional terahertz (THz)
antennas is studied to solve the problem of transferring traffic of distributed
small cells. To this end, we first characterize a detailed three-dimensional
modeling of the dynamic UAV-assisted HetNet, and then, we formulate the problem
for UAV trajectory to minimize the maximum outage probability of directional
THz links. Then, using deep reinforcement learning (DRL) method, we propose an
efficient algorithm to learn the optimal trajectory. Finally, using
simulations, we investigate the performance of the proposed DRL-based
trajectory method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physical Layer Secret Key Agreement Using One-Bit Quantization and
  Low-Density Parity-Check Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John A. Snoap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical layer approaches for generating secret encryption keys for wireless
systems using channel information have attracted increased interest from
researchers in recent years. This paper presents a new approach for calculating
log-likelihood ratios (LLRs) for secret key generation that is based on one-bit
quantization of channel measurements and the difference between channel
estimates at legitimate reciprocal nodes. The studied secret key agreement
approach, which implements advantage distillation along with information
reconciliation using Slepian-Wolf low-density parity-check (LDPC) codes, is
discussed and illustrated with numerical results obtained from simulations.
These results show the probability of bit disagreement for keys generated using
the proposed LLR calculations compared with alternative LLR calculation methods
for key generation based on channel state information. The proposed LLR
calculations are shown to be an improvement to the studied approach of physical
layer secret key agreement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Officially Published on ODU Digital Commons at
  https://digitalcommons.odu.edu/ece_etds/13</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDT: A Low-cost and Topology-reconfigurable Testbed for Network Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Zhigao Zhao, Zijian Li, Jiang Shao, Sen Liu, Yang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network experiments are essential to network-related scientific research
(e.g., congestion control, QoS, network topology design, and traffic
engineering). However, (re)configuring various topologies on a real testbed is
expensive, time-consuming, and error-prone. In this paper, we propose
\emph{Software Defined Topology Testbed (SDT)}, a method for constructing a
user-defined network topology using a few commodity switches. SDT is low-cost,
deployment-friendly, and reconfigurable, which can run multiple sets of
experiments under different topologies by simply using different topology
configuration files at the controller we designed. We implement a prototype of
SDT and conduct numerous experiments. Evaluations show that SDT only introduces
at most 2\% extra overhead than full testbeds on multi-hop latency and is far
more efficient than software simulators (reducing the evaluation time by up to
2899x). SDT is more cost-effective and scalable than existing Topology
Projection (TP) solutions. Further experiments show that SDT can support
various network research experiments at a low cost on topics including but not
limited to topology design, congestion control, and traffic engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper will be published in IEEE CLUSTER 2023. Preview version
  only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cellular LTE and Solar Energy Harvesting for Long-Term, Reliable Urban
  Sensor Networks: Challenges and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Cabral, Vaishnavi Ranganathan, Jim Waldo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a world driven by data, cities are increasingly interested in deploying
networks of smart city devices for urban and environmental monitoring. To be
successful, these networks must be reliable, scalable, real-time, low-cost, and
easy to install and maintain -- criteria that are all significantly affected by
the design choices around connectivity and power. LTE networks and solar energy
can seemingly both satisfy the necessary criteria and are often used in
real-world sensor network deployments. However, there have not been extensive
real-world studies to examine how well such networks perform and the challenges
they encounter in urban settings over long periods. In this work, we analyze
the performance of a stationary 118-node LTE-connected, solar-powered sensor
network over one year in Chicago. Results show the promise of LTE networks and
solar panels for city-wide IoT deployments, but also reveal areas for
improvement. Notably, we find 11 sites with inadequate RSS to support sensing
nodes and over 33,000 hours of data loss due to solar energy availability
issues between October and March. Furthermore, we discover that the
neighborhoods most affected by connectivity and charging issues are
socioeconomically disadvantaged areas with a majority Black and Latine
residents. This work presents observations from a networking and powering
perspective of the urban sensor network to help drive reliable, scalable future
smart city deployments. The work also analyzes the impact of land use, adaptive
energy harvesting management strategies, and shortcomings of open data, to
support the need for increased real-world deployments that ensure the design of
equitable smart city networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Internet Localization of Multi-Party Relay Users: Inherent Friction
  Between Internet Services and User Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Flynn, Francesco Bronzino, Paul Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet privacy is increasingly important on the modern Internet. Users are
looking to control the trail of data that they leave behind on the systems that
they interact with. Multi-Party Relay (MPR) architectures lower the traditional
barriers to adoption of privacy enhancing technologies on the Internet. MPRs
are unique from legacy architectures in that they are able to offer privacy
guarantees without paying significant performance penalties. Apple's iCloud
Private Relay is a recently deployed MPR service, creating the potential for
widespread consumer adoption of the architecture. However, many current
Internet-scale systems are designed based on assumptions that may no longer
hold for users of privacy enhancing systems like Private Relay. There are
inherent tensions between systems that rely on data about users -- estimated
location of a user based on their IP address, for example -- and the trend
towards a more private Internet.
  This work studies a core function that is widely used to control network and
application behavior, IP geolocation, in the context of iCloud Private Relay
usage. We study the location accuracy of popular IP geolocation services
compared against the published location dataset that Apple publicly releases to
explicitly aid in geolocating PR users. We characterize geolocation service
performance across a number of dimensions, including different countries, IP
version, infrastructure provider, and time. Our findings lead us to conclude
that existing approaches to IP geolocation (e.g., frequently updated databases)
perform inadequately for users of the MPR architecture. For example, we find
median location errors >1,000 miles in some countries for IPv4 addresses using
IP2Location. Our findings lead us to conclude that new, privacy-focused,
techniques for inferring user location may be required as privacy becomes a
default user expectation on the Internet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BER Analysis of Full Duplex Relay assisted BPSK-SIM based VLC System for
  Indoor Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        L Bhargava Kumar, Ramavath Prasad Naik, Datta Choudhari, Prabu Krishnan, Goutham Simha G D, Jagadeesh V K
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper contemplates a relay-assisted visible light communication (VLC)
system, where the light source (Table lamp) acts as a relay node and cooperates
with the main light source. Following the IEEE 802.15.7r1 VLC reference channel
model, we assume that there are two different light sources present in an
office room. The first one is the source terminal present on the ceiling and
another one is the desk lamp that serves as the relay station which works in
full-duplex method. Because of the loop interference channel, we model VLC
relay terminal using ray tracing simulations. We have analyzed bit error rate
(BER) performance of the relay-assisted VLC system using binary phase shift
keying-subcarrier intensity modulation (BPSK-SIM) technique. The proposed
method outperforms existing phase shift keying (PSK) and square M-quadrature
amplitude modulation (M-QAM) techniques. The proposed VLC system using BPSK-SIM
technique achieves a BER performance of for an SNR of 20 dB. The results of
proposed full duplex and half duplex relayed VLC system are evaluated using
equal power allocation (EPA) and optimum power allocations (OPA) techniques
over three different modulation schemes which are 2-PSK, square M-QAM,
BPSK-SIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secrets Revealed in Container Images: An Internet-wide Study on
  Occurrence and Impact 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Dahlmanns, Constantin Sander, Robin Decker, Klaus Wehrle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Containerization allows bundling applications and their dependencies into a
single image. The containerization framework Docker eases the use of this
concept and enables sharing images publicly, gaining high momentum. However, it
can lead to users creating and sharing images that include private keys or API
secrets-either by mistake or out of negligence. This leakage impairs the
creator's security and that of everyone using the image. Yet, the extent of
this practice and how to counteract it remains unclear.
  In this paper, we analyze 337,171 images from Docker Hub and 8,076 other
private registries unveiling that 8.5% of images indeed include secrets.
Specifically, we find 52,107 private keys and 3,158 leaked API secrets, both
opening a large attack surface, i.e., putting authentication and
confidentiality of privacy-sensitive data at stake and even allow active
attacks. We further document that those leaked keys are used in the wild: While
we discovered 1,060 certificates relying on compromised keys being issued by
public certificate authorities, based on further active Internet measurements,
we find 275,269 TLS and SSH hosts using leaked private keys for authentication.
To counteract this issue, we discuss how our methodology can be used to prevent
secret leakage and reuse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault Monitoring in Passive Optical Networks using Machine Learning
  Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khouloud Abdelli, Carsten Tropschug, Helmut Griesser, Stephan Pachnicke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive optical network (PON) systems are vulnerable to a variety of
failures, including fiber cuts and optical network unit (ONU)
transmitter/receiver failures. Any service interruption caused by a fiber cut
can result in huge financial losses for service providers or operators.
Identifying the faulty ONU becomes difficult in the case of nearly equidistant
branch terminations because the reflections from the branches overlap, making
it difficult to distinguish the faulty branch given the global backscattering
signal. With increasing network size, the complexity of fault monitoring in PON
systems increases, resulting in less reliable monitoring. To address these
challenges, we propose in this paper various machine learning (ML) approaches
for fault monitoring in PON systems, and we validate them using experimental
optical time domain reflectometry (OTDR) data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICTON 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing a Direct Feedback Loop between Humans and Convolutional Neural
  Networks through Local Explanations <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Steven Sun, Yuyang Gao, Shubham Khaladkar, Sijia Liu, Liang Zhao, Young-Ho Kim, Sungsoo Ray Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The local explanation provides heatmaps on images to explain how
Convolutional Neural Networks (CNNs) derive their output. Due to its visual
straightforwardness, the method has been one of the most popular explainable AI
(XAI) methods for diagnosing CNNs. Through our formative study (S1), however,
we captured ML engineers' ambivalent perspective about the local explanation as
a valuable and indispensable envision in building CNNs versus the process that
exhausts them due to the heuristic nature of detecting vulnerability. Moreover,
steering the CNNs based on the vulnerability learned from the diagnosis seemed
highly challenging. To mitigate the gap, we designed DeepFuse, the first
interactive design that realizes the direct feedback loop between a user and
CNNs in diagnosing and revising CNN's vulnerability using local explanations.
DeepFuse helps CNN engineers to systemically search "unreasonable" local
explanations and annotate the new boundaries for those identified as
unreasonable in a labor-efficient manner. Next, it steers the model based on
the given annotation such that the model doesn't introduce similar mistakes. We
conducted a two-day study (S2) with 12 experienced CNN engineers. Using
DeepFuse, participants made a more accurate and "reasonable" model than the
current state-of-the-art. Also, participants found the way DeepFuse guides
case-based reasoning can practically improve their current practice. We provide
implications for design that explain how future HCI-driven design can move our
practice forward to make XAI-driven insights more actionable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 6 figures, 5 tables. Accepted for publication in the
  Proceedings of the ACM on Human-Computer Interaction (PACM HCI), CSCW 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Room Security and Automating Class Attendance Using ID Cards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shravan Bhat, Nithin R, Pranav S
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancements in technology, automation has emerged as the
future of human endeavors. From simple tasks like attendance management to
complex security systems, automation has the potential to revolutionize various
aspects of our lives. This research paper explores the implementation of a
method aimed at enhancing room security in hostels and automating class
attendance using ID cards. In this study, we propose a system that utilizes the
unique identity information stored in ID cards for various security and
check-in tasks. By integrating RFID (Radio-Frequency Identification) reader
technology, GSM modules, Node MCU, and Arduino, we create a comprehensive
solution. The RFID reader scans the ID card, extracting the relevant
information and verifying the user's identity. The data is then transmitted via
the GSM module to a central database, ensuring real-time monitoring and
security measures. Moreover, the system also enables the automation of class
attendance. By utilizing the same ID cards, students can simply tap their cards
on a reader placed in the classroom. This information is recorded
automatically, eliminating the need for manual attendance taking and reducing
errors and time consumption. This research project highlights the practical
implementation of ID card technology to enhance room security in hostels and
automate class attendance processes. By leveraging the power of automation, we
aim to streamline administrative tasks, improve security measures, and optimize
efficiency in educational institutions and other relevant settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying human-centered AI in developing effective human-AI teaming: A
  perspective of human-AI joint cognitive systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xu, Zaifeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the "teaming" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
  Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Prototypical Part Networks with Reward Reweighing,
  Reselection, and Retraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Netzorg, Jiaxun Li, Bin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model's output to specific
features of the data. One such of these methods is the prototypical part
network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
this method often learns to classify from spurious or inconsistent parts of the
image. Hoping to remedy this, we take inspiration from the recent developments
in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns to
identify non-spurious prototypes. In place of a full RL update, we propose the
reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet),
which adds an additional three steps to the ProtoPNet training loop. The first
two steps are reward-based reweighting and reselection, which align prototypes
with human feedback. The final step is retraining to realign the model's
features with the updated prototypes. We find that R3-ProtoPNet improves the
overall consistency and meaningfulness of the prototypes, but lower the test
predictive accuracy when used independently. When multiple R3-ProtoPNets are
incorporated into an ensemble, we find an increase in test predictive
performance while maintaining interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Mixed-Initiative Video Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daijin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Artificial Intelligence (AI) enables humans to co-create
content with machines. The unexpectedness of AI-generated content can bring
inspiration and entertainment to users. However, the co-creation interactions
are always designed for content creators and have poor accessibility. To
explore gamification of mixed-initiative co-creation and make human-AI
interactions accessible and fun for players, I prototyped Snake Story, a
mixed-initiative game where players can select AI-generated texts to write a
story of a snake by playing a "Snake" like game. A controlled experiment was
conducted to investigate the dynamics of player-AI interactions with and
without the game component in the designed interface. As a result of a study
with 11 players (n=11), I found that players utilized different strategies when
playing with the two versions, game mechanics significantly affected the output
stories, players' creative process, as well as role perceptions, and players
with different backgrounds showed different preferences for the two versions.
Based on these results, I further discussed considerations for mixed-initiative
game design. This work aims to inspire the design of engaging co-creation
experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A computational medical XR discipline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.04136v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.04136v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Papagiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational medical XR (extended reality) brings together life sciences and
neuroscience with mathematics, engineering, and computer science. It unifies
computational science (scientific computing) with intelligent extended reality
and spatial computing for the medical field. It significantly extends previous
Clinical XR, by integrating computational methods from neural simulation to
computational geometry, computational vision and computer graphics up to
theoretical computer science to solve hard problems in medicine and
neuroscience: from low-code/no-code authoring medical XR platforms to deep
learning systems for diagnostics, therapeutics, rehabilitation and from
surgical planning to real-time operative navigation in XR.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">39</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Dynamic Pricing with Strategic Buyers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pangpang Liu, Zhuoran Yang, Zhaoran Wang, Will Wei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized pricing, which involves tailoring prices based on individual
characteristics, is commonly used by firms to implement a consumer-specific
pricing policy. In this process, buyers can also strategically manipulate their
feature data to obtain a lower price, incurring certain manipulation costs.
Such strategic behavior can hinder firms from maximizing their profits. In this
paper, we study the contextual dynamic pricing problem with strategic buyers.
The seller does not observe the buyer's true feature, but a manipulated feature
according to buyers' strategic behavior. In addition, the seller does not
observe the buyers' valuation of the product, but only a binary response
indicating whether a sale happens or not. Recognizing these challenges, we
propose a strategic dynamic pricing policy that incorporates the buyers'
strategic behavior into the online learning to maximize the seller's cumulative
revenue. We first prove that existing non-strategic pricing policies that
neglect the buyers' strategic behavior result in a linear $\Omega(T)$ regret
with $T$ the total time horizon, indicating that these policies are not better
than a random pricing policy. We then establish that our proposed policy
achieves a sublinear regret upper bound of $O(\sqrt{T})$. Importantly, our
policy is not a mere amalgamation of existing dynamic pricing policies and
strategic behavior handling algorithms. Our policy can also accommodate the
scenario when the marginal cost of manipulation is unknown in advance. To
account for it, we simultaneously estimate the valuation parameter and the cost
parameter in the online pricing policy, which is shown to also achieve an
$O(\sqrt{T})$ regret bound. Extensive experiments support our theoretical
developments and demonstrate the superior performance of our policy compared to
other pricing policies that are unaware of the strategic behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization-based Learning for Dynamic Load Planning in Trucking
  Service Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritesh Ojha, Wenbo Chen, Hanyu Zhang, Reem Khir, Alan Erera, Pascal Van Hentenryck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The load planning problem is a critical challenge in service network design
for parcel carriers: it decides how many trailers (or loads) to assign for
dispatch over time between pairs of terminals. Another key challenge is to
determine a flow plan, which specifies how parcel volumes are assigned to
planned loads. This paper considers the Dynamic Load Planning Problem (DLPP)
that considers both flow and load planning challenges jointly to adjust loads
and flows as the demand forecast changes over time before the day of
operations. The paper aims at developing a decision-support tool to inform
planners making these decisions at terminals across the network. The paper
formulates the DLPP as a MIP and shows that it admits a large number of
symmetries in a network where each commodity can be routed through primary and
alternate paths. As a result, an optimization solver may return fundamentally
different solutions to closely related problems, confusing planners and
reducing trust in optimization. To remedy this limitation, the paper proposes a
Goal-Directed Optimization that eliminates those symmetries by generating
optimal solutions staying close to a reference plan. The paper also proposes an
optimization proxy to address the computational challenges of the optimization
models. The proxy combines a machine learning model and a feasibility
restoration model and finds solutions that satisfy real-time constraints
imposed by planners-in-the-loop. An extensive computational study on industrial
instances shows that the optimization proxy is around 10 times faster than the
commercial solver in obtaining the same quality solutions and orders of
magnitude faster for generating solutions that are consistent with each other.
The proposed approach also demonstrates the benefits of the DLPP for load
consolidation, and the significant savings obtained from combining machine
learning and optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing a Direct Feedback Loop between Humans and Convolutional Neural
  Networks through Local Explanations <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Steven Sun, Yuyang Gao, Shubham Khaladkar, Sijia Liu, Liang Zhao, Young-Ho Kim, Sungsoo Ray Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The local explanation provides heatmaps on images to explain how
Convolutional Neural Networks (CNNs) derive their output. Due to its visual
straightforwardness, the method has been one of the most popular explainable AI
(XAI) methods for diagnosing CNNs. Through our formative study (S1), however,
we captured ML engineers' ambivalent perspective about the local explanation as
a valuable and indispensable envision in building CNNs versus the process that
exhausts them due to the heuristic nature of detecting vulnerability. Moreover,
steering the CNNs based on the vulnerability learned from the diagnosis seemed
highly challenging. To mitigate the gap, we designed DeepFuse, the first
interactive design that realizes the direct feedback loop between a user and
CNNs in diagnosing and revising CNN's vulnerability using local explanations.
DeepFuse helps CNN engineers to systemically search "unreasonable" local
explanations and annotate the new boundaries for those identified as
unreasonable in a labor-efficient manner. Next, it steers the model based on
the given annotation such that the model doesn't introduce similar mistakes. We
conducted a two-day study (S2) with 12 experienced CNN engineers. Using
DeepFuse, participants made a more accurate and "reasonable" model than the
current state-of-the-art. Also, participants found the way DeepFuse guides
case-based reasoning can practically improve their current practice. We provide
implications for design that explain how future HCI-driven design can move our
practice forward to make XAI-driven insights more actionable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 6 figures, 5 tables. Accepted for publication in the
  Proceedings of the ACM on Human-Computer Interaction (PACM HCI), CSCW 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Variational Neighbor Labels for Test-Time Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sameer Ambekar, Zehao Xiao, Jiayi Shen, Xiantong Zhen, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper strives for domain generalization, where models are trained
exclusively on source domains before being deployed at unseen target domains.
We follow the strict separation of source training and target testing but
exploit the value of the unlabeled target data itself during inference. We make
three contributions. First, we propose probabilistic pseudo-labeling of target
samples to generalize the source-trained model to the target domain at test
time. We formulate the generalization at test time as a variational inference
problem by modeling pseudo labels as distributions to consider the uncertainty
during generalization and alleviate the misleading signal of inaccurate pseudo
labels. Second, we learn variational neighbor labels that incorporate the
information of neighboring target samples to generate more robust pseudo
labels. Third, to learn the ability to incorporate more representative target
information and generate more precise and robust variational neighbor labels,
we introduce a meta-generalization stage during training to simulate the
generalization procedure. Experiments on six widely-used datasets demonstrate
the benefits, abilities, and effectiveness of our proposal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On "Indifference" and Backward Induction in Games with Perfect
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nimrod Megiddo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indifference of a player with respect to two distinct outcomes of a game
cannot be handled by small perturbations, because the actual choice may have
significant impact on other players, and cause them to act in a way that has
significant impact of the indifferent player. It is argued that ties among
rational choices can be resolved by refinements of the concept of rationality
based on the utilities of other players. One such refinement is the concept of
Tit-for-Tat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring the Success of Diffusion Models at Imitating Human Artists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Casper, Zifan Guo, Shreya Mogulothu, Zachary Marinov, Chinmay Deshpande, Rui-Jie Yew, Zheng Dai, Dylan Hadfield-Menell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern diffusion models have set the state-of-the-art in AI image generation.
Their success is due, in part, to training on Internet-scale data which often
includes copyrighted work. This prompts questions about the extent to which
these models learn from, imitate, or copy the work of human artists. This work
suggests that tying copyright liability to the capabilities of the model may be
useful given the evolving ecosystem of generative models. Specifically, much of
the legal analysis of copyright and generative systems focuses on the use of
protected data for training. As a result, the connections between data,
training, and the system are often obscured. In our approach, we consider
simple image classification techniques to measure a model's ability to imitate
specific artists. Specifically, we use Contrastive Language-Image Pretrained
(CLIP) encoders to classify images in a zero-shot fashion. Our process first
prompts a model to imitate a specific artist. Then, we test whether CLIP can be
used to reclassify the artist (or the artist's work) from the imitation. If
these tests match the imitation back to the original artist, this suggests the
model can imitate that artist's expression. Our approach is simple and
quantitative. Furthermore, it uses standard techniques and does not require
additional training. We demonstrate our approach with an audit of Stable
Diffusion's capacity to imitate 70 professional digital artists with
copyrighted work online. When Stable Diffusion is prompted to imitate an artist
from this set, we find that the artist can be identified from the imitation
with an average accuracy of 81.0%. Finally, we also show that a sample of the
artist's work can be matched to these imitation images with a high degree of
statistical reliability. Overall, these results suggest that Stable Diffusion
is broadly successful at imitating individual human artists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 1 st Workshop on Generative AI and Law</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered
  Environments <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ihab S. Mohamed, Mahmoud Ali, Lantao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic navigation in unknown, cluttered environments with limited sensing
capabilities poses significant challenges in robotics. Local trajectory
optimization methods, such as Model Predictive Path Intergal (MPPI), are a
promising solution to this challenge. However, global guidance is required to
ensure effective navigation, especially when encountering challenging
environmental conditions or navigating beyond the planning horizon. This study
presents the GP-MPPI, an online learning-based control strategy that integrates
MPPI with a local perception model based on Sparse Gaussian Process (SGP). The
key idea is to leverage the learning capability of SGP to construct a variance
(uncertainty) surface, which enables the robot to learn about the navigable
space surrounding it, identify a set of suggested subgoals, and ultimately
recommend the optimal subgoal that minimizes a predefined cost function to the
local MPPI planner. Afterward, MPPI computes the optimal control sequence that
satisfies the robot and collision avoidance constraints. Such an approach
eliminates the necessity of a global map of the environment or an offline
training process. We validate the efficiency and robustness of our proposed
control strategy through both simulated and real-world experiments of 2D
autonomous navigation tasks in complex unknown environments, demonstrating its
superiority in guiding the robot safely towards its desired goal while avoiding
obstacles and escaping entrapment in local minima. The GPU implementation of
GP-MPPI, including the supplementary video, is available at
https://github.com/IhabMohamed/GP-MPPI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has 8 pages, 6 figures, 2 tables. It has been accepted for
  publication at the IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS), Detroit, Michigan, USA, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proceedings Ninetheenth conference on Theoretical Aspects of Rationality
  and Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rineke Verbrugge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a
conference that aims to bring together researchers from a wide variety of
fields, including computer science, artificial intelligence, game theory,
decision theory, philosophy, logic, linguistics, and cognitive science. Its
goal is to further our understanding of interdisciplinary issues involving
reasoning about rationality and knowledge.
  Previous conferences have been held biennially around the world since 1986,
on the initiative of Joe Halpern (Cornell University). Topics of interest
include, but are not limited to, semantic models for knowledge, belief,
awareness and uncertainty, bounded rationality and resource-bounded reasoning,
commonsense epistemic reasoning, epistemic logic, epistemic game theory,
knowledge and action, applications of reasoning about knowledge and other
mental states, belief revision, computational social choice, algorithmic game
theory, and foundations of multi-agent systems. Information about TARK,
including conference proceedings, is available at the website
http://www.tark.org/
  These proceedings contain the papers that have been accepted for presentation
at the Nineteenth Conference on Theoretical Aspects of Rationality and
Knowledge (TARK 2023), held between June 28 and June 30, 2023, at the
University of Oxford, United Kingdom. The conference website can be found at
https://sites.google.com/view/tark-2023
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PCG-based Static Underground Garage Scenario Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjin Li, Kai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving technology has five levels, from L0 to L5. Currently, only
the L2 level (partial automation) can be achieved, and there is a long way to
go before reaching the final level of L5 (full automation). The key to crossing
these levels lies in training the autonomous driving model. However, relying
solely on real-world road data to train the model is far from enough and
consumes a great deal of resources. Although there are already examples of
training autonomous driving models through simulators that simulate real-world
scenarios, these scenarios require complete manual construction. Directly
converting 3D scenes from road network formats will lack a large amount of
detail and cannot be used as training sets. Underground parking garage static
scenario simulation is regarded as a procedural content generation (PCG)
problem. This paper will use the Sarsa algorithm to solve procedural content
generation on underground garage structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomy 2.0: The Quest for Economies of Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Wu, Bo Yu, Shaoshan Liu, Yuhao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of robotics and AI technologies in the past decade, we
have now entered the age of autonomous machines. In this new age of information
technology, autonomous machines, such as service robots, autonomous drones,
delivery robots, and autonomous vehicles, rather than humans, will provide
services. In this article, through examining the technical challenges and
economic impact of the digital economy, we argue that scalability is both
highly necessary from a technical perspective and significantly advantageous
from an economic perspective, thus is the key for the autonomy industry to
achieve its full potential. Nonetheless, the current development paradigm,
dubbed Autonomy 1.0, scales with the number of engineers, instead of with the
amount of data or compute resources, hence preventing the autonomy industry to
fully benefit from the economies of scale, especially the exponentially
cheapening compute cost and the explosion of available data. We further analyze
the key scalability blockers and explain how a new development paradigm, dubbed
Autonomy 2.0, can address these problems to greatly boost the autonomy
industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Intent Detection in User Provided Annotations for Programming by
  Examples Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nischal Ashok Kumar, Nitin Gupta, Shanmukha Guttula, Hima Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In mapping enterprise applications, data mapping remains a fundamental part
of integration development, but its time consuming. An increasing number of
applications lack naming standards, and nested field structures further add
complexity for the integration developers. Once the mapping is done, data
transformation is the next challenge for the users since each application
expects data to be in a certain format. Also, while building integration flow,
developers need to understand the format of the source and target data field
and come up with transformation program that can change data from source to
target format. The problem of automatic generation of a transformation program
through program synthesis paradigm from some specifications has been studied
since the early days of Artificial Intelligence (AI). Programming by Example
(PBE) is one such kind of technique that targets automatic inferencing of a
computer program to accomplish a format or string conversion task from
user-provided input and output samples. To learn the correct intent, a diverse
set of samples from the user is required. However, there is a possibility that
the user fails to provide a diverse set of samples. This can lead to multiple
intents or ambiguity in the input and output samples. Hence, PBE systems can
get confused in generating the correct intent program. In this paper, we
propose a deep neural network based ambiguity prediction model, which analyzes
the input-output strings and maps them to a different set of properties
responsible for multiple intent. Users can analyze these properties and
accordingly can provide new samples or modify existing samples which can help
in building a better PBE system for mapping enterprise applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Right to be Forgotten in the Era of Large Language Models: Implications,
  Challenges, and Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, Xiwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Right to be Forgotten (RTBF) was first established as the result of the
ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and
was later included as the Right to Erasure under the General Data Protection
Regulation (GDPR) of European Union to allow individuals the right to request
personal data be deleted by organizations. Specifically for search engines,
individuals can send requests to organizations to exclude their information
from the query results. With the recent development of Large Language Models
(LLMs) and their use in chatbots, LLM-enabled software systems have become
popular. But they are not excluded from the RTBF. Compared with the indexing
approach used by search engines, LLMs store, and process information in a
completely different way. This poses new challenges for compliance with the
RTBF. In this paper, we explore these challenges and provide our insights on
how to implement technical solutions for the RTBF, including the use of machine
unlearning, model editing, and prompting engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inductive Meta-path Learning for Schema-complex Heterogeneous
  Information Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixuan Liu, Changjun Fan, Kewei Cheng, Yunfei Wang, Peng Cui, Yizhou Sun, Zhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous Information Networks (HINs) are information networks with
multiple types of nodes and edges. The concept of meta-path, i.e., a sequence
of entity types and relation types connecting two entities, is proposed to
provide the meta-level explainable semantics for various HIN tasks.
Traditionally, meta-paths are primarily used for schema-simple HINs, e.g.,
bibliographic networks with only a few entity types, where meta-paths are often
enumerated with domain knowledge. However, the adoption of meta-paths for
schema-complex HINs, such as knowledge bases (KBs) with hundreds of entity and
relation types, has been limited due to the computational complexity associated
with meta-path enumeration. Additionally, effectively assessing meta-paths
requires enumerating relevant path instances, which adds further complexity to
the meta-path learning process. To address these challenges, we propose
SchemaWalk, an inductive meta-path learning framework for schema-complex HINs.
We represent meta-paths with schema-level representations to support the
learning of the scores of meta-paths for varying relations, mitigating the need
of exhaustive path instance enumeration for each relation. Further, we design a
reinforcement-learning based path-finding agent, which directly navigates the
network schema (i.e., schema graph) to learn policies for establishing
meta-paths with high coverage and confidence for multiple relations. Extensive
experiments on real data sets demonstrate the effectiveness of our proposed
paradigm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient In-memory Computing Hardware for Quantized Neural
  Networks: State-of-the-art, Open Challenges and Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Krestinskaya, Li Zhang, Khaled Nabil Salama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The amount of data processed in the cloud, the development of
Internet-of-Things (IoT) applications, and growing data privacy concerns force
the transition from cloud-based to edge-based processing. Limited energy and
computational resources on edge push the transition from traditional von
Neumann architectures to In-memory Computing (IMC), especially for machine
learning and neural network applications. Network compression techniques are
applied to implement a neural network on limited hardware resources.
Quantization is one of the most efficient network compression techniques
allowing to reduce the memory footprint, latency, and energy consumption. This
paper provides a comprehensive review of IMC-based Quantized Neural Networks
(QNN) and links software-based quantization approaches to IMC hardware
implementation. Moreover, open challenges, QNN design requirements,
recommendations, and perspectives along with an IMC-based QNN hardware roadmap
are provided.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bounding data reconstruction attacks with the hypothesis testing
  interpretation of differential privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Kaissis, Jamie Hayes, Alexander Ziller, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore Reconstruction Robustness (ReRo), which was recently proposed as
an upper bound on the success of data reconstruction attacks against machine
learning models. Previous research has demonstrated that differential privacy
(DP) mechanisms also provide ReRo, but so far, only asymptotic Monte Carlo
estimates of a tight ReRo bound have been shown. Directly computable ReRo
bounds for general DP mechanisms are thus desirable. In this work, we establish
a connection between hypothesis testing DP and ReRo and derive closed-form,
analytic or numerical ReRo bounds for the Laplace and Gaussian mechanisms and
their subsampled variants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying human-centered AI in developing effective human-AI teaming: A
  perspective of human-AI joint cognitive systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xu, Zaifeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the "teaming" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
  Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScriptWorld: Text Based Environment For Learning Procedural Knowledge <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Joshi, Areeb Ahmad, Umang Pandey, Ashutosh Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based games provide a framework for developing natural language
understanding and commonsense knowledge about the world in reinforcement
learning based agents. Existing text-based environments often rely on fictional
situations and characters to create a gaming framework and are far from
real-world scenarios. In this paper, we introduce ScriptWorld: a text-based
environment for teaching agents about real-world daily chores and hence
imparting commonsense knowledge. To the best of our knowledge, it is the first
interactive text-based gaming framework that consists of daily real-world human
activities designed using scripts dataset. We provide gaming environments for
10 daily activities and perform a detailed analysis of the proposed
environment. We develop RL-based baseline models/agents to play the games in
Scriptworld. To understand the role of language models in such environments, we
leverage features obtained from pre-trained language models in the RL agents.
Our experiments show that prior knowledge obtained from a pre-trained language
model helps to solve real-world text-based gaming environments. We release the
environment via Github: https://github.com/Exploration-Lab/ScriptWorld
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IJCAI 2023, 26 Pages (7 main + 19 for appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Prototypical Part Networks with Reward Reweighing,
  Reselection, and Retraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Netzorg, Jiaxun Li, Bin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model's output to specific
features of the data. One such of these methods is the prototypical part
network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
this method often learns to classify from spurious or inconsistent parts of the
image. Hoping to remedy this, we take inspiration from the recent developments
in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns to
identify non-spurious prototypes. In place of a full RL update, we propose the
reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet),
which adds an additional three steps to the ProtoPNet training loop. The first
two steps are reward-based reweighting and reselection, which align prototypes
with human feedback. The final step is retraining to realign the model's
features with the updated prototypes. We find that R3-ProtoPNet improves the
overall consistency and meaningfulness of the prototypes, but lower the test
predictive accuracy when used independently. When multiple R3-ProtoPNets are
incorporated into an ensemble, we find an increase in test predictive
performance while maintaining interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Mixed-Initiative Video Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daijin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Artificial Intelligence (AI) enables humans to co-create
content with machines. The unexpectedness of AI-generated content can bring
inspiration and entertainment to users. However, the co-creation interactions
are always designed for content creators and have poor accessibility. To
explore gamification of mixed-initiative co-creation and make human-AI
interactions accessible and fun for players, I prototyped Snake Story, a
mixed-initiative game where players can select AI-generated texts to write a
story of a snake by playing a "Snake" like game. A controlled experiment was
conducted to investigate the dynamics of player-AI interactions with and
without the game component in the designed interface. As a result of a study
with 11 players (n=11), I found that players utilized different strategies when
playing with the two versions, game mechanics significantly affected the output
stories, players' creative process, as well as role perceptions, and players
with different backgrounds showed different preferences for the two versions.
Based on these results, I further discussed considerations for mixed-initiative
game design. This work aims to inspire the design of engaging co-creation
experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Supply Chain Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, Ishai Menache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supply chain operations traditionally involve a variety of complex decision
making problems. Over the last few decades, supply chains greatly benefited
from advances in computation, which allowed the transition from manual
processing to automation and cost-effective optimization. Nonetheless, business
operators still need to spend substantial efforts in \emph{explaining} and
interpreting the optimization outcomes to stakeholders. Motivated by the recent
advances in Large Language Models (LLMs), we study how this disruptive
technology can help bridge the gap between supply chain automation and human
comprehension and trust thereof. We design \name{} -- a framework that accepts
as input queries in plain text, and outputs insights about the underlying
optimization outcomes. Our framework does not forgo the state-of-the-art
combinatorial optimization technology, but rather leverages it to
quantitatively answer what-if scenarios (e.g., how would the cost change if we
used supplier B instead of supplier A for a given demand?). Importantly, our
design does not require sending proprietary data over to LLMs, which can be a
privacy concern in some circumstances. We demonstrate the effectiveness of our
framework on a real server placement scenario within Microsoft's cloud supply
chain. Along the way, we develop a general evaluation benchmark, which can be
used to evaluate the accuracy of the LLM output in other scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in
  Digital Pathology: A Step Closer to Widescale Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amanda Dy, Ngoc-Nhu Jennifer Nguyen, Seyed Hossein Mirjahanmardi, Melanie Dawe, Anthony Fyles, Wei Shi, Fei-Fei Liu, Dimitrios Androutsos, Susan Done, April Khademi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning systems have been proposed to improve the objectivity and
efficiency of Ki- 67 PI scoring. The challenge is that while very accurate,
deep learning techniques suffer from reduced performance when applied to
out-of-domain data. This is a critical challenge for clinical translation, as
models are typically trained using data available to the vendor, which is not
from the target domain. To address this challenge, this study proposes a domain
adaptation pipeline that employs an unsupervised framework to generate silver
standard (pseudo) labels in the target domain, which is used to augment the
gold standard (GS) source domain data. Five training regimes were tested on two
validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained
on target silver standard (SS) labels, (2) GS Only: trained on source GS
labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS:
trained on source GS labels and fine-tuned on target SS labels, and our
proposed method (5) SS+GS: trained on source SS labels and fine-tuned on source
GS labels. The SS+GS method yielded significantly (p < 0.05) higher PI accuracy
(95.9%) and more consistent results compared to the GS Only model on target
data. Analysis of t-SNE plots showed features learned by the SS+GS models are
more aligned for source and target data, resulting in improved generalization.
The proposed pipeline provides an efficient method for learning the target
distribution without manual annotations, which are time-consuming and costly to
generate for medical images. This framework can be applied to any target site
as a per-laboratory calibration method, for widescale deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Editors: Accepted for publication at MIDL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Resource Allocation in Wireless Networks: An AI-Enabled and
  Big Data-Driven Multi-Objective Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rawan Alkurd, Ibrahim Abualhaol, Halim Yanikomeroglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The design and optimization of wireless networks have mostly been based on
strong mathematical and theoretical modeling. Nonetheless, as novel
applications emerge in the era of 5G and beyond, unprecedented levels of
complexity will be encountered in the design and optimization of the network.
As a result, the use of Artificial Intelligence (AI) is envisioned for wireless
network design and optimization due to the flexibility and adaptability it
offers in solving extremely complex problems in real-time. One of the main
future applications of AI is enabling user-level personalization for numerous
use cases. AI will revolutionize the way we interact with computers in which
computers will be able to sense commands and emotions from humans in a
non-intrusive manner, making the entire process transparent to users. By
leveraging this capability, and accelerated by the advances in computing
technologies, wireless networks can be redesigned to enable the personalization
of network services to the user level in real-time. While current wireless
networks are being optimized to achieve a predefined set of quality
requirements, the personalization technology advocated in this article is
supported by an intelligent big data-driven layer designed to micro-manage the
scarce network resources. This layer provides the intelligence required to
decide the necessary service quality that achieves the target satisfaction
level for each user. Due to its dynamic and flexible design, personalized
networks are expected to achieve unprecedented improvements in optimizing two
contradicting objectives in wireless networks: saving resources and improving
user satisfaction levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence Rates for Localized Actor-Critic in Networked Markov
  Potential Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Zhou, Zaiwei Chen, Yiheng Lin, Adam Wierman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a class of networked Markov potential games in which agents are
associated with nodes in a network. Each agent has its own local potential
function, and the reward of each agent depends only on the states and actions
of the agents within a neighborhood. In this context, we propose a localized
actor-critic algorithm. The algorithm is scalable since each agent uses only
local information and does not need access to the global state. Further, the
algorithm overcomes the curse of dimensionality through the use of function
approximation. Our main results provide finite-sample guarantees up to a
localization error and a function approximation error. Specifically, we achieve
an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by
the averaged Nash regret. This is the first finite-sample bound for multi-agent
competitive games that does not depend on the number of agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A unified logical framework for explanations in classifier systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.14452v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.14452v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghan Liu, Emiliano Lorini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a renewed interest in Boolean function in
explaining binary classifiers in the field of explainable AI (XAI). The
standard approach of Boolean function is propositional logic. We present a
modal language of a ceteris paribus nature which supports reasoning about
binary input classifiers and their properties. We study a family of classifier
models, axiomatize it as two proof systems regarding the cardinality of the
language and show completeness of our axiomatics. Moreover, we prove that
satisfiability checking problem for our modal language is NEXPTIME-complete in
the infinite-variable case, while it becomes polynomial in the finite-variable
case. We furthermore identify an interesting NP fragment of our language in the
infinite-variable case. We leverage the language to formalize counterfactual
conditional as well as a variety of notions of explanation including abductive,
contrastive and counterfactual explanations, and biases. Finally, we present
two extensions of our language: a dynamic extension by the notion of assignment
enabling classifier change and an epistemic extension in which the classifier's
uncertainty about the actual input can be represented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Index Item IDs for Recommendation Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06569v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06569v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Shuyuan Xu, Yingqiang Ge, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text when deciding which item(s) to recommend,
creating LLM-compatible item IDs is essential for recommendation foundation
models. In this study, we systematically examine the item indexing problem for
recommendation foundation models, using P5 as the representative backbone model
and replicating its results with various indexing methods. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our reproducibility study of P5 highlights the significant
influence of item indexing methods on the model performance, and our results on
real-world datasets validate the effectiveness of our proposed solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Generative Diffusion Models for Structured
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejoon Koo, To Eun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, generative diffusion models have achieved a rapid paradigm
shift in deep generative models by showing groundbreaking performance across
various applications. Meanwhile, structured data, encompassing tabular and time
series data, has been received comparatively limited attention from the deep
learning research community, despite its omnipresence and extensive
applications. Thus, there is still a lack of literature and its reviews on
structured data modelling via diffusion models, compared to other data
modalities such as visual and textual data. To address this gap, we present a
comprehensive review of recently proposed diffusion models in the field of
structured data. First, this survey provides a concise overview of the
score-based diffusion model theory, subsequently proceeding to the technical
descriptions of the majority of pioneering works that used structured data in
both data-driven general tasks and domain-specific applications. Thereafter, we
analyse and discuss the limitations and challenges shown in existing works and
suggest potential research directions. We hope this review serves as a catalyst
for the research community, promoting developments in generative diffusion
models for structured data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Robust Saliency-based Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14106v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14106v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Chen, Chenghua Guo, Guixiang Ma, Ming Zeng, Xi Zhang, Sihong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust explanations of machine learning models are critical to establishing
human trust in the models. The top-$k$ intersection is widely used to evaluate
the robustness of explanations. However, most existing attacking and defense
strategies are based on $\ell_p$ norms, thus creating a mismatch between the
evaluation and optimization objectives. To this end, we define explanation
thickness for measuring top-$k$ salient features ranking stability, and design
the \textit{R2ET} algorithm based on a novel tractable surrogate to maximize
the thickness and stabilize the top salient features efficiently.
Theoretically, we prove a connection between R2ET and adversarial training;
using a novel multi-objective optimization formulation and a generalization
error bound, we further prove that the surrogate objective can improve both the
numerical and statistical stability of the explanations. Experiments with a
wide spectrum of network architectures and data modalities demonstrate that
R2ET attains higher explanation robustness under stealthy attacks while
retaining model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge
  Collaborative AutoML System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Xue, Wei Liu, Shuai Xie, Zhenfang Wang, Jiaxing Li, Xuyang Peng, Liang Ding, Shanshan Zhao, Qiong Cao, Yibo Yang, Fengxiang He, Bohua Cai, Rongcheng Bian, Yiyan Zhao, Heliang Zheng, Xiangyang Liu, Dongkai Liu, Daqing Liu, Li Shen, Chang Li, Shijin Zhang, Yukang Zhang, Guanpu Chen, Shixiang Chen, Yibing Zhan, Jing Zhang, Chaoyue Wang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated machine learning (AutoML) seeks to build ML models with minimal
human effort. While considerable research has been conducted in the area of
AutoML in general, aiming to take humans out of the loop when building
artificial intelligence (AI) applications, scant literature has focused on how
AutoML works well in open-environment scenarios such as the process of training
and updating large models, industrial supply chains or the industrial
metaverse, where people often face open-loop problems during the search
process: they must continuously collect data, update data and models, satisfy
the requirements of the development and deployment environment, support massive
devices, modify evaluation metrics, etc. Addressing the open-environment issue
with pure data-driven approaches requires considerable data, computing
resources, and effort from dedicated data engineers, making current AutoML
systems and platforms inefficient and computationally intractable.
Human-computer interaction is a practical and feasible way to tackle the
problem of open-environment AI. In this paper, we introduce OmniForce, a
human-centered AutoML (HAML) system that yields both human-assisted ML and
ML-assisted human techniques, to put an AutoML system into practice and build
adaptive AI in open-environment scenarios. Specifically, we present OmniForce
in terms of ML version management; pipeline-driven development and deployment
collaborations; a flexible search strategy framework; and widely provisioned
and crowdsourced application algorithms, including large models. Furthermore,
the (large) models constructed by OmniForce can be automatically turned into
remote services in a few minutes; this process is dubbed model as a service
(MaaS). Experimental results obtained in multiple search spaces and real-world
use cases demonstrate the efficacy and efficiency of OmniForce.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine learning for discovering laws of nature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhi Xin, Kevin Xin, Houwen Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on Darwin's natural selection, we developed "machine scientists" to
discover the laws of nature by learning from raw data. "Machine scientists"
construct physical theories by applying a logic tree (state Decision Tree) and
a value tree (observation Function Tree); the logical tree determines the state
of the entity, and the value tree determines the absolute value between the two
observations of the entity. A logic Tree and a value tree together can
reconstruct an entity's trajectory and make predictions about its future
outcomes. Our proposed algorithmic model has an emphasis on machine learning -
where "machine scientists" builds up its experience by being rewarded or
punished for each decision they make - eventually leading to rediscovering
Newton's equation (classical physics) and the Born's rule (quantum mechanics).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Interpretable Greedy-Tree Sums 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11931v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11931v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Shuo Tan, Chandan Singh, Keyan Nasseri, Abhineet Agarwal, James Duncan, Omer Ronen, Matthew Epland, Aaron Kornblith, Bin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning has achieved impressive prediction performance, but
often sacrifices interpretability, a critical consideration in high-stakes
domains such as medicine. In such settings, practitioners often use highly
interpretable decision tree models, but these suffer from inductive bias
against additive structure. To overcome this bias, we propose Fast
Interpretable Greedy-Tree Sums (FIGS), which generalizes the CART algorithm to
simultaneously grow a flexible number of trees in summation. By combining
logical rules with addition, FIGS is able to adapt to additive structure while
remaining highly interpretable. Extensive experiments on real-world datasets
show that FIGS achieves state-of-the-art prediction performance. To demonstrate
the usefulness of FIGS in high-stakes domains, we adapt FIGS to learn clinical
decision instruments (CDIs), which are tools for guiding clinical
decision-making. Specifically, we introduce a variant of FIGS known as G-FIGS
that accounts for the heterogeneity in medical data. G-FIGS derives CDIs that
reflect domain knowledge and enjoy improved specificity (by up to 20% over
CART) without sacrificing sensitivity or interpretability. To provide further
insight into FIGS, we prove that FIGS learns components of additive models, a
property we refer to as disentanglement. Further, we show (under oracle
conditions) that unconstrained tree-sum models leverage disentanglement to
generalize more efficiently than single decision tree models when fitted to
additive regression functions. Finally, to avoid overfitting with an
unconstrained number of splits, we develop Bagging-FIGS, an ensemble version of
FIGS that borrows the variance reduction techniques of random forests.
Bagging-FIGS enjoys competitive performance with random forests and XGBoost on
real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Multi-Objective A* with Partial Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valmiki Kothare, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Multi-Objective Shortest Path Problem (MO-SPP), typically posed on a
graph, determines a set of paths from a start vertex to a destination vertex
while optimizing multiple objectives. In general, there does not exist a single
solution path that can simultaneously optimize all the objectives and the
problem thus seeks to find a set of so-called Pareto-optimal solutions. To
address this problem, several Multi-Objective A* (MOA*) algorithms were
recently developed to quickly compute solutions with quality guarantees.
However, these MOA* algorithms often suffer from high memory usage, especially
when the branching factor (i.e. the number of neighbors of any vertex) of the
graph is large. This work thus aims at reducing the high memory consumption of
MOA* with little increase in the runtime. By generalizing and unifying several
single- and multi-objective search algorithms, we develop the Runtime and
Memory Efficient MOA* (RME-MOA*) approach, which can balance between runtime
and memory efficiency by tuning two user-defined hyper-parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks for temporal graphs: State of the art, open
  challenges, and opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01018v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01018v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Longa, Veronica Lachi, Gabriele Santin, Monica Bianchini, Bruno Lepri, Pietro Lio, Franco Scarselli, Andrea Passerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have become the leading paradigm for learning on
(static) graph-structured data. However, many real-world systems are dynamic in
nature, since the graph and node/edge attributes change over time. In recent
years, GNN-based models for temporal graphs have emerged as a promising area of
research to extend the capabilities of GNNs. In this work, we provide the first
comprehensive overview of the current state-of-the-art of temporal GNN,
introducing a rigorous formalization of learning settings and tasks and a novel
taxonomy categorizing existing approaches in terms of how the temporal aspect
is represented and processed. We conclude the survey with a discussion of the
most relevant open challenges for the field, from both research and application
perspectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BlackVIP: Black-Box Visual <span class="highlight-title">Prompt</span>ing for Robust Transfer Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, Kyungwoo Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the surge of large-scale pre-trained models (PTMs), fine-tuning these
models to numerous downstream tasks becomes a crucial problem. Consequently,
parameter efficient transfer learning (PETL) of large models has grasped huge
attention. While recent PETL methods showcase impressive performance, they rely
on optimistic assumptions: 1) the entire parameter set of a PTM is available,
and 2) a sufficiently large memory capacity for the fine-tuning is equipped.
However, in most real-world applications, PTMs are served as a black-box API or
proprietary software without explicit parameter accessibility. Besides, it is
hard to meet a large memory requirement for modern PTMs. In this work, we
propose black-box visual prompting (BlackVIP), which efficiently adapts the
PTMs without knowledge about model architectures and parameters. BlackVIP has
two components; 1) Coordinator and 2) simultaneous perturbation stochastic
approximation with gradient correction (SPSA-GC). The Coordinator designs
input-dependent image-shaped visual prompts, which improves few-shot adaptation
and robustness on distribution/location shift. SPSA-GC efficiently estimates
the gradient of a target model to update Coordinator. Extensive experiments on
16 datasets demonstrate that BlackVIP enables robust adaptation to diverse
domains without accessing PTMs' parameters, with minimal memory requirements.
Code: \url{https://github.com/changdaeoh/BlackVIP}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023 (v2: citation error was fixed)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Machine with Short-Term, Episodic, and Semantic Memory Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taewoon Kim, Michael Cochez, Vincent François-Lavet, Mark Neerincx, Piek Vossen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the cognitive science theory of the explicit human memory
systems, we have modeled an agent with short-term, episodic, and semantic
memory systems, each of which is modeled with a knowledge graph. To evaluate
this system and analyze the behavior of this agent, we designed and released
our own reinforcement learning agent environment, "the Room", where an agent
has to learn how to encode, store, and retrieve memories to maximize its return
by answering questions. We show that our deep Q-learning based agent
successfully learns whether a short-term memory should be forgotten, or rather
be stored in the episodic or semantic memory systems. Our experiments indicate
that an agent with human-like memory systems can outperform an agent without
this memory structure in the environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain
  Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06951v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06951v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihang Fu, Zesen Zhuang, Luyao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain has empowered computer systems to be more secure using a
distributed network. However, the current blockchain design suffers from
fairness issues in transaction ordering. Miners are able to reorder
transactions to generate profits, the so-called miner extractable value (MEV).
Existing research recognizes MEV as a severe security issue and proposes
potential solutions, including prominent Flashbots. However, previous studies
have mostly analyzed blockchain data, which might not capture the impacts of
MEV in a much broader AI society. Thus, in this research, we applied natural
language processing (NLP) methods to comprehensively analyze topics in tweets
on MEV. We collected more than 20000 tweets with #MEV and #Flashbots hashtags
and analyzed their topics. Our results show that the tweets discussed profound
topics of ethical concern, including security, equity, emotional sentiments,
and the desire for solutions to MEV. We also identify the co-movements of MEV
activities on blockchain and social media platforms. Our study contributes to
the literature at the interface of blockchain security, MEV solutions, and AI
ethics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cryptocurrency Valuation: An Explainable AI Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12893v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12893v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Liu, Luyao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, there are no convincing proxies for the fundamentals of
cryptocurrency assets. We propose a new market-to-fundamental ratio, the
price-to-utility (PU) ratio, utilizing unique blockchain accounting methods. We
then proxy various existing fundamental-to-market ratios by Bitcoin historical
data and find they have little predictive power for short-term bitcoin returns.
However, PU ratio effectively predicts long-term bitcoin returns than
alternative methods. Furthermore, we verify the explainability of PU ratio
using machine learning. Finally, we present an automated trading strategy
advised by the PU ratio that outperforms the conventional buy-and-hold and
market-timing strategies. Our research contributes to explainable AI in finance
from three facets: First, our market-to-fundamental ratio is based on classic
monetary theory and the unique UTXO model of Bitcoin accounting rather than ad
hoc; Second, the empirical evidence testifies the buy-low and sell-high
implications of the ratio; Finally, we distribute the trading algorithms as
open-source software via Python Package Index for future research, which is
exceptional in finance research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re-imagining health and well-being in low resource African settings
  using an augmented AI system and a 3D digital twin <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deshendran Moodley, Christopher Seebregts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses and explores the potential and relevance of recent
developments in artificial intelligence (AI) and digital twins for health and
well-being in low-resource African countries. We use the case of public health
emergency response to disease outbreaks and epidemic control. There is
potential to take advantage of the increasing availability of data and
digitization to develop advanced AI methods for analysis and prediction. Using
an AI systems perspective, we review emerging trends in AI systems and digital
twins and propose an initial augmented AI system architecture to illustrate how
an AI system can work with a 3D digital twin to address public health goals. We
highlight scientific knowledge discovery, continual learning, pragmatic
interoperability, and interactive explanation and decision-making as essential
research challenges for AI systems and digital twins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Workshop on AI for Digital Twins and Cyber-physical
  applications at IJCAI 2023, August 19--21, 2023, Macau, S.A.R</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conservative Distributional Reinforcement Learning with Safety
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengrui Zhang, Youfang Lin, Sheng Han, Shuo Wang, Kai Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety exploration can be regarded as a constrained Markov decision problem
where the expected long-term cost is constrained. Previous off-policy
algorithms convert the constrained optimization problem into the corresponding
unconstrained dual problem by introducing the Lagrangian relaxation technique.
However, the cost function of the above algorithms provides inaccurate
estimations and causes the instability of the Lagrange multiplier learning. In
this paper, we present a novel off-policy reinforcement learning algorithm
called Conservative Distributional Maximum a Posteriori Policy Optimization
(CDMPO). At first, to accurately judge whether the current situation satisfies
the constraints, CDMPO adapts distributional reinforcement learning method to
estimate the Q-function and C-function. Then, CDMPO uses a conservative value
function loss to reduce the number of violations of constraints during the
exploration process. In addition, we utilize Weighted Average Proportional
Integral Derivative (WAPID) to update the Lagrange multiplier stably. Empirical
results show that the proposed method has fewer violations of constraints in
the early exploration process. The final test results also illustrate that our
method has better risk control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantization-Aware and Tensor-Compressed Training of <span class="highlight-title">Transformer</span>s for
  Natural Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Yang, Samridhi Choudhary, Siegfried Kunzmann, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuned transformer models have shown superior performances in many
natural language tasks. However, the large model size prohibits deploying
high-performance transformer models on resource-constrained devices. This paper
proposes a quantization-aware tensor-compressed training approach to reduce the
model size, arithmetic operations, and ultimately runtime latency of
transformer-based models. We compress the embedding and linear layers of
transformers into small low-rank tensor cores, which significantly reduces
model parameters. A quantization-aware training with learnable scale factors is
used to further obtain low-precision representations of the tensor-compressed
models. The developed approach can be used for both end-to-end training and
distillation-based training. To improve the convergence, a layer-by-layer
distillation is applied to distill a quantized and tensor-compressed student
model from a pre-trained transformer. The performance is demonstrated in two
natural language understanding tasks, showing up to $63\times$ compression
ratio, little accuracy loss and remarkable inference and training speedup.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-07T00:00:00Z">2023-07-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Roman Numeral Analysis with Graph Neural Networks: Onset-wise
  Predictions from Note-wise Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil Karystinaios, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Roman Numeral analysis is the important task of identifying chords and their
functional context in pieces of tonal music. This paper presents a new approach
to automatic Roman Numeral analysis in symbolic music. While existing
techniques rely on an intermediate lossy representation of the score, we
propose a new method based on Graph Neural Networks (GNNs) that enable the
direct description and processing of each individual note in the score. The
proposed architecture can leverage notewise features and interdependencies
between notes but yield onset-wise representation by virtue of our novel edge
contraction algorithm. Our results demonstrate that ChordGNN outperforms
existing state-of-the-art models, achieving higher accuracy in Roman Numeral
analysis on the reference datasets. In addition, we investigate variants of our
model using proposed techniques such as NADE, and post-processing of the chord
predictions. The full source code for this work is available at
https://github.com/manoskary/chordgnn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for ISMIR 2023 conference, 6 pages of content, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The CHiME-7 UDASE task: Unsupervised domain adaptation for
  conversational speech enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Leglaive, Léonie Borne, Efthymios Tzinis, Mostafa Sadeghi, Matthieu Fraticelli, Scott Wisdom, Manuel Pariente, Daniel Pressnitzer, John R. Hershey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised speech enhancement models are trained using artificially generated
mixtures of clean speech and noise signals, which may not match real-world
recording conditions at test time. This mismatch can lead to poor performance
if the test domain significantly differs from the synthetic training domain. In
this paper, we introduce the unsupervised domain adaptation for conversational
speech enhancement (UDASE) task of the 7th CHiME challenge. This task aims to
leverage real-world noisy speech recordings from the target test domain for
unsupervised domain adaptation of speech enhancement models. The target test
domain corresponds to the multi-speaker reverberant conversational speech
recordings of the CHiME-5 dataset, for which the ground-truth clean speech
reference is not available. Given a CHiME-5 recording, the task is to estimate
the clean, potentially multi-speaker, reverberant speech, removing the additive
background noise. We discuss the motivation for the CHiME-7 UDASE task and
describe the data, the task, and the baseline system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-Level Serialized Output Training for Joint Streaming ASR and ST
  Leveraging Textual Alignments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Papi, Peidong Wan, Junkun Chen, Jian Xue, Jinyu Li, Yashesh Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, users often require both translations and
transcriptions of speech to enhance their comprehension, particularly in
streaming scenarios where incremental generation is necessary. This paper
introduces a streaming Transformer-Transducer that jointly generates automatic
speech recognition (ASR) and speech translation (ST) outputs using a single
decoder. To produce ASR and ST content effectively with minimal latency, we
propose a joint token-level serialized output training method that interleaves
source and target words by leveraging an off-the-shelf textual aligner.
Experiments in monolingual (it-en) and multilingual (\{de,es,it\}-en) settings
demonstrate that our approach achieves the best quality-latency balance. With
an average ASR latency of 1s and ST latency of 1.3s, our model shows no
degradation or even improves output quality compared to separate ASR and ST
models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the
multilingual case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Launchpad<span class="highlight-title">GPT</span>: Language Model as Music Visualization Designer on
  Launchpad 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siting Xu, Yunlong Tang, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Launchpad is a musical instrument that allows users to create and perform
music by pressing illuminated buttons. To assist and inspire the design of the
Launchpad light effect, and provide a more accessible approach for beginners to
create music visualization with this instrument, we proposed the LaunchpadGPT
model to generate music visualization designs on Launchpad automatically. Based
on the language model with excellent generation ability, our proposed
LaunchpadGPT takes an audio piece of music as input and outputs the lighting
effects of Launchpad-playing in the form of a video (Launchpad-playing video).
We collect Launchpad-playing videos and process them to obtain music and
corresponding video frame of Launchpad-playing as prompt-completion pairs, to
train the language model. The experiment result shows the proposed method can
create better music visualization than random generation methods and hold the
potential for a broader range of music visualization applications. Our code is
available at https://github.com/yunlong10/LaunchpadGPT/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Computer Music Conference (ICMC) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Ethical Implications of Generative Audio Models: A Systematic
  Literature <span class="highlight-title">Review</span> <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Barnett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative audio models typically focus their applications in music and
speech generation, with recent models having human-like quality in their audio
output. This paper conducts a systematic literature review of 884 papers in the
area of generative audio models in order to both quantify the degree to which
researchers in the field are considering potential negative impacts and
identify the types of ethical implications researchers in this area need to
consider. Though 65% of generative audio research papers note positive
potential impacts of their work, less than 10% discuss any negative impacts.
This jarringly small percentage of papers considering negative impact is
particularly worrying because the issues brought to light by the few papers
doing so are raising serious ethical implications and concerns relevant to the
broader field such as the potential for fraud, deep-fakes, and copyright
infringement. By quantifying this lack of ethical consideration in generative
audio research and identifying key areas of potential harm, this paper lays the
groundwork for future work in the field at a critical point in time in order to
guide more conscientious research as this field progresses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the AAAI/ACM Conference on AI, Ethics, and Society
  (AIES '23). 10 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by
  Whispering to Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen, Wei Xue, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic
lyrics transcription method achieving state-of-the-art performance on various
lyrics transcription datasets, even in challenging genres such as rock and
metal. Our novel, training-free approach utilizes Whisper, a weakly supervised
robust speech recognition model, and GPT-4, today's most performant chat-based
large language model. In the proposed method, Whisper functions as the "ear" by
transcribing the audio, while GPT-4 serves as the "brain," acting as an
annotator with a strong performance for contextualized output selection and
correction. Our experiments show that LyricWhiz significantly reduces Word
Error Rate compared to existing methods in English and can effectively
transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to
create the first publicly available, large-scale, multilingual lyrics
transcription dataset with a CC-BY-NC-SA copyright license, based on
MTG-Jamendo, and offer a human-annotated subset for noise level estimation and
evaluation. We anticipate that our proposed method and dataset will advance the
development of multilingual lyrics transcription, a challenging and emerging
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 5 tables, accepted by ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Binary classification of spoken words with passive phononic
  metamaterials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tena Dubček, Daniel Moreno-Garcia, Thomas Haag, Parisa Omidvar, Henrik R. Thomsen, Theodor S. Becker, Lars Gebraad, Christoph Bärlocher, Fredrik Andersson, Sebastian D. Huber, Dirk-Jan van Manen, Luis Guillermo Villanueva, Johan O. A. Robertsson, Marc Serra-Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the energy requirements of artificial intelligence requires novel
physical substrates for computation. Phononic metamaterials have a vanishingly
low power dissipation and hence are a prime candidate for green, always-on
computers. However, their use in machine learning applications has not been
explored due to the complexity of their design process: Current phononic
metamaterials are restricted to simple geometries (e.g. periodic, tapered), and
hence do not possess sufficient expressivity to encode machine learning tasks.
We design and fabricate a non-periodic phononic metamaterial, directly from
data samples, that can distinguish between pairs of spoken words in the
presence of a simple readout nonlinearity; hence demonstrating that phononic
metamaterials are a viable avenue towards zero-power smart devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Phonetic-assisted Multi-Target Units Modeling for Improving
  Conformer-Transducer ASR system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Li, Dongxing Xu, Haoran Wei, Yanhua Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploiting effective target modeling units is very important and has always
been a concern in end-to-end automatic speech recognition (ASR). In this work,
we propose a phonetic-assisted multi target units (PMU) modeling approach, to
enhance the Conformer-Transducer ASR system in a progressive representation
learning manner. Specifically, PMU first uses the pronunciation-assisted
subword modeling (PASM) and byte pair encoding (BPE) to produce
phonetic-induced and text-induced target units separately; Then, three new
frameworks are investigated to enhance the acoustic encoder, including a basic
PMU, a paraCTC and a pcaCTC, they integrate the PASM and BPE units at different
levels for CTC and transducer multi-task training. Experiments on both
LibriSpeech and accented ASR tasks show that, the proposed PMU significantly
outperforms the conventional BPE, it reduces the WER of LibriSpeech clean,
other, and six accented ASR testsets by relative 12.7%, 6.0% and 7.7%,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for
  Music Information Retrieval <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Wei, Jun Yuan, Rui Zhang, Yueguo Chen, Gang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Melody extraction is a core task in music information retrieval, and the
estimation of pitch, onset and offset are key sub-tasks in melody extraction.
Existing methods have limited accuracy, and work for only one type of data,
either single-pitch or multipitch. In this paper, we propose a highly accurate
method for joint estimation of pitch, onset and offset, named JEPOO. We address
the challenges of joint learning optimization and handling both single-pitch
and multi-pitch data through novel model design and a new optimization
technique named Pareto modulated loss with loss weight regularization. This is
the first method that can accurately handle both single-pitch and multi-pitch
music data, and even a mix of them. A comprehensive experimental study on a
wide range of real datasets shows that JEPOO outperforms state-ofthe-art
methods by up to 10.6%, 8.3% and 10.3% for the prediction of Pitch, Onset and
Offset, respectively, and JEPOO is robust for various types of data and
instruments. The ablation study shows the effectiveness of each component of
JEPOO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IJCAI 2023; 11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Relationship Between Speech Features Changes When You Get Depressed:
  Feature Correlations for Improving Speed and Performance of Depression
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuxiang Tao, Wei Ma, Xuri Ge, Anna Esposito, Alessandro Vinciarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work shows that depression changes the correlation between features
extracted from speech. Furthermore, it shows that using such an insight can
improve the training speed and performance of depression detectors based on
SVMs and LSTMs. The experiments were performed over the Androids Corpus, a
publicly available dataset involving 112 speakers, including 58 people
diagnosed with depression by professional psychiatrists. The results show that
the models used in the experiments improve in terms of training speed and
performance when fed with feature correlation matrices rather than with feature
vectors. The relative reduction of the error rate ranges between 23.1% and
26.6% depending on the model. The probable explanation is that feature
correlation matrices appear to be more variable in the case of depressed
speakers. Correspondingly, such a phenomenon can be thought of as a depression
marker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WACO: Word-Aligned Contrastive Learning for Speech Translation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Ouyang, Rong Ye, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end Speech Translation (E2E ST) aims to directly translate source
speech into target text. Existing ST methods perform poorly when only extremely
small speech-text data are available for training. We observe that an ST
model's performance closely correlates with its embedding similarity between
speech and source transcript. In this paper, we propose Word-Aligned
COntrastive learning (WACO), a simple and effective method for extremely
low-resource speech-to-text translation. Our key idea is bridging word-level
representations for both speech and text modalities via contrastive learning.
We evaluate WACO and other methods on the MuST-C dataset, a widely used ST
benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our
experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU
points with only 1-hour parallel ST data. Code is available at
https://github.com/owaski/WACO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defend Data Poisoning Attacks on Voice Authentication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Li, Cameron Baird, Dan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advances in deep learning, speaker verification has achieved very
high accuracy and is gaining popularity as a type of biometric authentication
option in many scenes of our daily life, especially the growing market of web
services. Compared to traditional passwords, "vocal passwords" are much more
convenient as they relieve people from memorizing different passwords. However,
new machine learning attacks are putting these voice authentication systems at
risk. Without a strong security guarantee, attackers could access legitimate
users' web accounts by fooling the deep neural network (DNN) based voice
recognition models. In this paper, we demonstrate an easy-to-implement data
poisoning attack to the voice authentication system, which can hardly be
captured by existing defense mechanisms. Thus, we propose a more robust defense
method, called Guardian, which is a convolutional neural network-based
discriminator. The Guardian discriminator integrates a series of novel
techniques including bias reduction, input augmentation, and ensemble learning.
Our approach is able to distinguish about 95% of attacked accounts from normal
accounts, which is much more effective than existing approaches with only 60%
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The CHiME-7 UDASE task: Unsupervised domain adaptation for
  conversational speech enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Leglaive, Léonie Borne, Efthymios Tzinis, Mostafa Sadeghi, Matthieu Fraticelli, Scott Wisdom, Manuel Pariente, Daniel Pressnitzer, John R. Hershey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised speech enhancement models are trained using artificially generated
mixtures of clean speech and noise signals, which may not match real-world
recording conditions at test time. This mismatch can lead to poor performance
if the test domain significantly differs from the synthetic training domain. In
this paper, we introduce the unsupervised domain adaptation for conversational
speech enhancement (UDASE) task of the 7th CHiME challenge. This task aims to
leverage real-world noisy speech recordings from the target test domain for
unsupervised domain adaptation of speech enhancement models. The target test
domain corresponds to the multi-speaker reverberant conversational speech
recordings of the CHiME-5 dataset, for which the ground-truth clean speech
reference is not available. Given a CHiME-5 recording, the task is to estimate
the clean, potentially multi-speaker, reverberant speech, removing the additive
background noise. We discuss the motivation for the CHiME-7 UDASE task and
describe the data, the task, and the baseline system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-Level Serialized Output Training for Joint Streaming ASR and ST
  Leveraging Textual Alignments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Papi, Peidong Wan, Junkun Chen, Jian Xue, Jinyu Li, Yashesh Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, users often require both translations and
transcriptions of speech to enhance their comprehension, particularly in
streaming scenarios where incremental generation is necessary. This paper
introduces a streaming Transformer-Transducer that jointly generates automatic
speech recognition (ASR) and speech translation (ST) outputs using a single
decoder. To produce ASR and ST content effectively with minimal latency, we
propose a joint token-level serialized output training method that interleaves
source and target words by leveraging an off-the-shelf textual aligner.
Experiments in monolingual (it-en) and multilingual (\{de,es,it\}-en) settings
demonstrate that our approach achieves the best quality-latency balance. With
an average ASR latency of 1s and ST latency of 1.3s, our model shows no
degradation or even improves output quality compared to separate ASR and ST
models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the
multilingual case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Launchpad<span class="highlight-title">GPT</span>: Language Model as Music Visualization Designer on
  Launchpad 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siting Xu, Yunlong Tang, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Launchpad is a musical instrument that allows users to create and perform
music by pressing illuminated buttons. To assist and inspire the design of the
Launchpad light effect, and provide a more accessible approach for beginners to
create music visualization with this instrument, we proposed the LaunchpadGPT
model to generate music visualization designs on Launchpad automatically. Based
on the language model with excellent generation ability, our proposed
LaunchpadGPT takes an audio piece of music as input and outputs the lighting
effects of Launchpad-playing in the form of a video (Launchpad-playing video).
We collect Launchpad-playing videos and process them to obtain music and
corresponding video frame of Launchpad-playing as prompt-completion pairs, to
train the language model. The experiment result shows the proposed method can
create better music visualization than random generation methods and hold the
potential for a broader range of music visualization applications. Our code is
available at https://github.com/yunlong10/LaunchpadGPT/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Computer Music Conference (ICMC) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Ethical Implications of Generative Audio Models: A Systematic
  Literature <span class="highlight-title">Review</span> <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Barnett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative audio models typically focus their applications in music and
speech generation, with recent models having human-like quality in their audio
output. This paper conducts a systematic literature review of 884 papers in the
area of generative audio models in order to both quantify the degree to which
researchers in the field are considering potential negative impacts and
identify the types of ethical implications researchers in this area need to
consider. Though 65% of generative audio research papers note positive
potential impacts of their work, less than 10% discuss any negative impacts.
This jarringly small percentage of papers considering negative impact is
particularly worrying because the issues brought to light by the few papers
doing so are raising serious ethical implications and concerns relevant to the
broader field such as the potential for fraud, deep-fakes, and copyright
infringement. By quantifying this lack of ethical consideration in generative
audio research and identifying key areas of potential harm, this paper lays the
groundwork for future work in the field at a critical point in time in order to
guide more conscientious research as this field progresses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the AAAI/ACM Conference on AI, Ethics, and Society
  (AIES '23). 10 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by
  Whispering to Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen, Wei Xue, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic
lyrics transcription method achieving state-of-the-art performance on various
lyrics transcription datasets, even in challenging genres such as rock and
metal. Our novel, training-free approach utilizes Whisper, a weakly supervised
robust speech recognition model, and GPT-4, today's most performant chat-based
large language model. In the proposed method, Whisper functions as the "ear" by
transcribing the audio, while GPT-4 serves as the "brain," acting as an
annotator with a strong performance for contextualized output selection and
correction. Our experiments show that LyricWhiz significantly reduces Word
Error Rate compared to existing methods in English and can effectively
transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to
create the first publicly available, large-scale, multilingual lyrics
transcription dataset with a CC-BY-NC-SA copyright license, based on
MTG-Jamendo, and offer a human-annotated subset for noise level estimation and
evaluation. We anticipate that our proposed method and dataset will advance the
development of multilingual lyrics transcription, a challenging and emerging
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 5 tables, accepted by ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Binary classification of spoken words with passive phononic
  metamaterials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tena Dubček, Daniel Moreno-Garcia, Thomas Haag, Parisa Omidvar, Henrik R. Thomsen, Theodor S. Becker, Lars Gebraad, Christoph Bärlocher, Fredrik Andersson, Sebastian D. Huber, Dirk-Jan van Manen, Luis Guillermo Villanueva, Johan O. A. Robertsson, Marc Serra-Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the energy requirements of artificial intelligence requires novel
physical substrates for computation. Phononic metamaterials have a vanishingly
low power dissipation and hence are a prime candidate for green, always-on
computers. However, their use in machine learning applications has not been
explored due to the complexity of their design process: Current phononic
metamaterials are restricted to simple geometries (e.g. periodic, tapered), and
hence do not possess sufficient expressivity to encode machine learning tasks.
We design and fabricate a non-periodic phononic metamaterial, directly from
data samples, that can distinguish between pairs of spoken words in the
presence of a simple readout nonlinearity; hence demonstrating that phononic
metamaterials are a viable avenue towards zero-power smart devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Phonetic-assisted Multi-Target Units Modeling for Improving
  Conformer-Transducer ASR system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Li, Dongxing Xu, Haoran Wei, Yanhua Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploiting effective target modeling units is very important and has always
been a concern in end-to-end automatic speech recognition (ASR). In this work,
we propose a phonetic-assisted multi target units (PMU) modeling approach, to
enhance the Conformer-Transducer ASR system in a progressive representation
learning manner. Specifically, PMU first uses the pronunciation-assisted
subword modeling (PASM) and byte pair encoding (BPE) to produce
phonetic-induced and text-induced target units separately; Then, three new
frameworks are investigated to enhance the acoustic encoder, including a basic
PMU, a paraCTC and a pcaCTC, they integrate the PASM and BPE units at different
levels for CTC and transducer multi-task training. Experiments on both
LibriSpeech and accented ASR tasks show that, the proposed PMU significantly
outperforms the conventional BPE, it reduces the WER of LibriSpeech clean,
other, and six accented ASR testsets by relative 12.7%, 6.0% and 7.7%,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for
  Music Information Retrieval <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Wei, Jun Yuan, Rui Zhang, Yueguo Chen, Gang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Melody extraction is a core task in music information retrieval, and the
estimation of pitch, onset and offset are key sub-tasks in melody extraction.
Existing methods have limited accuracy, and work for only one type of data,
either single-pitch or multipitch. In this paper, we propose a highly accurate
method for joint estimation of pitch, onset and offset, named JEPOO. We address
the challenges of joint learning optimization and handling both single-pitch
and multi-pitch data through novel model design and a new optimization
technique named Pareto modulated loss with loss weight regularization. This is
the first method that can accurately handle both single-pitch and multi-pitch
music data, and even a mix of them. A comprehensive experimental study on a
wide range of real datasets shows that JEPOO outperforms state-ofthe-art
methods by up to 10.6%, 8.3% and 10.3% for the prediction of Pitch, Onset and
Offset, respectively, and JEPOO is robust for various types of data and
instruments. The ablation study shows the effectiveness of each component of
JEPOO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IJCAI 2023; 11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Relationship Between Speech Features Changes When You Get Depressed:
  Feature Correlations for Improving Speed and Performance of Depression
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuxiang Tao, Wei Ma, Xuri Ge, Anna Esposito, Alessandro Vinciarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work shows that depression changes the correlation between features
extracted from speech. Furthermore, it shows that using such an insight can
improve the training speed and performance of depression detectors based on
SVMs and LSTMs. The experiments were performed over the Androids Corpus, a
publicly available dataset involving 112 speakers, including 58 people
diagnosed with depression by professional psychiatrists. The results show that
the models used in the experiments improve in terms of training speed and
performance when fed with feature correlation matrices rather than with feature
vectors. The relative reduction of the error rate ranges between 23.1% and
26.6% depending on the model. The probable explanation is that feature
correlation matrices appear to be more variable in the case of depressed
speakers. Correspondingly, such a phenomenon can be thought of as a depression
marker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WACO: Word-Aligned Contrastive Learning for Speech Translation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Ouyang, Rong Ye, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end Speech Translation (E2E ST) aims to directly translate source
speech into target text. Existing ST methods perform poorly when only extremely
small speech-text data are available for training. We observe that an ST
model's performance closely correlates with its embedding similarity between
speech and source transcript. In this paper, we propose Word-Aligned
COntrastive learning (WACO), a simple and effective method for extremely
low-resource speech-to-text translation. Our key idea is bridging word-level
representations for both speech and text modalities via contrastive learning.
We evaluate WACO and other methods on the MuST-C dataset, a widely used ST
benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our
experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU
points with only 1-hour parallel ST data. Code is available at
https://github.com/owaski/WACO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building High-accuracy Multilingual ASR with Gated Language Experts and
  Curriculum Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Sun, Jinyu Li, Yuxuan Hu, Yimeng Zhu, Long Zhou, Jian Xue, Peidong Wang, Linquan Liu, Shujie Liu, Edward Lin, Yifan Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose gated language experts and curriculum training to enhance
multilingual transformer transducer models without requiring language
identification (LID) input from users during inference. Our method incorporates
a gating mechanism and LID loss, enabling transformer experts to learn
language-specific information. By combining gated transformer experts with
shared transformer layers, we construct multilingual transformer blocks and
utilize linear experts to effectively regularize the joint network. The
curriculum training scheme leverages LID to guide the gated experts in
improving their respective language performance. Experimental results on a
bilingual task involving English and Spanish demonstrate significant
improvements, with average relative word error reductions of 12.5% and 7.3%
compared to the baseline bilingual and monolingual models, respectively.
Notably, our method achieves performance comparable to the upper-bound model
trained and inferred with oracle LID. Extending our approach to trilingual,
quadrilingual, and pentalingual models reveals similar advantages to those
observed in the bilingual models, highlighting its ease of extension to
multiple languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defend Data Poisoning Attacks on Voice Authentication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Li, Cameron Baird, Dan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advances in deep learning, speaker verification has achieved very
high accuracy and is gaining popularity as a type of biometric authentication
option in many scenes of our daily life, especially the growing market of web
services. Compared to traditional passwords, "vocal passwords" are much more
convenient as they relieve people from memorizing different passwords. However,
new machine learning attacks are putting these voice authentication systems at
risk. Without a strong security guarantee, attackers could access legitimate
users' web accounts by fooling the deep neural network (DNN) based voice
recognition models. In this paper, we demonstrate an easy-to-implement data
poisoning attack to the voice authentication system, which can hardly be
captured by existing defense mechanisms. Thus, we propose a more robust defense
method, called Guardian, which is a convolutional neural network-based
discriminator. The Guardian discriminator integrates a series of novel
techniques including bias reduction, input augmentation, and ensemble learning.
Our approach is able to distinguish about 95% of attacked accounts from normal
accounts, which is much more effective than existing approaches with only 60%
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LTE SFBC MIMO Transmitter Modelling and Performance Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriela Morillo, John Cosmas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High data rates are one of the most prevalent requirements in current mobile
communications. To cover this and other high standards regarding performance,
increasing coverage, capacity, and reliability, numerous works have proposed
the development of systems employing the combination of several techniques such
as Multiple Input Multiple Output (MIMO) wireless technologies with Orthogonal
Frequency Division Multiplexing (OFDM) in the evolving 4G wireless
communications. Our proposed system is based on the 2x2 MIMO antenna technique,
which is defined to enhance the performance of radio communication systems in
terms of capacity and spectral efficiency, and the OFDM technique, which can be
implemented using two types of sub-carrier mapping modes: Space-Time Block
Coding and Space Frequency Block Code. SFBC has been considered in our
developed model. The main advantage of SFBC over STBC is that SFBC encodes two
modulated symbols over two subcarriers of the same OFDM symbol, whereas STBC
encodes two modulated symbols over two subcarriers of the same OFDM symbol;
thus, the coding is performed in the frequency domain. Our solution aims to
demonstrate the performance analysis of the Space Frequency Block Codes scheme,
increasing the Signal Noise Ratio (SNR) at the receiver and decreasing the Bit
Error Rate (BER) through the use of 4 QAM, 16 QAM and 64QAM modulation over a
2x2 MIMO channel for an LTE downlink transmission, in different channel radio
environments. In this work, an analytical tool to evaluate the performance of
SFBC - Orthogonal Frequency Division Multiplexing, using two transmit antennas
and two receive antennas has been implemented, and the analysis using the
average SNR has been considered as a sufficient statistic to describe the
performance of SFBC in the 3GPP Long Term Evolution system over Multiple Input
Multiple Output channels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 20 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact recovery of the support of piecewise constant images via total
  variation regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohann De Castro, Vincent Duval, Romain Petit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work is concerned with the recovery of piecewise constant images from
noisy linear measurements. We study the noise robustness of a variational
reconstruction method, which is based on total (gradient) variation
regularization. We show that, if the unknown image is the superposition of a
few simple shapes, and if a non-degenerate source condition holds, then, in the
low noise regime, the reconstructed images have the same structure: they are
the superposition of the same number of shapes, each a smooth deformation of
one of the unknown shapes. Moreover, the reconstructed shapes and the
associated intensities converge to the unknown ones as the noise goes to zero.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using electrical impedance spectroscopy to identify equivalent circuit
  models of lubricated contacts with complex geometry: in-situ application to
  mini traction machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Yu, Jie Zhang, Arndt Joedicke, Tom Reddyhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrical contact resistance or capacitance as measured between a lubricated
contact has been used in tribometers, partially reflecting the lubrication
condition. In contrast, the electrical impedance provides rich information of
magnitude and phase, which can be interpreted using equivalent circuit models,
enabling more comprehensive measurements, including the variation of lubricant
film thickness and the asperity (metal to metal) contact area. An accurate
circuit model of the lubricated contact is critical as needed for the
electrical impedance analysis. However, existing circuit models are hand
derived and suited to interfaces with simple geometry, such as parallel plates,
concentric and eccentric cylinders. Circuit model identification of lubricated
contacts with complex geometry is challenging. This work takes the ball-on-disc
lubricated contact in a Mini Traction Machine (MTM) as an example, where screws
on the ball, grooves on the disc, and contact close to the disc edge make the
overall interface geometry complicated. The electrical impedance spectroscopy
(EIS) is used to capture its frequency response, with a group of load, speed,
and temperature varied and tested separately. The results enable an
identification of equivalent circuit models by fitting parallel
resistor-capacitor models, the dependence on the oil film thickness is further
calibrated using a high-accuracy optical interferometry, which is operated
under the same lubrication condition as in the MTM. Overall, the proposed
method is applicable to general lubricated interfaces for the identification of
equivalent circuit models, which in turn facilitates in-situ tribo-contacts
with electric impedance measurement of oil film thickness. It does not need
transparent materials as optical techniques do, or structural modifications for
piezoelectric sensor mounting as ultrasound techniques do.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Anti-Jamming Strategy for Disco Intelligent Reflecting Surfaces Based
  Fully-Passive Jamming Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Huang, Hongliang Zhang, Yi Cai, A. Lee Swindlehurst, Zhu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging intelligent reflecting surfaces (IRSs) significantly improve system
performance, while also pose a huge risk for physical layer security. A disco
IRS (DIRS), i.e., an illegitimate IRS with random time-varying reflection
properties, can be employed by an attacker to actively age the channels of
legitimate users (LUs). Such active channel aging (ACA) generated by the
DIRS-based fully-passive jammer (FPJ) can be applied to jam multi-user
multiple-input single-output (MU-MISO) systems without relying on either
jamming power or LU channel state information (CSI). To address the significant
threats posed by the DIRS-based FPJ, an anti-jamming strategy is proposed that
requires only the statistical characteristics of DIRS-jammed channels instead
of their CSI. Statistical characteristics of DIRS-jammed channels are first
derived, and then the anti-jamming precoder is given based on the derived
statistical characteristics. Numerical results are also presented to evaluate
the effectiveness of the proposed anti-jamming precoder against the DIRS-based
FPJ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Antenna Impedance Estimation in Correlated Rayleigh Fading Channels <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaohan Wu, Brian Hughes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formulate antenna impedance estimation in a classical estimation framework
under correlated Raleigh fading channels. Based on training sequences of
multiple packets, we derive the ML estimators for antenna impedance and channel
variance, treating the fading path gains as nuisance parameters. These ML
estimators can be found via scalar optimization. We explore the efficiency of
these estimators against Cramer-Rao lower bounds by numerical examples. The
impact of channel correlation on impedance estimation accuracy is investigated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, ICASSP 2023. arXiv admin note: substantial text
  overlap with arXiv:2006.11443</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContextLabeler <span class="highlight-title">Dataset</span>: physical and virtual sensors data collected from
  smartphone usage in-the-wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Giovanni Campana, Franca Delmastro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a data collection campaign and the resulting dataset
derived from smartphone sensors characterizing the daily life activities of 3
volunteers in a period of two weeks. The dataset is released as a collection of
CSV files containing more than 45K data samples, where each sample is composed
by 1332 features related to a heterogeneous set of physical and virtual
sensors, including motion sensors, running applications, devices in proximity,
and weather conditions. Moreover, each data sample is associated with a ground
truth label that describes the user activity and the situation in which she was
involved during the sensing experiment (e.g., working, at restaurant, and doing
sport activity). To avoid introducing any bias during the data collection, we
performed the sensing experiment in-the-wild, that is, by using the volunteers'
devices, and without defining any constraint related to the user's behavior.
For this reason, the collected dataset represents a useful source of real data
to both define and evaluate a broad set of novel context-aware solutions (both
algorithms and protocols) that aim to adapt their behavior according to the
changes in the user's situation in a mobile environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate Maximum a Posteriori Carrier Phase Estimator for Wiener
  Phase Noise Channels using Belief Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Chimmalgi, Andrej Rode, Luca Schmid, Laurent Schmalen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The blind phase search (BPS) algorithm for carrier phase estimation is known
to have sub-optimal performance for probabilistically shaped constellations. We
present a belief propagation based approximate maximum a posteriori carrier
phase estimator and compare its performance with the standard and an improved
BPS algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at European Conference on Optical
  Communications 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RNN Based Channel Estimation in Doubly Selective Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Karim Gizzini, Marwa Chafii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Doubly-selective channel estimation represents a key element in ensuring
communication reliability in wireless systems. Due to the impact of multi-path
propagation and Doppler interference in dynamic environments, doubly-selective
channel estimation becomes challenging. Conventional symbol-by-symbol (SBS) and
frame-by-frame (FBF) channel estimation schemes encounter performance
degradation in high mobility scenarios due to the usage of limited training
pilots. Recently, deep learning (DL) has been utilized for doubly-selective
channel estimation, where long short-term memory (LSTM) and convolutional
neural network (CNN) networks are employed in the SBS and FBF, respectively.
However, their usage is not optimal, since LSTM suffers from long-term memory
problem, whereas, CNN-based estimators require high complexity. For this
purpose, we overcome these issues by proposing an optimized recurrent neural
network (RNN)-based channel estimation schemes, where gated recurrent unit
(GRU) and Bi-GRU units are used in SBS and FBF channel estimation,
respectively. The proposed estimators are based on the average correlation of
the channel in different mobility scenarios, where several
performance-complexity trade-offs are provided. Moreover, the performance of
several RNN networks is analyzed. The performance superiority of the proposed
estimators against the recently proposed DL-based SBS and FBF estimators is
demonstrated for different scenarios while recording a significant reduction in
complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to the IEEE Transactions on Machine
  Learning in Communications and Networking (TMLCN). arXiv admin note: text
  overlap with arXiv:2305.00208</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Compound Gaussian Model for Bivariate Surface EMG Signals
  Related to Strength Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Durgesh Kusuru, Anish C. Turlapaty, Mainak Thakur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent literature suggests that the surface electromyography (sEMG) signals
have non-stationary statistical characteristics specifically due to random
nature of the covariance. Thus suitability of a statistical model for sEMG
signals is determined by the choice of an appropriate model for describing the
covariance. The purpose of this study is to propose a Compound-Gaussian (CG)
model for multivariate sEMG signals in which latent variable of covariance is
modeled as a random variable that follows an exponential model. The parameters
of the model are estimated using the iterative Expectation Maximization (EM)
algorithm. Further, a new dataset, electromyography analysis of human
activities database 2 (EMAHA-DB2) is developed. Based on the model fitting
analysis on the sEMG signals from EMAHA-DB2, it is found that the proposed CG
model fits more closely to the empirical pdf of sEMG signals than the existing
models. The proposed model is validated by visual inspection, further validated
by matching central moments and better quantitative metrics in comparison with
other models. The proposed compound model provides an improved fit to the
statistical behavior of sEMG signals. Further, the estimate of rate parameter
of the exponential model shows clear relation to the training weights. Finally,
the average signal power estimates of the channels shows distinctive dependency
on the training weights, the subject's training experience and the type of
activity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article supersedes arXiv:2301.05417. This work has been
  submitted to the IEEE for possible publication. Copyright may be transferred
  without notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Joint Design for Full-duplex OFDM AF Relay System with Precoded Short
  Guard Interval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pu Yang, Xiang-Gen Xia, Qingyue Qu, Han Wang, Yi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-band full-duplex relay (FDR) has attracted much attention as an effective
solution to improve the coverage and spectral efficiency in wireless
communication networks. The basic problem for FDR transmission is how to
eliminate the inherent self-interference and re-use the residual
self-interference (RSI) at the relay to improve the end-to-end performance.
Considering the RSI at the FDR, the overall equivalent channel can be modeled
as an infinite impulse response (IIR) channel. For this IIR channel, a joint
design for precoding, power gain control and equalization of cooperative OFDM
relay systems is presented. Compared with the traditional OFDM systems, the
length of the guard interval for the proposed design can be distinctly reduced,
thereby improving the spectral efficiency. By analyzing the noise sources, this
paper evaluates the signal to noise ratio (SNR) of the proposed scheme and
presents a power gain control algorithm at the FDR. Compared with the existing
schemes, the proposed scheme shows a superior bit error rate (BER) performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Waveform-Domain Adaptive Matched Filtering: A Novel Approach to
  Suppressing Interrupted-Sampling Repeater Jamming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanning Su, Qinglong Bao, Jiameng Pan, Fucheng Guo, Weidong Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inadequate adaptability to flexible interference scenarios remains an
unresolved challenge in the majority of techniques utilized for mitigating
interrupted-sampling repeater jamming (ISRJ). Matched filtering system based
methods is desirable to incorporate anti-ISRJ measures based on prior ISRJ
modeling, either preceding or succeeding the matched filtering. Due to the
partial matching nature of ISRJ, its characteristics are revealed during the
process of matched filtering. Therefore, this paper introduces an extended
domain called the waveform domain within the matched filtering process. On this
domain, a novel matched filtering model, known as the waveform-domain adaptive
matched filtering (WD-AMF), is established to tackle the problem of ISRJ
suppression without relying on a pre-existing ISRJ model. The output of the
WD-AMF encompasses an adaptive filtering term and a compensation term. The
adaptive filtering term encompasses the adaptive integration outcomes in the
waveform domain, which are determined by an adaptive weighted function. This
function, akin to a collection of bandpass filters, decomposes the integrated
function into multiple components, some of which contain interference while
others do not. The compensation term adheres to an integrated guideline for
discerning the presence of signal components or noise within the integrated
function. The integration results are then concatenated to reconstruct a
compensated matched filter signal output. Simulations are conducted to showcase
the exceptional capability of the proposed method in suppressing ISRJ in
diverse interference scenarios, even in the absence of a pre-existing ISRJ
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Modeling and Rate Coverage Analysis for Satellite-Terrestrial
  Integrated Networks: Coverage Extension or Data Offloading? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonghun Park, Jinseok Choi, Namyoon Lee, François Baccelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing interest in satellite networks, satellite-terrestrial
integrated networks (STINs) have gained significant attention because of their
potential benefits. However, due to the lack of a tractable network model for
the STIN architecture, analytical studies allowing one to investigate the
performance of such networks are not yet available. In this work, we propose a
unified network model that jointly captures satellite and terrestrial networks
into one analytical framework. Our key idea is based on Poisson point processes
distributed on concentric spheres, assigning a random height to each point as a
mark. This allows one to consider each point as a source of desired signal or a
source of interference while ensuring visibility to the typical user. Thanks to
this model, we derive the probability of coverage of STINs as a function of
major system parameters, chiefly path-loss exponent, satellites and terrestrial
base stations' height distributions and density, transmit power and biasing
factors. Leveraging the analysis, we concretely explore two benefits that STINs
provide: i) coverage extension in remote rural areas and ii) data offloading in
dense urban areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Prediction of Recurrent Stress Events Using <span class="highlight-title">Self-Supervised</span>
  Learning on Multimodal Time-Series Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanvir Islam, Peter Washington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic stress can significantly affect physical and mental health. The
advent of wearable technology allows for the tracking of physiological signals,
potentially leading to innovative stress prediction and intervention methods.
However, challenges such as label scarcity and data heterogeneity render stress
prediction difficult in practice. To counter these issues, we have developed a
multimodal personalized stress prediction system using wearable biosignal data.
We employ self-supervised learning (SSL) to pre-train the models on each
subject's data, allowing the models to learn the baseline dynamics of the
participant's biosignals prior to fine-tuning the stress prediction task. We
test our model on the Wearable Stress and Affect Detection (WESAD) dataset,
demonstrating that our SSL models outperform non-SSL models while utilizing
less than 5% of the annotations. These results suggest that our approach can
personalize stress prediction to each user with minimal annotations. This
paradigm has the potential to enable personalized prediction of a variety of
recurring health events using complex multimodal data streams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot Motion Prediction by Channel State Information <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rojin Zandi, Hojjat Salehinejad, Kian Behzad, Elaheh Motamedi, Milad Siami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robotic systems have gained a lot of attention, in recent years.
However, accurate prediction of robot motion in indoor environments with
limited visibility is challenging. While vision-based and light detection and
ranging (LiDAR) sensors are commonly used for motion detection and localization
of robotic arms, they are privacy-invasive and depend on a clear line-of-sight
(LOS) for precise measurements. In cases where additional sensors are not
available or LOS is not possible, these technologies may not be the best
option. This paper proposes a novel method that employs channel state
information (CSI) from WiFi signals affected by robotic arm motion. We
developed a convolutional neural network (CNN) model to classify four different
activities of a Franka Emika robotic arm. The implemented method seeks to
accurately predict robot motion even in scenarios in which the robot is
obscured by obstacles, without relying on any attached or internal sensors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 10 figures, 2 tables, MLSP Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effect of Intensity Standardization on Deep Learning for WML
  Segmentation in Multi-Centre FLAIR MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdollah Ghazvanchahi, Pejman Jahbedar Maralani, Alan R. Moody, April Khademi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI
suffer a reduction in performance when applied on data from a scanner or centre
that is out-of-distribution (OOD) from the training data. This is critical for
translation and widescale adoption, since current models cannot be readily
applied to data from new institutions. In this work, we evaluate several
intensity standardization methods for MRI as a preprocessing step for WML
segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI.
We evaluate a method specifically developed for FLAIR MRI called IAMLAB along
with other popular normalization techniques such as White-strip, Nyul and
Z-score. We proposed an Ensemble model that combines predictions from each of
these models. A skip-connection UNet (SC UNet) was trained on the standardized
images, as well as the original data and segmentation performance was evaluated
over several dimensions. The training (in-distribution) data consists of a
single study, of 60 volumes, and the test (OOD) data is 128 unseen volumes from
three clinical cohorts. Results show IAMLAB and Ensemble provide higher WML
segmentation performance compared to models from original data or other
normalization methods. IAMLAB & Ensemble have the highest dice similarity
coefficient (DSC) on the in-distribution data (0.78 & 0.80) and on clinical OOD
data. DSC was significantly higher for IAMLAB compared to the original data
(p<0.05) for all lesion categories (LL>25mL: 0.77 vs. 0.71; 10mL<= LL<25mL:
0.66 vs. 0.61; LL<10mL: 0.53 vs. 0.52). The IAMLAB and Ensemble normalization
methods are mitigating MRI domain shift and are optimal for DL-based WML
segmentation in unseen FLAIR data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Over-the-Air Computation in OFDM Systems with Imperfect Channel State
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilong Chen, Huijun Xing, Jie Xu, Lexi Xu, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the over-the-air computation (AirComp) in an orthogonal
frequency division multiplexing (OFDM) system with imperfect channel state
information (CSI), in which multiple single-antenna wireless devices (WDs)
simultaneously send uncoded signals to a multi-antenna access point (AP) for
distributed functional computation over multiple subcarriers. In particular, we
consider two scenarios with best-effort and error-constrained computation
tasks, with the objectives of minimizing the average computation mean squared
error (MSE) and the computation outage probability over the multiple
subcarriers, respectively. Towards this end, we jointly optimize the transmit
coefficients at the WDs and the receive beamforming vectors at the AP over
subcarriers, subject to the maximum transmit power constraints at individual
WDs. First, for the special case with a single receive antenna at the AP, we
propose the semi-closed-form globally optimal solutions to the two problems
using the Lagrange-duality method. It is shown that at each subcarrier, the
WDs' optimized power control policy for average MSE minimization follows a
regularized channel inversion structure, while that for computation outage
probability minimization follows an on-off regularized channel inversion, with
the regularization dependent on the transmit power budget and channel
estimation error. Next, for the general case with multiple receive antennas at
the AP, we present efficient algorithms based on alternating optimization and
convex optimization to find converged solutions to both problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Self-Supervised</span> Algorithm for Denoising Photoplethysmography Signals
  for Heart Rate Estimation from Wearables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranay Jain, Cheng Ding, Cynthia Rudin, Xiao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart watches and other wearable devices are equipped with
photoplethysmography (PPG) sensors for monitoring heart rate and other aspects
of cardiovascular health. However, PPG signals collected from such devices are
susceptible to corruption from noise and motion artifacts, which cause errors
in heart rate estimation. Typical denoising approaches filter or reconstruct
the signal in ways that eliminate much of the morphological information, even
from the clean parts of the signal that would be useful to preserve. In this
work, we develop an algorithm for denoising PPG signals that reconstructs the
corrupted parts of the signal, while preserving the clean parts of the PPG
signal. Our novel framework relies on self-supervised training, where we
leverage a large database of clean PPG signals to train a denoising
autoencoder. As we show, our reconstructed signals provide better estimates of
heart rate from PPG signals than the leading heart rate estimation methods.
Further experiments show significant improvement in Heart Rate Variability
(HRV) estimation from PPG signals using our algorithm. We conclude that our
algorithm denoises PPG signals in a way that can improve downstream analysis of
many different health metrics from wearable devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Universal Adaptive Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cassio G. Lopes, Vítor H. Nascimento, Luiz F. O. Chamon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive networks (ANs) are effective real time techniques to process and
track events observed by sensor networks and, more recently, to equip Internet
of Things (IoT) applications. ANs operate over nodes equipped with
collaborative adaptive filters that solve distributively an estimation problem
common to the whole network. However, they do not guarantee that nodes do not
lose from cooperation, as compared to its non-cooperative operation; that poor
nodes are rejected and exceptional nodes estimates reach the entire network;
and that performance is uniform over all nodes. In order to enforce such
properties, this work introduces the concept of distributed universal
estimation, which encompasses the new concepts of local universality, global
universality and universality with respect to the non-cooperative operation. We
then construct a new cooperation protocol that is proven to be distributively
universal, outperforming direct competitors from the literature, as shown by
several simulations. Mean and mean-square analytical models are developed, with
good agreement between theory and simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Broadband Channel Estimation for Intelligent Reflecting Surface Aided
  mmWave Massive MIMO Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.01629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.01629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Wan, Zhen Gao, Mohamed-Slim Alouini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the broadband channel estimation (CE) for intelligent
reflecting surface (IRS)-aided millimeter-wave (mmWave) massive MIMO systems.
The CE for such systems is a challenging task due to the large dimension of
both the active massive MIMO at the base station (BS) and passive IRS. To
address this problem, this paper proposes a compressive sensing (CS)-based CE
solution for IRS-aided mmWave massive MIMO systems, whereby the angular channel
sparsity of large-scale array at mmWave is exploited for improved CE with
reduced pilot overhead. Specifically, we first propose a downlink pilot
transmission framework. By designing the pilot signals based on the prior
knowledge that the line-of-sight dominated BS-to-IRS channel is known, the
high-dimensional channels for BS-to-user and IRS-to-user can be jointly
estimated based on CS theory. Moreover, to efficiently estimate broadband
channels, a distributed orthogonal matching pursuit algorithm is exploited,
where the common sparsity shared by the channels at different subcarriers is
utilized. Additionally, the redundant dictionary to combat the power leakage is
also designed for the enhanced CE performance. Simulation results demonstrate
the effectiveness of the proposed scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures. Accepted by IEEE International Conference on
  Communications (ICC) 2020, Dublin, Ireland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Binary classification of spoken words with passive phononic
  metamaterials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tena Dubček, Daniel Moreno-Garcia, Thomas Haag, Parisa Omidvar, Henrik R. Thomsen, Theodor S. Becker, Lars Gebraad, Christoph Bärlocher, Fredrik Andersson, Sebastian D. Huber, Dirk-Jan van Manen, Luis Guillermo Villanueva, Johan O. A. Robertsson, Marc Serra-Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the energy requirements of artificial intelligence requires novel
physical substrates for computation. Phononic metamaterials have a vanishingly
low power dissipation and hence are a prime candidate for green, always-on
computers. However, their use in machine learning applications has not been
explored due to the complexity of their design process: Current phononic
metamaterials are restricted to simple geometries (e.g. periodic, tapered), and
hence do not possess sufficient expressivity to encode machine learning tasks.
We design and fabricate a non-periodic phononic metamaterial, directly from
data samples, that can distinguish between pairs of spoken words in the
presence of a simple readout nonlinearity; hence demonstrating that phononic
metamaterials are a viable avenue towards zero-power smart devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avoiding Post-Processing with Event-Based Detection in Biomedical
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Seeuws, Maarten De Vos, Alexander Bertrand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Finding events of interest is a common task in biomedical signal
processing. The detection of epileptic seizures and signal artefacts are two
key examples. Epoch-based classification is the typical machine learning
framework to detect such signal events because of the straightforward
application of classical machine learning techniques. Usually, post-processing
is required to achieve good performance and enforce temporal dependencies.
Designing the right post-processing scheme to convert these classification
outputs into events is a tedious, and labor-intensive element of this
framework. Methods: We propose an event-based modeling framework that directly
works with events as learning targets, stepping away from ad-hoc
post-processing schemes to turn model outputs into events. We illustrate the
practical power of this framework on simulated data and real-world data,
comparing it to epoch-based modeling approaches. Results: We show that
event-based modeling (without post-processing) performs on par with or better
than epoch-based modeling with extensive post-processing. Conclusion: These
results show the power of treating events as direct learning targets, instead
of using ad-hoc post-processing to obtain them, severely reducing design
effort. Significance: The event-based modeling framework can easily be applied
to other event detection problems in signal processing, removing the need for
intensive task-specific post-processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Hybrid Beamforming Design for Dual-Function Radar-Communication
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wang, Hongyu Li, Ziyang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates dynamic hybrid beamforming (HBF) for a dual-function
radar-communication (DFRC) system, where the DFRC base station (BS)
simultaneously serves multiple single-antenna users and senses a target in the
presence of multiple clutters. Particularly, we apply a HBF architecture with
dynamic subarrays and double phase shifters in the DFRC BS. Aiming at
maximizing the radar mutual information, we consider jointly designing the
dynamic HBF of the DFRC system, subject to the constraints of communication
quality of service (QoS), transmit power, and analog beamformer. To solve the
complicated non-convex optimization, an efficient alternating optimization
algorithm based on the majorization-minimization methods is developed.
Simulation results verify the advancement of the considered HBF architecture
and the effectiveness of the proposed design method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Importance-Aware Communications Using <span class="highlight-title">Pre-train</span>ed Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaishuai Guo, Yanhu Wang, Shujing Li, Nasir Saeed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter proposes a semantic importance-aware communication (SIAC) scheme
using pre-trained language models (e.g., ChatGPT, BERT, etc.). Specifically, we
propose a cross-layer design with a pre-trained language model embedded
in/connected by the cross-layer manager. The pre-trained language model is
utilized to quantify the semantic importance of data frames. Based on the
quantified semantic importance, we investigate semantic importance-aware power
allocation. Unlike existing deep joint source-channel coding (Deep-JSCC)-based
semantic communication schemes, SIAC can be directly embedded into current
communication systems by only introducing a cross-layer manager. Our
experimental results show that the proposed SIAC scheme can achieve lower
semantic loss than existing equal-priority communications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Communications Letters, Semantic communications,
  pre-trained language model, ChatGPT, BERT, data importance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Optimal Transport for Domain Adaptation on SPD Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05745v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05745v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Ju, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been significant interest in solving the domain
adaptation (DA) problem on symmetric positive definite (SPD) manifolds within
the machine learning community. This interest stems from the fact that complex
neurophysiological data generated by medical equipment, such as
electroencephalograms, magnetoencephalograms, and diffusion tensor imaging,
often exhibit a shift in data distribution across different domains. These data
representations, represented by signal covariance matrices, possess properties
of symmetry and positive definiteness. However, directly applying previous
experiences and solutions to the DA problem poses challenges due to the
manipulation complexities of covariance matrices.To address this, our research
introduces a category of deep learning-based transfer learning approaches
called deep optimal transport. This category utilizes optimal transport theory
and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we
present a comprehensive categorization of existing geometric methods to tackle
these problems effectively. This categorization provides practical solutions
for specific DA problems, including handling discrepancies in marginal and
conditional distributions between the source and target domains on the SPD
manifold. To evaluate the effectiveness, we conduct experiments on three
publicly available highly non-stationary cross-session brain-computer interface
scenarios. Moreover, we provide visualization results on the SPD cone to offer
further insights into the framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, and 4 tables; This work has been submitted to
  the IEEE for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed fusion filter over lossy wireless sensor networks with the
  presence of non-Gaussian noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng He, Bei Peng, Zhenyu Feng, Xuemei Mao, Song Gao, Gang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The information transmission between nodes in a wireless sensor networks
(WSNs) often causes packet loss due to denial-of-service (DoS) attack, energy
limitations, and environmental factors, and the information that is
successfully transmitted can also be contaminated by non-Gaussian noise. The
presence of these two factors poses a challenge for distributed state
estimation (DSE) over WSNs. In this paper, a generalized packet drop model is
proposed to describe the packet loss phenomenon caused by DoS attacks and other
factors. Moreover, a modified maximum correntropy Kalman filter is given, and
it is extended to distributed form (DM-MCKF). In addition, a distributed
modified maximum correntropy Kalman filter incorporating the generalized data
packet drop (DM-MCKF-DPD) algorithm is provided to implement DSE with the
presence of both non-Gaussian noise pollution and packet drop. A sufficient
condition to ensure the convergence of the fixed-point iterative process of the
DM-MCKF-DPD algorithm is presented and the computational complexity of the
DM-MCKF-DPD algorithm is analyzed. Finally, the effectiveness and feasibility
of the proposed algorithms are verified by simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Age of Information in Downlink Systems: Broadcast or Unicast
  Transmission? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15078v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15078v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Tang, Nan Yang, Parastoo Sadeghi, Xiangyun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analytically decide whether the broadcast transmission scheme or the
unicast transmission scheme achieves the optimal age of information (AoI)
performance of a multiuser system where a base station (BS) generates and
transmits status updates to multiple user equipments (UEs). In the broadcast
transmission scheme, the status update for all UEs is jointly encoded into a
packet for transmission, while in the unicast transmission scheme, the status
update for each UE is encoded individually and transmitted by following the
round robin policy. For both transmission schemes, we examine three packet
management strategies, namely the non-preemption strategy, the preemption in
buffer strategy, and the preemption in serving strategy. We first derive new
closed-form expressions for the average AoI achieved by two transmission
schemes with three packet management strategies. Based on them, we compare the
AoI performance of two transmission schemes in two systems, namely, the remote
control system and the dynamic system. Aided by simulation results, we verify
our analysis and investigate the impact of system parameters on the average
AoI. For example, the unicast transmission scheme is more appropriate for the
system with a large number UEs. Otherwise, the broadcast transmission scheme is
more appropriate.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LTE SFBC MIMO Transmitter Modelling and Performance Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriela Morillo, John Cosmas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High data rates are one of the most prevalent requirements in current mobile
communications. To cover this and other high standards regarding performance,
increasing coverage, capacity, and reliability, numerous works have proposed
the development of systems employing the combination of several techniques such
as Multiple Input Multiple Output (MIMO) wireless technologies with Orthogonal
Frequency Division Multiplexing (OFDM) in the evolving 4G wireless
communications. Our proposed system is based on the 2x2 MIMO antenna technique,
which is defined to enhance the performance of radio communication systems in
terms of capacity and spectral efficiency, and the OFDM technique, which can be
implemented using two types of sub-carrier mapping modes: Space-Time Block
Coding and Space Frequency Block Code. SFBC has been considered in our
developed model. The main advantage of SFBC over STBC is that SFBC encodes two
modulated symbols over two subcarriers of the same OFDM symbol, whereas STBC
encodes two modulated symbols over two subcarriers of the same OFDM symbol;
thus, the coding is performed in the frequency domain. Our solution aims to
demonstrate the performance analysis of the Space Frequency Block Codes scheme,
increasing the Signal Noise Ratio (SNR) at the receiver and decreasing the Bit
Error Rate (BER) through the use of 4 QAM, 16 QAM and 64QAM modulation over a
2x2 MIMO channel for an LTE downlink transmission, in different channel radio
environments. In this work, an analytical tool to evaluate the performance of
SFBC - Orthogonal Frequency Division Multiplexing, using two transmit antennas
and two receive antennas has been implemented, and the analysis using the
average SNR has been considered as a sufficient statistic to describe the
performance of SFBC in the 3GPP Long Term Evolution system over Multiple Input
Multiple Output channels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 20 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 6LoRa: Full Stack IPv6 Networking with DSME-LoRa on Low Power IoT Nodes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Álamos, Thomas Schmidt, Matthias Waehlisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long range wireless transmission techniques such as LoRa are preferential
candidates for a substantial class of IoT applications, as they avoid the
complexity of multi-hop wireless forwarding. The existing network solutions for
LoRa, however, are not suitable for peer-to-peer communication, which is a key
requirement for many IoT applications. In this work, we propose a networking
system - 6LoRa, that enables IPv6 communication over LoRa. We present a full
stack system implementation on RIOT OS and evaluate the system on a real
testbed using realistic application scenarios with CoAP. Our findings confirm
that our approach outperforms existing solutions in terms of transmission delay
and packet reception ratio at comparable energy consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large AI Model-Based Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communication (SC) is an emerging intelligent paradigm, offering
solutions for various future applications like metaverse, mixed-reality, and
the Internet of everything. However, in current SC systems, the construction of
the knowledge base (KB) faces several issues, including limited knowledge
representation, frequent knowledge updates, and insecure knowledge sharing.
Fortunately, the development of the large AI model provides new solutions to
overcome above issues. Here, we propose a large AI model-based SC framework
(LAM-SC) specifically designed for image data, where we first design the
segment anything model (SAM)-based KB (SKB) that can split the original image
into different semantic segments by universal semantic knowledge. Then, we
present an attention-based semantic integration (ASI) to weigh the semantic
segments generated by SKB without human participation and integrate them as the
semantic-aware image. Additionally, we propose an adaptive semantic compression
(ASC) encoding to remove redundant information in semantic features, thereby
reducing communication overhead. Finally, through simulations, we demonstrate
the effectiveness of the LAM-SC framework and the significance of the large AI
model-based KB development in future SC paradigms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Plan to submit it to journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning over a Wireless Network: Distributed User Selection
  through Random Access 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Sun, Shiyao Ma, Ce Zheng, Songtao Wu, Tao Cui, Lingjuan Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User selection has become crucial for decreasing the communication costs of
federated learning (FL) over wireless networks. However, centralized user
selection causes additional system complexity. This study proposes a network
intrinsic approach of distributed user selection that leverages the radio
resource competition mechanism in random access. Taking the carrier sensing
multiple access (CSMA) mechanism as an example of random access, we manipulate
the contention window (CW) size to prioritize certain users for obtaining radio
resources in each round of training. Training data bias is used as a target
scenario for FL with user selection. Prioritization is based on the distance
between the newly trained local model and the global model of the previous
round. To avoid excessive contribution by certain users, a counting mechanism
is used to ensure fairness. Simulations with various datasets demonstrate that
this method can rapidly achieve convergence similar to that of the centralized
user selection approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resource Management in Quantum Virtual Private Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03231v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03231v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahrooz Pouryousef, Nitish K. Panigrahy, Monimoy Deb Purkayastha, Sabyasachi Mukhopadhyay, Gert Grammel, Domenico Di Mola, Don Towsley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we develop a resource management framework for a quantum
virtual private network (qVPN), which involves the sharing of an underlying
public quantum network by multiple organizations for quantum entanglement
distribution. Our approach involves resolving the issue of link entanglement
resource allocation in a qVPN by utilizing a centralized optimization
framework. We provide insights into the potential of genetic and learning-based
algorithms for optimizing qVPNs, and emphasize the significance of path
selection and distillation in enabling efficient and reliable quantum
communication in multi-organizational settings. Our findings demonstrate that
compared to traditional greedy based heuristics, genetic and learning-based
algorithms can identify better paths. Furthermore, these algorithms can
effectively identify good distillation strategies to mitigate potential noises
in gates and quantum channels, while ensuring the necessary quality of service
for end users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Learning over Wireless Networks: The Effect of Broadcast
  with Random Access <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chen, Martin Dahl, Erik G. Larsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we focus on the communication aspect of decentralized learning,
which involves multiple agents training a shared machine learning model using
decentralized stochastic gradient descent (D-SGD) over distributed data. In
particular, we investigate the impact of broadcast transmission and
probabilistic random access policy on the convergence performance of D-SGD,
considering the broadcast nature of wireless channels and the link dynamics in
the communication topology. Our results demonstrate that optimizing the access
probability to maximize the expected number of successful links is a highly
effective strategy for accelerating the system convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, accepted in IEEE SPAWC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALBUS: a Probabilistic Monitoring Algorithm to Counter Burst-Flood
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Scherrer, Jo Vliegen, Arish Sateesan, Hsu-Chun Hsiao, Nele Mentens, Adrian Perrig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern DDoS defense systems rely on probabilistic monitoring algorithms to
identify flows that exceed a volume threshold and should thus be penalized.
Commonly, classic sketch algorithms are considered sufficiently accurate for
usage in DDoS defense. However, as we show in this paper, these algorithms
achieve poor detection accuracy under burst-flood attacks, i.e., volumetric
DDoS attacks composed of a swarm of medium-rate sub-second traffic bursts.
Under this challenging attack pattern, traditional sketch algorithms can only
detect a high share of the attack bursts by incurring a large number of false
positives.
  In this paper, we present ALBUS, a probabilistic monitoring algorithm that
overcomes the inherent limitations of previous schemes: ALBUS is highly
effective at detecting large bursts while reporting no legitimate flows, and
therefore improves on prior work regarding both recall and precision. Besides
improving accuracy, ALBUS scales to high traffic rates, which we demonstrate
with an FPGA implementation, and is suitable for programmable switches, which
we showcase with a P4 implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 42nd International Symposium on Reliable Distributed
  Systems (SRDS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Explainable AI for 6G O-RAN: Architecture, Use Cases,
  Challenges and Research Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00319v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00319v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bouziane Brik, Hatim Chergui, Lanfranco Zanzi, Francesco Devoti, Adlen Ksentini, Muhammad Shuaib Siddiqui, Xavier Costa-Pérez, Christos Verikoukis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent O-RAN specifications promote the evolution of RAN architecture by
function disaggregation, adoption of open interfaces, and instantiation of a
hierarchical closed-loop control architecture managed by RAN Intelligent
Controllers (RICs) entities. This paves the road to novel data-driven network
management approaches based on programmable logic. Aided by Artificial
Intelligence (AI) and Machine Learning (ML), novel solutions targeting
traditionally unsolved RAN management issues can be devised. Nevertheless, the
adoption of such smart and autonomous systems is limited by the current
inability of human operators to understand the decision process of such AI/ML
solutions, affecting their trust in such novel tools. eXplainable AI (XAI) aims
at solving this issue, enabling human users to better understand and
effectively manage the emerging generation of artificially intelligent schemes,
reducing the human-to-machine barrier. In this survey, we provide a summary of
the XAI methods and metrics before studying their deployment over the O-RAN
Alliance RAN architecture along with its main building blocks. We then present
various use-cases and discuss the automation of XAI pipelines for O-RAN as well
as the underlying security aspects. We also review some projects/standards that
tackle this area. Finally, we identify different challenges and research
directions that may arise from the heavy adoption of AI/ML decision entities in
this context, focusing on how XAI can help to interpret, understand, and
improve trust in O-RAN operational networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computationally Efficient Worst-Case Analysis of Flow-Controlled
  Networks with Network Calculus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffaele Zippo, Giovanni Stea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Networks with hop-by-hop flow control occur in several contexts, from data
centers to systems architectures (e.g., wormhole-routing networks on chip). A
worst-case end-to-end delay in such networks can be computed using Network
Calculus (NC), an algebraic theory where traffic and service guarantees are
represented as curves in a Cartesian plane. NC uses transformation operations,
e.g., the min-plus convolution, to model how the traffic profile changes with
the traversal of network nodes. NC allows one to model flow-controlled systems,
hence one can compute the end-to-end service curve describing the minimum
service guaranteed to a flow traversing a tandem of flow-controlled nodes.
However, while the algebraic expression of such an end-to-end service curve is
quite compact, its computation is often intractable from an algorithmic
standpoint: data structures tend to grow quickly to unfeasibly large sizes,
making operations intractable, even with as few as three hops. In this paper,
we propose computational and algebraic techniques to mitigate the above
problem. We show that existing techniques (such as reduction to compact
domains) cannot be used in this case, and propose an arsenal of solutions,
which include methods to mitigate the data representation space explosion as
well as computationally efficient algorithms for the min-plus convolution
operation. We show that our solutions allow a significant speedup, enable
analysis of previously unfeasible case studies, and -- since they do not rely
on any approximation -- still provide exact results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 33 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nancy: An efficient parallel Network Calculus library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffaele Zippo, Giovanni Stea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes Nancy, a Network Calculus (NC) library that allows users
to perform complex min-plus and max-plus algebra operations efficiently. To the
best of our knowledge, Nancy is the only open-source library that implements
operations working on arbitrary piecewise affine functions, as well as to
implement some of them (e.g. sub-additive closure and function composition).
Nancy allows researchers to compute NC results using a straightforward syntax,
which matches the algebraic one. Moreover, it is designed having computational
efficiency in mind: it exploits optimizations of data structures, it uses
inheritance to allow for faster algorithms when they are available (e.g., for
specific subclasses of functions), and it is natively parallel, thus reaping
the benefit of multicore hardware. This makes it usable to solve NC problems
which were previously considered beyond the realm of tractable.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Traditional and LLM-based Search for Consumer Choice: A
  Randomized Experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Eleni Spatharioti, David M. Rothschild, Daniel G. Goldstein, Jake M. Hofman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the development of large language models are rapidly
changing how online applications function. LLM-based search tools, for
instance, offer a natural language interface that can accommodate complex
queries and provide detailed, direct responses. At the same time, there have
been concerns about the veracity of the information provided by LLM-based tools
due to potential mistakes or fabrications that can arise in algorithmically
generated text. In a set of online experiments we investigate how LLM-based
search changes people's behavior relative to traditional search, and what can
be done to mitigate overreliance on LLM-based output. Participants in our
experiments were asked to solve a series of decision tasks that involved
researching and comparing different products, and were randomly assigned to do
so with either an LLM-based search tool or a traditional search engine. In our
first experiment, we find that participants using the LLM-based tool were able
to complete their tasks more quickly, using fewer but more complex queries than
those who used traditional search. Moreover, these participants reported a more
satisfying experience with the LLM-based search tool. When the information
presented by the LLM was reliable, participants using the tool made decisions
with a comparable level of accuracy to those using traditional search, however
we observed overreliance on incorrect information when the LLM erred. Our
second experiment further investigated this issue by randomly assigning some
users to see a simple color-coded highlighting scheme to alert them to
potentially incorrect or misleading information in the LLM responses. Overall
we find that this confidence-based highlighting substantially increases the
rate at which users spot incorrect information, improving the accuracy of their
overall decisions while leaving most other measures unaffected.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "How Did They Come Across?" Lessons Learned from Continuous Affective
  Ratings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Teresa Parreira, Michael J. Sack, Hifza Javed, Nawid Jamali, Malte Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social distance, or perception of the other, is recognized as a dynamic
dimension of an interaction, but yet to be widely explored or understood.
Through CORAE, a novel web-based open-source tool for COntinuous Retrospective
Affect Evaluation, we collected retrospective ratings of interpersonal
perceptions between 12 participant dyads. In this work, we explore how
different aspects of these interactions reflect on the ratings collected,
through a discourse analysis of individual and social behavior of the
interactants. We found that different events observed in the ratings can be
mapped to complex interaction phenomena, shedding light on relevant interaction
features that may play a role in interpersonal understanding and grounding.
This paves the way for better, more seamless human-robot interactions, where
affect is interpreted as highly dynamic and contingent on interaction history.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2306.16629</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physical-aware Cross-modal Adversarial Network for Wearable Sensor-based
  Human Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyuan Ni, Hao Tang, Anne H. H. Ngu, Gaowen Liu, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable sensor-based Human Action Recognition (HAR) has made significant
strides in recent times. However, the accuracy performance of wearable
sensor-based HAR is currently still lagging behind that of visual
modalities-based systems, such as RGB video and depth data. Although diverse
input modalities can provide complementary cues and improve the accuracy
performance of HAR, wearable devices can only capture limited kinds of
non-visual time series input, such as accelerometers and gyroscopes. This
limitation hinders the deployment of multimodal simultaneously using visual and
non-visual modality data in parallel on current wearable devices. To address
this issue, we propose a novel Physical-aware Cross-modal Adversarial (PCA)
framework that utilizes only time-series accelerometer data from four inertial
sensors for the wearable sensor-based HAR problem. Specifically, we propose an
effective IMU2SKELETON network to produce corresponding synthetic skeleton
joints from accelerometer data. Subsequently, we imposed additional constraints
on the synthetic skeleton data from a physical perspective, as accelerometer
data can be regarded as the second derivative of the skeleton sequence
coordinates. After that, the original accelerometer as well as the constrained
skeleton sequence were fused together to make the final classification. In this
way, when individuals wear wearable devices, the devices can not only capture
accelerometer data, but can also generate synthetic skeleton sequences for
real-time wearable sensor-based HAR applications that need to be conducted
anytime and anywhere. To demonstrate the effectiveness of our proposed PCA
framework, we conduct extensive experiments on Berkeley-MHAD, UTD-MHAD, and
MMAct datasets. The results confirm that the proposed PCA approach has
competitive performance compared to the previous methods on the mono
sensor-based HAR classification problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First IMU2SKELETON GANs approach for wearable HAR problem. arXiv
  admin note: text overlap with arXiv:2208.08090</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Patch, or not To Patch? That is the Question: A Case Study of System
  Administrators' Online Collaborative Behaviour 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Jenkins, Maria Wolters, Kami Vaniea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System administrators, similar to end users, may delay or avoid software
patches, also known as updates, despite the impact their timely application can
have on system security. These admins are responsible for large, complex,
amalgamated systems and must balance the security related needs of their
organizations, which would benefit from the patch, with the need to ensure that
systems must continue to run unimpeded. In this paper, we present a case study
which follows the online life-cycle of a pair of Microsoft patches. We find
that communities of sysadmins have evolved sophisticated mechanisms to perform
risk assessments that are centred around collecting, synthesizing, and
generating information on patches. These communities span different Virtual
Communities of Practice, as well as influencers who monitor and report on the
impact of new patches. As information is propagated and aggregated across
blogs, forums, web sites, and mailing lists, eventually resulting in a
consensus around the risk of a patch. Our findings highlight the role that
these communities play in informing risk management decisions: Patch
information is not static, and it transforms as communities collaborate to
understand patch issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContextLabeler <span class="highlight-title">Dataset</span>: physical and virtual sensors data collected from
  smartphone usage in-the-wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Giovanni Campana, Franca Delmastro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a data collection campaign and the resulting dataset
derived from smartphone sensors characterizing the daily life activities of 3
volunteers in a period of two weeks. The dataset is released as a collection of
CSV files containing more than 45K data samples, where each sample is composed
by 1332 features related to a heterogeneous set of physical and virtual
sensors, including motion sensors, running applications, devices in proximity,
and weather conditions. Moreover, each data sample is associated with a ground
truth label that describes the user activity and the situation in which she was
involved during the sensing experiment (e.g., working, at restaurant, and doing
sport activity). To avoid introducing any bias during the data collection, we
performed the sensing experiment in-the-wild, that is, by using the volunteers'
devices, and without defining any constraint related to the user's behavior.
For this reason, the collected dataset represents a useful source of real data
to both define and evaluate a broad set of novel context-aware solutions (both
algorithms and protocols) that aim to adapt their behavior according to the
changes in the user's situation in a mobile environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Perceived and Mechanical Challenge in Games Through
  Cognitive Activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christine Hegedues, Joao Pedro Dias Constantino, Laurits Dixen, Paolo Burelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Game difficulty is a crucial aspect of game design, that can be directly
influenced by tweaking game mechanics. Perceived difficulty can however also be
influenced by simply altering the graphics to something more threatening. Here,
we present a study with 12 participants playing 4 different minigames with
either altered graphics or mechanics to make the game more difficult. Using EEG
bandpower analysis, we find that frontal lobe activity is heightened in all 4
of the mechanically challenging versions and 2/4 of the visually altered
versions, all differences that do not emerge from the self-reported player
experience. This suggests that EEG could aid researchers with a more sensitive
tool for investigating challenge in games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality for Maintenance Tasks with Chat<span class="highlight-title">GPT</span> for Automated
  Text-to-Action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Xu, Tri Nguyen, Jing Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in sensor technology, artificial intelligence (AI), and
augmented reality (AR) have unlocked opportunities across various domains. AR
and large language models like GPT have witnessed substantial progress and are
increasingly being employed in diverse fields. One such promising application
is in operations and maintenance (O&M). O&M tasks often involve complex
procedures and sequences that can be challenging to memorize and execute
correctly, particularly for novices or under high-stress situations. By
marrying the advantages of superimposing virtual objects onto the physical
world, and generating human-like text using GPT, we can revolutionize O&M
operations. This study introduces a system that combines AR, Optical Character
Recognition (OCR), and the GPT language model to optimize user performance
while offering trustworthy interactions and alleviating workload in O&M tasks.
This system provides an interactive virtual environment controlled by the Unity
game engine, facilitating a seamless interaction between virtual and physical
realities. A case study (N=15) is conducted to illustrate the findings and
answer the research questions. The results indicate that users can complete
similarly challenging tasks in less time using our proposed AR and AI system.
Moreover, the collected data also suggests a reduction in cognitive load and an
increase in trust when executing the same operations using the AR and AI
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Drivers of Smart Livestock Technology Adoption in Japan:
  A Scoping <span class="highlight-title">Review</span>, Expert Interviews, and Grounded Theory Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takumi Ohashi, Miki Saijo, Kento Suzuki, Shinsuke Arafuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With global demand for animal products projected to increase significantly by
2050, understanding the factors that influence the adoption of smart livestock
technologies has become increasingly crucial. Conducted within the unique
agricultural context of Japan, our study builds upon traditional theoretical
frameworks that often oversimplify farmers' decision-making processes. By
employing a scoping review, expert interviews, and a Modified Grounded Theory
Approach, our research uncovers the intricate interplay between individual
farmer values, farm management policies, social relations, agricultural
policies, and livestock industry trends. We particularly highlight the unique
dynamics within family-owned businesses, noting the tension between an
"advanced management mindset" and "conservatism." Our study underscores
technology adoption's sequential and iterative nature, intricately tied to
technology availability, farmers' digital literacy, technology implementation
support, and observable technology impacts on animal health and productivity.
Despite certain limitations, our findings carry profound implications for
stakeholders, providing valuable insights to overcome adoption barriers and
advocating for more sustainable, efficient, and animal welfare-oriented
livestock production systems. This research establishes a solid foundation for
future explorations into smart livestock technology adoption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIG: The Data Interface Grammar <span class="chip">SIGMOD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiru Chen, Jeffery Tao, Eugene Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building interactive data interfaces is hard because the design of an
interface depends on the data processing needs for the underlying analysis
task, yet we do not have a good representation for analysis tasks. To fill this
gap, this paper advocates for a Data Interface Grammar (DIG) as an intermediate
representation of analysis tasks. We show that DIG is compatible with existing
data engineering practices, compact to represent any analysis, simple to
translate into an interface design, and amenable to offline analysis. We
further illustrate the potential benefits of this abstraction, such as
automatic interface generation, automatic interface backend optimization,
tutorial generation, and workload generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Workshop on Human-In-the-Loop Data Analytics(HILDA) at
  SIGMOD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teach Me How to Learn: A Perspective <span class="highlight-title">Review</span> towards User-centered
  Neuro-symbolic Learning for Robotic Surgical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amr Gomaa, Bilal Mahdy, Niko Kleer, Michael Feld, Frank Kirchner, Antonio Krüger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in machine learning models allowed robots to identify objects
on a perceptual nonsymbolic level (e.g., through sensor fusion and natural
language understanding). However, these primarily black-box learning models
still lack interpretation and transferability and require high data and
computational demand. An alternative solution is to teach a robot on both
perceptual nonsymbolic and conceptual symbolic levels through hybrid
neurosymbolic learning approaches with expert feedback (i.e., human-in-the-loop
learning). This work proposes a concept for this user-centered hybrid learning
paradigm that focuses on robotic surgical situations. While most recent
research focused on hybrid learning for non-robotic and some generic robotic
domains, little work focuses on surgical robotics. We survey this related
research while focusing on human-in-the-loop surgical robotic systems. This
evaluation highlights the most prominent solutions for autonomous surgical
robots and the challenges surgeons face when interacting with these systems.
Finally, we envision possible ways to address these challenges using online
apprenticeship learning based on implicit and explicit feedback from expert
surgeons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How does AI chat change search behaviors? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Capra, Jaime Arguello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI tools such as chatGPT are poised to change the way people
engage with online information. Recently, Microsoft announced their "new Bing"
search system which incorporates chat and generative AI technology from OpenAI.
Google has announced plans to deploy search interfaces that incorporate similar
types of technology. These new technologies will transform how people can
search for information. The research presented here is an early investigation
into how people make use of a generative AI chat system (referred to simply as
chat from here on) as part of a search process, and how the incorporation of
chat systems with existing search tools may effect users search behaviors and
strategies.
  We report on an exploratory user study with 10 participants who used a
combined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing
Web Search v5 API. Participants completed three search tasks. In this pre-print
paper of preliminary results, we report on ways that users integrated AI chat
into their search process, things they liked and disliked about the chat
system, their trust in the chat responses, and their mental models of how the
chat system generated responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Student Teacher Interaction While Learning Computer Science: Early
  Results from an Experiment on Undergraduates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuela Petrescu, Kuderna Bentasup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scope of this paper was to find out how the students in Computer Science
perceive different teaching styles and how the teaching style impacts the
learning desire and interest in the course. To find out, we designed and
implemented an experiment in which the same groups of students (86 students)
were exposed to different teaching styles (presented by the same teacher at a
difference of two weeks between lectures). We tried to minimize external
factors' impact by carefully selecting the dates (close ones), having the
courses in the same classroom and on the same day of the week, at the same
hour, and checking the number and the complexity of the introduced items to be
comparable. We asked for students' feedback and we define a set of countable
body signs for their involvement in the course. The results were comparable by
both metrics (body language) and text analysis results, students prefer a more
interactive course, with a relaxing atmosphere, and are keener to learn in
these conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CSEDU 2023, 15th International Conference on Computer Supported
  Education</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Procedurally generating rules to adapt difficulty for narrative puzzle
  games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Volden, Djordje Grbic, Paolo Burelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on procedurally generating rules and communicating them to
players to adjust the difficulty. This is part of a larger project to collect
and adapt games in educational games for young children using a digital puzzle
game designed for kindergarten. A genetic algorithm is used together with a
difficulty measure to find a target number of solution sets and a large
language model is used to communicate the rules in a narrative context. During
testing the approach was able to find rules that approximate any given target
difficulty within two dozen generations on average. The approach was combined
with a large language model to create a narrative puzzle game where players
have to host a dinner for animals that can't get along. Future experiments will
try to improve evaluation, specialize the language model on children's
literature, and collect multi-modal data from players to guide adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Comparison of Large Language Models on VNHSGE English
  <span class="highlight-title">Dataset</span>: OpenAI Chat<span class="highlight-title">GPT</span>, Microsoft Bing Chat, and Google Bard 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Quy Dao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a performance comparison of three large language models
(LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard, on the
VNHSGE English dataset. The results show that BingChat is better than ChatGPT
and Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is not
yet officially available in Vietnam. The results also indicate that ChatGPT,
Bing Chat, and Bard outperform Vietnamese students in English language
proficiency. The findings of this study contribute to the understanding of the
potential of LLMs in English language education. The remarkable performance of
ChatGPT, Bing Chat, and Bard demonstrates their potential as effective tools
for teaching and learning English at the high school level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures. arxiv admin note: substantial text overlap with
  arXiv: 2305.12199</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Social Context from Smartphone Sensing: Generalization
  Across Countries and Daily Life Moments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurel Ruben Mader, Lakmal Meegahapola, Daniel Gatica-Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and longitudinally tracking the social context of people help
in understanding their behavior and mental well-being better. Hence, instead of
burdensome questionnaires, some studies used passive smartphone sensors to
infer social context with machine learning models. However, the few studies
that have been done up to date have focused on unique, situated contexts (i.e.,
when eating or drinking) in one or two countries, hence limiting the
understanding of the inference in terms of generalization to (i) everyday life
occasions and (ii) different countries. In this paper, we used a novel,
large-scale, and multimodal smartphone sensing dataset with over 216K
self-reports collected from over 580 participants in five countries (Mongolia,
Italy, Denmark, UK, Paraguay), first to understand whether social context
inference (i.e., alone or not) is feasible with sensor data, and then, to know
how behavioral and country-level diversity affects the inference. We found that
(i) sensor features from modalities such as activity, location, app usage,
Bluetooth, and WiFi could be informative of social context; (ii) partially
personalized multi-country models (trained and tested with data from all
countries) and country-specific models (trained and tested within countries)
achieved similar accuracies in the range of 80%-90%; and (iii) models do not
generalize well to unseen countries regardless of geographic similarity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy, Security, and Usability Tradeoffs of Telehealth from
  Practitioners' Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faiza Tazi, Archana Nandakumar, Josiah Dykstra, Prashanth Rajivan, Sanchari Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has significantly transformed the healthcare sector,
with telehealth services being among the most prominent changes. The adoption
of telehealth services, however, has raised new challenges, particularly in the
areas of security and privacy. To better comprehend the telehealth needs and
concerns of medical professionals, particularly those in private practice, we
conducted a study comprised of 20 semi-structured interviews with telehealth
practitioners in audiology and speech therapy. Our findings indicate that
private telehealth practitioners encounter difficult choices when it comes to
balancing security, privacy, usability, and accessibility, particularly while
caring for vulnerable populations. Additionally, the study revealed that
practitioners face challenges in ensuring HIPAA compliance due to inadequate
resources and a lack of technological comprehension. Policymakers and
healthcare providers should take proactive measures to address these
challenges, including offering resources and training to ensure HIPAA
compliance and enhancing technology infrastructure to support secure and
accessible telehealth.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">64</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When does the ID algorithm fail? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Shpitser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ID algorithm solves the problem of identification of interventional
distributions of the form p(Y | do(a)) in graphical causal models, and has been
formulated in a number of ways [12, 9, 6]. The ID algorithm is sound (outputs
the correct functional of the observed data distribution whenever p(Y | do(a))
is identified in the causal model represented by the input graph), and complete
(explicitly flags as a failure any input p(Y | do(a)) whenever this
distribution is not identified in the causal model represented by the input
graph).
  The reference [9] provides a result, the so called "hedge criterion"
(Corollary 3), which aims to give a graphical characterization of situations
when the ID algorithm fails to identify its input in terms of a structure in
the input graph called the hedge. While the ID algorithm is, indeed, a sound
and complete algorithm, and the hedge structure does arise whenever the input
distribution is not identified, Corollary 3 presented in [9] is incorrect as
stated. In this note, I outline the modern presentation of the ID algorithm,
discuss a simple counterexample to Corollary 3, and provide a number of
graphical characterizations of the ID algorithm failing to identify its input
distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2108.06818</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intelligent Robotic Sonographer: Mutual Information-based Disentangled
  Reward Learning from Few Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongliang Jiang, Yuan Bi, Mingchuan Zhou, Ying Hu, Michael Burke, and Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound (US) imaging is widely used for biometric measurement and
diagnosis of internal organs due to the advantages of being real-time and
radiation-free. However, due to high inter-operator variability, resulting
images highly depend on operators' experience. In this work, an intelligent
robotic sonographer is proposed to autonomously "explore" target anatomies and
navigate a US probe to a relevant 2D plane by learning from expert. The
underlying high-level physiological knowledge from experts is inferred by a
neural reward function, using a ranked pairwise image comparisons approach in a
self-supervised fashion. This process can be referred to as understanding the
"language of sonography". Considering the generalization capability to overcome
inter-patient variations, mutual information is estimated by a network to
explicitly extract the task-related and domain features in latent space.
Besides, a Gaussian distribution-based filter is developed to automatically
evaluate and take the quality of the expert's demonstrations into account. The
robotic localization is carried out in coarse-to-fine mode based on the
predicted reward associated to B-mode images. To demonstrate the performance of
the proposed approach, representative experiments for the "line" target and
"point" target are performed on vascular phantom and two ex-vivo animal organ
phantoms (chicken heart and lamb kidney), respectively. The results
demonstrated that the proposed advanced framework can robustly work on
different kinds of known and unseen phantoms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Potential of Knowledge-<span class="highlight-title">Prompt</span>ed Chat<span class="highlight-title">GPT</span> for Enhancing Drug
  Trafficking Detection on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanbo Hu, Bin Liu, Xin Li, Yanfang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms such as Instagram and Twitter have emerged as critical
channels for drug marketing and illegal sale. Detecting and labeling online
illicit drug trafficking activities becomes important in addressing this issue.
However, the effectiveness of conventional supervised learning methods in
detecting drug trafficking heavily relies on having access to substantial
amounts of labeled data, while data annotation is time-consuming and
resource-intensive. Furthermore, these models often face challenges in
accurately identifying trafficking activities when drug dealers use deceptive
language and euphemisms to avoid detection. To overcome this limitation, we
conduct the first systematic study on leveraging large language models (LLMs),
such as ChatGPT, to detect illicit drug trafficking activities on social media.
We propose an analytical framework to compose \emph{knowledge-informed
prompts}, which serve as the interface that humans can interact with and use
LLMs to perform the detection task. Additionally, we design a Monte Carlo
dropout based prompt optimization method to further to improve performance and
interpretability. Our experimental findings demonstrate that the proposed
framework outperforms other baseline language models in terms of drug
trafficking detection accuracy, showing a remarkable improvement of nearly
12\%. By integrating prior knowledge and the proposed prompts, ChatGPT can
effectively identify and label drug trafficking activities on social networks,
even in the presence of deceptive language and euphemisms used by drug dealers
to evade detection. The implications of our research extend to social networks,
emphasizing the importance of incorporating prior knowledge and scenario-based
prompts into analytical tools to improve online security and public safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Membership Inference Attacks via Quantile Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Bertran, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, Zhiwei Steven Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference attacks are designed to determine, using black box
access to trained models, whether a particular example was used in training or
not. Membership inference can be formalized as a hypothesis testing problem.
The most effective existing attacks estimate the distribution of some test
statistic (usually the model's confidence on the true label) on points that
were (and were not) used in training by training many \emph{shadow models} --
i.e. models of the same architecture as the model being attacked, trained on a
random subsample of data. While effective, these attacks are extremely
computationally expensive, especially when the model under attack is large.
  We introduce a new class of attacks based on performing quantile regression
on the distribution of confidence scores induced by the model under attack on
points that are not used in training. We show that our method is competitive
with state-of-the-art shadow model attacks, while requiring substantially less
compute because our attack requires training only a single model. Moreover,
unlike shadow model attacks, our proposed attack does not require any knowledge
of the architecture of the model under attack and is therefore truly
``black-box". We show the efficacy of this approach in an extensive series of
experiments on various datasets and model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposing the Generalization Gap in Imitation Learning for Visual
  Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annie Xie, Lisa Lee, Ted Xiao, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What makes generalization hard for imitation learning in visual robotic
manipulation? This question is difficult to approach at face value, but the
environment from the perspective of a robot can often be decomposed into
enumerable factors of variation, such as the lighting conditions or the
placement of the camera. Empirically, generalization to some of these factors
have presented a greater obstacle than others, but existing work sheds little
light on precisely how much each factor contributes to the generalization gap.
Towards an answer to this question, we study imitation learning policies in
simulation and on a real robot language-conditioned manipulation task to
quantify the difficulty of generalization to different (sets of) factors. We
also design a new simulated benchmark of 19 tasks with 11 factors of variation
to facilitate more controlled evaluations of generalization. From our study, we
determine an ordering of factors based on generalization difficulty, that is
consistent across simulation and our real robot setup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage at https://sites.google.com/view/generalization-gap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Variable Binding Circuitry with Desiderata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that computation in language models may be
human-understandable, with successful efforts to localize and intervene on both
single-unit features and input-output circuits. Here, we introduce an approach
which extends causal mediation experiments to automatically identify model
components responsible for performing a specific subtask by solely specifying a
set of \textit{desiderata}, or causal attributes of the model components
executing that subtask. As a proof of concept, we apply our method to
automatically discover shared \textit{variable binding circuitry} in LLaMA-13B,
which retrieves variable values for multiple arithmetic tasks. Our method
successfully localizes variable binding to only 9 attention heads (of the 1.6k)
and one MLP in the final token's residual stream.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitan Yang, Malcolm Wolff, Shankar Ramasubramanian, Vincent Quenneville-Belair, Ronak Metha, Michael W. Mahoney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encoder-decoder deep neural networks have been increasingly studied for
multi-horizon time series forecasting, especially in real-world applications.
However, to forecast accurately, these sophisticated models typically rely on a
large number of time series examples with substantial history. A rapidly
growing topic of interest is forecasting time series which lack sufficient
historical data -- often referred to as the ``cold start'' problem. In this
paper, we introduce a novel yet simple method to address this problem by
leveraging graph neural networks (GNNs) as a data augmentation for enhancing
the encoder used by such forecasters. These GNN-based features can capture
complex inter-series relationships, and their generation process can be
optimized end-to-end with the forecasting task. We show that our architecture
can use either data-driven or domain knowledge-defined graphs, scaling to
incorporate information from multiple very large graphs with millions of nodes.
In our target application of demand forecasting for a large e-commerce
retailer, we demonstrate on both a small dataset of 100K products and a large
dataset with over 2 million products that our method improves overall
performance over competitive baseline models. More importantly, we show that it
brings substantially more gains to ``cold start'' products such as those newly
launched or recently out-of-stock.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel
  Synthesis <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paula Feldman, Miguel Fainstein, Viviana Siless, Claudio Delrieux, Emmanuel Iarussi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a data-driven generative framework for synthesizing blood vessel
3D geometry. This is a challenging task due to the complexity of vascular
systems, which are highly variating in shape, size, and structure. Existing
model-based methods provide some degree of control and variation in the
structures produced, but fail to capture the diversity of actual anatomical
data. We developed VesselVAE, a recursive variational Neural Network that fully
exploits the hierarchical organization of the vessel and learns a
low-dimensional manifold encoding branch connectivity along with geometry
features describing the target surface. After training, the VesselVAE latent
space can be sampled to generate new vessel geometries. To the best of our
knowledge, this work is the first to utilize this technique for synthesizing
blood vessels. We achieve similarities of synthetic and real data for radius
(.97), length (.95), and tortuosity (.96). By leveraging the power of deep
neural networks, we generate 3D models of blood vessels that are both accurate
and diverse, which is crucial for medical and surgical training, hemodynamic
simulations, and many other purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Deep Learning for Personalized Renal Cell Carcinoma
  Prognosis: Integrating CT Imaging and Clinical Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryamalsadat Mahootiha, Hemin Ali Qadir, Jacob Bergsland, Ilangko Balasingham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Renal cell carcinoma represents a significant global health challenge with a
low survival rate. This research aimed to devise a comprehensive deep-learning
model capable of predicting survival probabilities in patients with renal cell
carcinoma by integrating CT imaging and clinical data and addressing the
limitations observed in prior studies. The aim is to facilitate the
identification of patients requiring urgent treatment. The proposed framework
comprises three modules: a 3D image feature extractor, clinical variable
selection, and survival prediction. The feature extractor module, based on the
3D CNN architecture, predicts the ISUP grade of renal cell carcinoma tumors
linked to mortality rates from CT images. A selection of clinical variables is
systematically chosen using the Spearman score and random forest importance
score as criteria. A deep learning-based network, trained with discrete
LogisticHazard-based loss, performs the survival prediction. Nine distinct
experiments are performed, with varying numbers of clinical variables
determined by different thresholds of the Spearman and importance scores. Our
findings demonstrate that the proposed strategy surpasses the current
literature on renal cancer prognosis based on CT scans and clinical factors.
The best-performing experiment yielded a concordance index of 0.84 and an area
under the curve value of 0.8 on the test cohort, which suggests strong
predictive power. The multimodal deep-learning approach developed in this study
shows promising results in estimating survival probabilities for renal cell
carcinoma patients using CT imaging and clinical data. This may have potential
implications in identifying patients who require urgent treatment, potentially
improving patient outcomes. The code created for this project is available for
the public on:
\href{https://github.com/Balasingham-AI-Group/Survival_CTplusClinical}{GitHub}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Batteries-Included Zero-Shot ESCO Skills
  Matchers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Clavié, Guillaume Soulié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding labour market dynamics requires accurately identifying the
skills required for and possessed by the workforce. Automation techniques are
increasingly being developed to support this effort. However, automatically
extracting skills from job postings is challenging due to the vast number of
existing skills. The ESCO (European Skills, Competences, Qualifications and
Occupations) framework provides a useful reference, listing over 13,000
individual skills. However, skills extraction remains difficult and accurately
matching job posts to the ESCO taxonomy is an open problem. In this work, we
propose an end-to-end zero-shot system for skills extraction from job
descriptions based on large language models (LLMs). We generate synthetic
training data for the entirety of ESCO skills and train a classifier to extract
skill mentions from job posts. We also employ a similarity retriever to
generate skill candidates which are then re-ranked using a second LLM. Using
synthetic data achieves an RP@10 score 10 points higher than previous distant
supervision approaches. Adding GPT-4 re-ranking improves RP@10 by over 22
points over previous methods. We also show that Framing the task as mock
programming when prompting the LLM can lead to better performance than natural
language prompts, especially with weaker LLMs. We demonstrate the potential of
integrating large language models at both ends of skills matching pipelines.
Our approach requires no human annotations and achieve extremely promising
results on skills extraction against ESCO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tranfer Learning of Semantic Segmentation Methods for Identifying Buried
  Archaeological Structures on LiDAR Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Soleni, Wouter B. Verschoof-van der Vaart, Žiga Kokalj, Arianna Traviglia, Marco Fiorucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Geoscience and Remote Sensing
  Symposium 2023 (IGARSS 2023) @IEEE copyright</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivative Free Weight-space Ensembling <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dean Ninalga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work suggests that interpolating between the weights of two
specialized language models can transfer knowledge between tasks in a way that
multi-task learning cannot. However, very few have explored interpolation
between more than two models, where each has a distinct knowledge base. In this
paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new
few-sample task transfer approach for open-domain dialogue. Our framework
creates a set of diverse expert language models trained using a predefined set
of source tasks. Next, we finetune each of the expert models on the target
task, approaching the target task from several distinct knowledge bases.
Finally, we linearly interpolate between the model weights using a
gradient-free-optimization algorithm, to efficiently find a good interpolation
weighting. We demonstrate the effectiveness of the method on FETA-Friends
outperforming the standard pretrain-finetune approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For consideration at the 5th Workshop on NLP for Conversational AI
  (co-located with ACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCDN -- Robust X-Corner Detection Algorithm based on Advanced CNN Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Chen, Caihua Xiong, Quanlin Li, Zhonghua Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate detection and localization of X-corner on both planar and non-planar
patterns is a core step in robotics and machine vision. However, previous works
could not make a good balance between accuracy and robustness, which are both
crucial criteria to evaluate the detectors performance. To address this
problem, in this paper we present a novel detection algorithm which can
maintain high sub-pixel precision on inputs under multiple interference, such
as lens distortion, extreme poses and noise. The whole algorithm, adopting a
coarse-to-fine strategy, contains a X-corner detection network and three
post-processing techniques to distinguish the correct corner candidates, as
well as a mixed sub-pixel refinement technique and an improved region growth
strategy to recover the checkerboard pattern partially visible or occluded
automatically. Evaluations on real and synthetic images indicate that the
presented algorithm has the higher detection rate, sub-pixel accuracy and
robustness than other commonly used methods. Finally, experiments of camera
calibration and pose estimation verify it can also get smaller re-projection
error in quantitative comparisons to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures and 4 tables. Unpublished further research and
  experiments of Checkerboard corner detection network CCDN (arXiv:2302.05097)
  and application exploration for robust camera calibration
  (https://ieeexplore.ieee.org/abstract/document/9428389)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large AI Model-Based Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communication (SC) is an emerging intelligent paradigm, offering
solutions for various future applications like metaverse, mixed-reality, and
the Internet of everything. However, in current SC systems, the construction of
the knowledge base (KB) faces several issues, including limited knowledge
representation, frequent knowledge updates, and insecure knowledge sharing.
Fortunately, the development of the large AI model provides new solutions to
overcome above issues. Here, we propose a large AI model-based SC framework
(LAM-SC) specifically designed for image data, where we first design the
segment anything model (SAM)-based KB (SKB) that can split the original image
into different semantic segments by universal semantic knowledge. Then, we
present an attention-based semantic integration (ASI) to weigh the semantic
segments generated by SKB without human participation and integrate them as the
semantic-aware image. Additionally, we propose an adaptive semantic compression
(ASC) encoding to remove redundant information in semantic features, thereby
reducing communication overhead. Finally, through simulations, we demonstrate
the effectiveness of the LAM-SC framework and the significance of the large AI
model-based KB development in future SC paradigms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Plan to submit it to journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Hierarchical Achievements in Reinforcement Learning via
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungyong Moon, Junyoung Yeom, Bumsoo Park, Hyun Oh Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering achievements with a hierarchical structure on procedurally
generated environments poses a significant challenge. This requires agents to
possess a broad range of abilities, including generalization and long-term
reasoning. Many prior methods are built upon model-based or hierarchical
approaches, with the belief that an explicit module for long-term planning
would be beneficial for learning hierarchical achievements. However, these
methods require an excessive amount of environment interactions or large model
sizes, limiting their practicality. In this work, we identify that proximal
policy optimization (PPO), a simple and versatile model-free algorithm,
outperforms the prior methods with recent implementation practices. Moreover,
we find that the PPO agent can predict the next achievement to be unlocked to
some extent, though with low confidence. Based on this observation, we propose
a novel contrastive learning method, called achievement distillation, that
strengthens the agent's capability to predict the next achievement. Our method
exhibits a strong capacity for discovering hierarchical achievements and shows
state-of-the-art performance on the challenging Crafter environment using fewer
model parameters in a sample-efficient regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task
  Foundation Model Learning <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelun Zhang, Xue Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The AllInOne training paradigm squeezes a wide range of tasks into a unified
model in a multi-task learning manner. However, optimization in multi-task
learning is more challenge than single-task learning, as the gradient norm from
different tasks may vary greatly, making the backbone overly biased towards one
specific task. To address this issue, we propose the task-level
backbone-oriented gradient clip paradigm, compared with the vanilla gradient
clip method, it has two points of emphasis:1) gradient clip is performed
independently for each task. 2) backbone gradients generated from each task are
rescaled to the same norm scale. Based on the experimental results, we argue
that the task-level backbone-oriented gradient clip paradigm can relieve the
gradient bias problem to some extent. We also propose a novel multi-branch data
augmentation strategy where conflict augmentations are placed in different
branches. Our approach has been shown to be effective and finally achieve 1st
place in the Leaderboard A and 2nd place in the Leaderboard B of the CVPR2023
Foundation Model Challenge. It's worth noting that instead of evaluating all
three tasks(detection, segmentation and fine-grained classification) in
Leaderboard A, the segmentation task is not evaluated in Leaderboard B, in
which our team has a huge advantage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Foundation Model Challenge@CVPR2023, Accepted by CVPR2023 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Deep Network Steganography: From Networks to Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guobiao Li, Sheng Li, Meiling Li, Zhenxing Qian, Xinpeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread applications of the deep neural network (DNN), how to
covertly transmit the DNN models in public channels brings us the attention,
especially for those trained for secret-learning tasks. In this paper, we
propose deep network steganography for the covert communication of DNN models.
Unlike the existing steganography schemes which focus on the subtle
modification of the cover data to accommodate the secrets, our scheme is
learning task oriented, where the learning task of the secret DNN model (termed
as secret-learning task) is disguised into another ordinary learning task
conducted in a stego DNN model (termed as stego-learning task). To this end, we
propose a gradient-based filter insertion scheme to insert interference filters
into the important positions in the secret DNN model to form a stego DNN model.
These positions are then embedded into the stego DNN model using a key by side
information hiding. Finally, we activate the interference filters by a partial
optimization strategy, such that the generated stego DNN model works on the
stego-learning task. We conduct the experiments on both the intra-task
steganography and inter-task steganography (i.e., the secret and stego-learning
tasks belong to the same and different categories), both of which demonstrate
the effectiveness of our proposed method for covert communication of DNN
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. arXiv admin note: text overlap with arXiv:2302.14521</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-iterative Coarse-to-fine <span class="highlight-title">Transformer</span> Networks for Joint Affine and
  Deformable Image Registration <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, Jinman Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image registration is a fundamental requirement for medical image analysis.
Deep registration methods based on deep learning have been widely recognized
for their capabilities to perform fast end-to-end registration. Many deep
registration methods achieved state-of-the-art performance by performing
coarse-to-fine registration, where multiple registration steps were iterated
with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE)
registration methods have been proposed to perform coarse-to-fine registration
in a single network and showed advantages in both registration accuracy and
runtime. However, existing NICE registration methods mainly focus on deformable
registration, while affine registration, a common prerequisite, is still
reliant on time-consuming traditional optimization-based methods or extra
affine registration networks. In addition, existing NICE registration methods
are limited by the intrinsic locality of convolution operations. Transformers
may address this limitation for their capabilities to capture long-range
dependency, but the benefits of using transformers for NICE registration have
not been explored. In this study, we propose a Non-Iterative Coarse-to-finE
Transformer network (NICE-Trans) for image registration. Our NICE-Trans is the
first deep registration method that (i) performs joint affine and deformable
coarse-to-fine registration within a single network, and (ii) embeds
transformers into a NICE registration framework to model long-range relevance
between images. Extensive experiments with seven public datasets show that our
NICE-Trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Medical Image Computing and
  Computer Assisted Intervention (MICCAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QI2 -- an Interactive Tool for Data Quality Assurance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Geerkens, Christian Sieberichs, Alexander Braun, Thomas Waschulzik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of high data quality is increasing with the growing impact and
distribution of ML systems and big data. Also the planned AI Act from the
European commission defines challenging legal requirements for data quality
especially for the market introduction of safety relevant ML systems. In this
paper we introduce a novel approach that supports the data quality assurance
process of multiple data quality aspects. This approach enables the
verification of quantitative data quality requirements. The concept and
benefits are introduced and explained on small example data sets. How the
method is applied is demonstrated on the well known MNIST data set based an
handwritten digits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Conditioned Predictive Coding as an Implicit Planner for Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilai Zeng, Ce Zhang, Shijie Wang, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has demonstrated the effectiveness of formulating decision making
as a supervised learning problem on offline-collected trajectories. However,
the benefits of performing sequence modeling on trajectory data is not yet
clear. In this work we investigate if sequence modeling has the capability to
condense trajectories into useful representations that can contribute to policy
learning. To achieve this, we adopt a two-stage framework that first summarizes
trajectories with sequence modeling techniques, and then employs these
representations to learn a policy along with a desired goal. This design allows
many existing supervised offline RL methods to be considered as specific
instances of our framework. Within this framework, we introduce
Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful
trajectory representations and leads to performant policies. We conduct
extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion
environments, and observe that sequence modeling has a significant impact on
some decision making tasks. In addition, we demonstrate that GCPC learns a
goal-conditioned latent representation about the future, which serves as an
"implicit planner", and enables competitive performance on all three
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potential of Large Language Models (LLMs) in Learning on
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Formal Feature Attribution and Its Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqiang Yu, Alexey Ignatiev, Peter J. Stuckey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the widespread use of artificial intelligence
(AI) algorithms and machine learning (ML) models. Despite their tremendous
success, a number of vital problems like ML model brittleness, their fairness,
and the lack of interpretability warrant the need for the active developments
in explainable artificial intelligence (XAI) and formal ML model verification.
The two major lines of work in XAI include feature selection methods, e.g.
Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their
promise, most of the existing feature selection and attribution approaches are
susceptible to a range of critical issues, including explanation unsoundness
and out-of-distribution sampling. A recent formal approach to XAI (FXAI)
although serving as an alternative to the above and free of these issues
suffers from a few other limitations. For instance and besides the scalability
limitation, the formal approach is unable to tackle the feature attribution
problem. Additionally, a formal explanation despite being formally sound is
typically quite large, which hampers its applicability in practical settings.
Motivated by the above, this paper proposes a way to apply the apparatus of
formal XAI to the case of feature attribution based on formal explanation
enumeration. Formal feature attribution (FFA) is argued to be advantageous over
the existing methods, both formal and non-formal. Given the practical
complexity of the problem, the paper then proposes an efficient technique for
approximating exact FFA. Finally, it offers experimental evidence of the
effectiveness of the proposed approximate FFA in comparison to the existing
feature attribution algorithms not only in terms of feature importance and but
also in terms of their relative order.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Ground Vehicle Path Following in Game AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigue de Schaetzen, Alessandro Sestini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This short paper presents an efficient path following solution for ground
vehicles tailored to game AI. Our focus is on adapting established techniques
to design simple solutions with parameters that are easily tunable for an
efficient benchmark path follower. Our solution pays particular attention to
computing a target speed which uses quadratic Bezier curves to estimate the
path curvature. The performance of the proposed path follower is evaluated
through a variety of test scenarios in a first-person shooter game,
demonstrating its effectiveness and robustness in handling different types of
paths and vehicles. We achieved a 70% decrease in the total number of stuck
events compared to an existing path following solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, to be published in IEEE Conference on Games 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All in One: Exploring Unified Vision-Language Tracking with Multi-Modal
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Zhang, Xin Sun, Li Liu, Yiqian Yang, Qiong Liu, Xi Zhou, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current mainstream vision-language (VL) tracking framework consists of three
parts, \ie a visual feature extractor, a language feature extractor, and a
fusion model. To pursue better performance, a natural modus operandi for VL
tracking is employing customized and heavier unimodal encoders, and multi-modal
fusion models. Albeit effective, existing VL trackers separate feature
extraction and feature integration, resulting in extracted features that lack
semantic guidance and have limited target-aware capability in complex
scenarios, \eg similar distractors and extreme illumination. In this work,
inspired by the recent success of exploring foundation models with unified
architecture for both natural language and computer vision tasks, we propose an
All-in-One framework, which learns joint feature extraction and interaction by
adopting a unified transformer backbone. Specifically, we mix raw vision and
language signals to generate language-injected vision tokens, which we then
concatenate before feeding into the unified backbone architecture. This
approach achieves feature integration in a unified backbone, removing the need
for carefully-designed fusion modules and resulting in a more effective and
efficient VL tracking framework. To further improve the learning efficiency, we
introduce a multi-modal alignment module based on cross-modal and intra-modal
contrastive objectives, providing more reasonable representations for the
unified All-in-One transformer backbone. Extensive experiments on five
benchmarks, \ie OTB99-L, TNL2K, LaSOT, LaSOT$_{\rm Ext}$ and WebUAV-3M,
demonstrate the superiority of the proposed tracker against existing
state-of-the-arts on VL tracking. Codes will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptation and Communication in Human-Robot Teaming to Handle
  Discrepancies in Agents' Beliefs about Plans <span class="chip">ICAPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuening Zhang, Brian C. Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When agents collaborate on a task, it is important that they have some shared
mental model of the task routines -- the set of feasible plans towards
achieving the goals. However, in reality, situations often arise that such a
shared mental model cannot be guaranteed, such as in ad-hoc teams where agents
may follow different conventions or when contingent constraints arise that only
some agents are aware of. Previous work on human-robot teaming has assumed that
the team has a set of shared routines, which breaks down in these situations.
In this work, we leverage epistemic logic to enable agents to understand the
discrepancy in each other's beliefs about feasible plans and dynamically plan
their actions to adapt or communicate to resolve the discrepancy. We propose a
formalism that extends conditional doxastic logic to describe knowledge bases
in order to explicitly represent agents' nested beliefs on the feasible plans
and state of execution. We provide an online execution algorithm based on Monte
Carlo Tree Search for the agent to plan its action, including communication
actions to explain the feasibility of plans, announce intent, and ask
questions. Finally, we evaluate the success rate and scalability of the
algorithm and show that our agent is better equipped to work in teams without
the guarantee of a shared mental model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Published at ICAPS 2023 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Biased Attitude Associations of Language Models in an
  Intersectional Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiva Omrani Sabbaghi, Robert Wolfe, Aylin Caliskan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are trained on large-scale corpora that embed implicit biases
documented in psychology. Valence associations (pleasantness/unpleasantness) of
social groups determine the biased attitudes towards groups and concepts in
social cognition. Building on this established literature, we quantify how
social groups are valenced in English language models using a sentence template
that provides an intersectional context. We study biases related to age,
education, gender, height, intelligence, literacy, race, religion, sex, sexual
orientation, social class, and weight. We present a concept projection approach
to capture the valence subspace through contextualized word embeddings of
language models. Adapting the projection-based approach to embedding
association tests that quantify bias, we find that language models exhibit the
most biased attitudes against gender identity, social class, and sexual
orientation signals in language. We find that the largest and better-performing
model that we study is also more biased as it effectively captures bias
embedded in sociocultural data. We validate the bias evaluation method by
overperforming on an intrinsic valence evaluation task. The approach enables us
to measure complex intersectional biases as they are known to manifest in the
outputs and applications of language models that perpetuate historical biases.
Moreover, our approach contributes to design justice as it studies the
associations of groups underrepresented in language such as transgender and
homosexual individuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in AIES 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement and Deep Reinforcement Learning-based Solutions for
  Machine Maintenance Planning, Scheduling Policies, and Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oluwaseyi Ogunfowora, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systems and machines undergo various failure modes that result in machine
health degradation, so maintenance actions are required to restore them back to
a state where they can perform their expected functions. Since maintenance
tasks are inevitable, maintenance planning is essential to ensure the smooth
operations of the production system and other industries at large. Maintenance
planning is a decision-making problem that aims at developing optimum
maintenance policies and plans that help reduces maintenance costs, extend
asset life, maximize their availability, and ultimately ensure workplace
safety. Reinforcement learning is a data-driven decision-making algorithm that
has been increasingly applied to develop dynamic maintenance plans while
leveraging the continuous information from condition monitoring of the system
and machine states. By leveraging the condition monitoring data of systems and
machines with reinforcement learning, smart maintenance planners can be
developed, which is a precursor to achieving a smart factory. This paper
presents a literature review on the applications of reinforcement and deep
reinforcement learning for maintenance planning and optimization problems. To
capture the common ideas without losing touch with the uniqueness of each
publication, taxonomies used to categorize the systems were developed, and
reviewed publications were highlighted, classified, and summarized based on
these taxonomies. Adopted methodologies, findings, and well-defined
interpretations of the reviewed studies were summarized in graphical and
tabular representations to maximize the utility of the work for both
researchers and practitioners. This work also highlights the research gaps, key
insights from the literature, and areas for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages with references, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teach Me How to Learn: A Perspective <span class="highlight-title">Review</span> towards User-centered
  Neuro-symbolic Learning for Robotic Surgical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amr Gomaa, Bilal Mahdy, Niko Kleer, Michael Feld, Frank Kirchner, Antonio Krüger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in machine learning models allowed robots to identify objects
on a perceptual nonsymbolic level (e.g., through sensor fusion and natural
language understanding). However, these primarily black-box learning models
still lack interpretation and transferability and require high data and
computational demand. An alternative solution is to teach a robot on both
perceptual nonsymbolic and conceptual symbolic levels through hybrid
neurosymbolic learning approaches with expert feedback (i.e., human-in-the-loop
learning). This work proposes a concept for this user-centered hybrid learning
paradigm that focuses on robotic surgical situations. While most recent
research focused on hybrid learning for non-robotic and some generic robotic
domains, little work focuses on surgical robotics. We survey this related
research while focusing on human-in-the-loop surgical robotic systems. This
evaluation highlights the most prominent solutions for autonomous surgical
robots and the challenges surgeons face when interacting with these systems.
Finally, we envision possible ways to address these challenges using online
apprenticeship learning based on implicit and explicit feedback from expert
surgeons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Learners for Realizable Regression: PAC Learning and Online
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, Grigoris Velegkas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we aim to characterize the statistical complexity of realizable
regression both in the PAC learning setting and the online learning setting.
  Previous work had established the sufficiency of finiteness of the fat
shattering dimension for PAC learnability and the necessity of finiteness of
the scaled Natarajan dimension, but little progress had been made towards a
more complete characterization since the work of Simon 1997 (SICOMP '97). To
this end, we first introduce a minimax instance optimal learner for realizable
regression and propose a novel dimension that both qualitatively and
quantitatively characterizes which classes of real-valued predictors are
learnable. We then identify a combinatorial dimension related to the Graph
dimension that characterizes ERM learnability in the realizable setting.
Finally, we establish a necessary condition for learnability based on a
combinatorial dimension related to the DS dimension, and conjecture that it may
also be sufficient in this context.
  Additionally, in the context of online learning we provide a dimension that
characterizes the minimax instance optimal cumulative loss up to a constant
factor and design an optimal online learner for realizable regression, thus
resolving an open question raised by Daskalakis and Golowich in STOC '22.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RADAR: Robust AI-Text Detection via Adversarial Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) and the intensifying
popularity of ChatGPT-like applications have blurred the boundary of
high-quality text generation between humans and machines. However, in addition
to the anticipated revolutionary changes to our technology and society, the
difficulty of distinguishing LLM-generated texts (AI-text) from human-generated
texts poses new challenges of misuse and fairness, such as fake content
generation, plagiarism, and false accusation of innocent writers. While
existing works show that current AI-text detectors are not robust to LLM-based
paraphrasing, this paper aims to bridge this gap by proposing a new framework
called RADAR, which jointly trains a Robust AI-text Detector via Adversarial
leaRning. RADAR is based on adversarial training of a paraphraser and a
detector. The paraphraser's goal is to generate realistic contents to evade
AI-text detection. RADAR uses the feedback from the detector to update the
paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly
2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,
experimental results show that RADAR significantly outperforms existing AI-text
detection methods, especially when paraphrasing is in place. We also identify
the strong transferability of RADAR from instruction-tuned LLMs to other LLMs,
and evaluate the improved capability of RADAR via GPT-3.5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Project page and demos: https://radar.vizhub.ai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, Jenq-Neng Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based methods have dominated the 3D human pose estimation (HPE)
tasks with significantly better performance in most benchmarks than traditional
optimization-based methods. Nonetheless, 3D HPE in the wild is still the
biggest challenge of learning-based models, whether with 2D-3D lifting,
image-to-3D, or diffusion-based methods, since the trained networks implicitly
learn camera intrinsic parameters and domain-based 3D human pose distributions
and estimate poses by statistical average. On the other hand, the
optimization-based methods estimate results case-by-case, which can predict
more diverse and sophisticated human poses in the wild. By combining the
advantages of optimization-based and learning-based methods, we propose the
Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the
problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO
achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mm
without training with any 2D-3D or image-3D pairs. Moreover, our
single-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE
$42.6$mm on cross-dataset evaluation, which even outperforms learning-based
methods trained on 3DPW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effect of Intensity Standardization on Deep Learning for WML
  Segmentation in Multi-Centre FLAIR MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdollah Ghazvanchahi, Pejman Jahbedar Maralani, Alan R. Moody, April Khademi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI
suffer a reduction in performance when applied on data from a scanner or centre
that is out-of-distribution (OOD) from the training data. This is critical for
translation and widescale adoption, since current models cannot be readily
applied to data from new institutions. In this work, we evaluate several
intensity standardization methods for MRI as a preprocessing step for WML
segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI.
We evaluate a method specifically developed for FLAIR MRI called IAMLAB along
with other popular normalization techniques such as White-strip, Nyul and
Z-score. We proposed an Ensemble model that combines predictions from each of
these models. A skip-connection UNet (SC UNet) was trained on the standardized
images, as well as the original data and segmentation performance was evaluated
over several dimensions. The training (in-distribution) data consists of a
single study, of 60 volumes, and the test (OOD) data is 128 unseen volumes from
three clinical cohorts. Results show IAMLAB and Ensemble provide higher WML
segmentation performance compared to models from original data or other
normalization methods. IAMLAB & Ensemble have the highest dice similarity
coefficient (DSC) on the in-distribution data (0.78 & 0.80) and on clinical OOD
data. DSC was significantly higher for IAMLAB compared to the original data
(p<0.05) for all lesion categories (LL>25mL: 0.77 vs. 0.71; 10mL<= LL<25mL:
0.66 vs. 0.61; LL<10mL: 0.53 vs. 0.52). The IAMLAB and Ensemble normalization
methods are mitigating MRI domain shift and are optimal for DL-based WML
segmentation in unseen FLAIR data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How does AI chat change search behaviors? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Capra, Jaime Arguello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI tools such as chatGPT are poised to change the way people
engage with online information. Recently, Microsoft announced their "new Bing"
search system which incorporates chat and generative AI technology from OpenAI.
Google has announced plans to deploy search interfaces that incorporate similar
types of technology. These new technologies will transform how people can
search for information. The research presented here is an early investigation
into how people make use of a generative AI chat system (referred to simply as
chat from here on) as part of a search process, and how the incorporation of
chat systems with existing search tools may effect users search behaviors and
strategies.
  We report on an exploratory user study with 10 participants who used a
combined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing
Web Search v5 API. Participants completed three search tasks. In this pre-print
paper of preliminary results, we report on ways that users integrated AI chat
into their search process, things they liked and disliked about the chat
system, their trust in the chat responses, and their mental models of how the
chat system generated responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring and Characterizing Large Language Models For Embedded System
  Development and Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Englhardt, Richard Li, Dilini Nissanka, Zhihan Zhang, Girish Narayanswamy, Joseph Breda, Xin Liu, Shwetak Patel, Vikram Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable abilities to generate
code, however their ability to develop software for embedded systems, which
requires cross-domain knowledge of hardware and software has not been studied.
In this paper we systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2)
to assess their performance for embedded system development, study how human
programmers interact with these tools, and develop an AI-based software
engineering workflow for building embedded systems.
  We develop an an end-to-end hardware-in-the-loop evaluation platform for
verifying LLM generated programs using sensor actuator pairs. We compare all
three models with N=450 experiments and find surprisingly that GPT-4 especially
shows an exceptional level of cross-domain understanding and reasoning, in some
cases generating fully correct programs from a single prompt. In N=50 trials,
GPT-4 produces functional I2C interfaces 66% of the time. GPT-4 also produces
register-level drivers, code for LoRa communication, and context-specific power
optimizations for an nRF52 program resulting in over 740x current reduction to
12.2 uA. We also characterize the models' limitations to develop a
generalizable workflow for using LLMs in embedded system development. We
evaluate the workflow with 15 users including novice and expert programmers. We
find that our workflow improves productivity for all users and increases the
success rate for building a LoRa environmental sensor from 25% to 100%,
including for users with zero hardware or C/C++ experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis
  of a Watershed Moment in Iran's Gender Struggles <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adel Khorramrouz, Sujan Dutta, Ashiqur R. KhudaBukhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a computational analysis of the Persian language
Twitter discourse with the aim to estimate the shift in stance toward gender
equality following the death of Mahsa Amini in police custody. We present an
ensemble active learning pipeline to train a stance classifier. Our novelty
lies in the involvement of Iranian women in an active role as annotators in
building this AI system. Our annotators not only provide labels, but they also
suggest valuable keywords for more meaningful corpus creation as well as
provide short example documents for a guided sampling step. Our analyses
indicate that Mahsa Amini's death triggered polarized Persian language
discourse where both fractions of negative and positive tweets toward gender
equality increased. The increase in positive tweets was slightly greater than
the increase in negative tweets. We also observe that with respect to account
creation time, between the state-aligned Twitter accounts and pro-protest
Twitter accounts, pro-protest accounts are more similar to baseline Persian
Twitter activity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IJCAI 2023 (AI for good track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ URL: A Representation Learning Benchmark for Transferable Uncertainty
  Estimates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Kirchhof, Bálint Mucsányi, Seong Joon Oh, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning has significantly driven the field to develop
pretrained models that can act as a valuable starting point when transferring
to new datasets. With the rising demand for reliable machine learning and
uncertainty quantification, there is a need for pretrained models that not only
provide embeddings but also transferable uncertainty estimates. To guide the
development of such models, we propose the Uncertainty-aware Representation
Learning (URL) benchmark. Besides the transferability of the representations,
it also measures the zero-shot transferability of the uncertainty estimate
using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers
that are pretrained on ImageNet and transferred to eight downstream datasets.
We find that approaches that focus on the uncertainty of the representation
itself or estimate the prediction risk directly outperform those that are based
on the probabilities of upstream classes. Yet, achieving transferable
uncertainty quantification remains an open challenge. Our findings indicate
that it is not necessarily in conflict with traditional representation learning
goals. Code is provided under https://github.com/mkirchhof/url .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Should Data Science Education Do with Large Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinming Tu, James Zou, Weijie J. Su, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advances of large language models (LLMs), such as ChatGPT, are
revolutionizing data science and statistics. These state-of-the-art tools can
streamline complex processes. As a result, it reshapes the role of data
scientists. We argue that LLMs are transforming the responsibilities of data
scientists, shifting their focus from hands-on coding, data-wrangling and
conducting standard analyses to assessing and managing analyses performed by
these automated AIs. This evolution of roles is reminiscent of the transition
from a software engineer to a product manager. We illustrate this transition
with concrete data science case studies using LLMs in this paper. These
developments necessitate a meaningful evolution in data science education.
Pedagogy must now place greater emphasis on cultivating diverse skillsets among
students, such as LLM-informed creativity, critical thinking, AI-guided
programming. LLMs can also play a significant role in the classroom as
interactive teaching and learning tools, contributing to personalized
education. This paper discusses the opportunities, resources and open
challenges for each of these directions. As with any transformative technology,
integrating LLMs into education calls for careful consideration. While LLMs can
perform repetitive tasks efficiently, it's crucial to remember that their role
is to supplement human intelligence and creativity, not to replace it.
Therefore, the new era of data science education should balance the benefits of
LLMs while fostering complementary human expertise and innovations. In
conclusion, the rise of LLMs heralds a transformative period for data science
and its education. This paper seeks to shed light on the emerging trends,
potential opportunities, and challenges accompanying this paradigm shift,
hoping to spark further discourse and investigation into this exciting,
uncharted territory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with
  Robotic and Human Co-Workers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11498v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11498v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandar Krnjaic, Raul D. Steleac, Jonathan D. Thomas, Georgios Papoudakis, Lukas Schäfer, Andrew Wing Keung To, Kuan-Ho Lao, Murat Cubuktepe, Matthew Haley, Peter Börsting, Stefano V. Albrecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We envision a warehouse in which dozens of mobile robots and human pickers
work together to collect and deliver items within the warehouse. The
fundamental problem we tackle, called the order-picking problem, is how these
worker agents must coordinate their movement and actions in the warehouse to
maximise performance (e.g. order throughput). Established industry methods
using heuristic approaches require large engineering efforts to optimise for
innately variable warehouse configurations. In contrast, multi-agent
reinforcement learning (MARL) can be flexibly applied to diverse warehouse
configurations (e.g. size, layout, number/types of workers, item replenishment
frequency), as the agents learn through experience how to optimally cooperate
with one another. We develop hierarchical MARL algorithms in which a manager
assigns goals to worker agents, and the policies of the manager and workers are
co-trained toward maximising a global objective (e.g. pick rate). Our
hierarchical algorithms achieve significant gains in sample efficiency and
overall pick rates over baseline MARL algorithms in diverse warehouse
configurations, and substantially outperform two established industry
heuristics for order-picking systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creativity of AI: Hierarchical Planning Model Learning for Facilitating
  Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hankz Hankui Zhuo, Shuting Deng, Mu Jin, Zhihao Ma, Kebing Jin, Chen Chen, Chao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite of achieving great success in real-world applications, Deep
Reinforcement Learning (DRL) is still suffering from three critical issues,
i.e., data efficiency, lack of the interpretability and transferability. Recent
research shows that embedding symbolic knowledge into DRL is promising in
addressing those challenges. Inspired by this, we introduce a novel deep
reinforcement learning framework with symbolic options. Our framework features
a loop training procedure, which enables guiding the improvement of policy by
planning with planning models (including action models and hierarchical task
network models) and symbolic options learned from interactive trajectories
automatically. The learned symbolic options alleviate the dense requirement of
expert domain knowledge and provide inherent interpretability of policies.
Moreover, the transferability and data efficiency can be further improved by
planning with the symbolic planning models. To validate the effectiveness of
our framework, we conduct experiments on two domains, Montezuma's Revenge and
Office World, respectively. The results demonstrate the comparable performance,
improved data efficiency, interpretability and transferability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Equivariance with Learned Canonicalization Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06489v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06489v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sékou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang, <span class="highlight-author">Yoshua Bengio</span>, Siamak Ravanbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetry-based neural networks often constrain the architecture in order to
achieve invariance or equivariance to a group of transformations. In this
paper, we propose an alternative that avoids this architectural constraint by
learning to produce canonical representations of the data. These
canonicalization functions can readily be plugged into non-equivariant backbone
architectures. We offer explicit ways to implement them for some groups of
interest. We show that this approach enjoys universality while providing
interpretable insights. Our main hypothesis, supported by our empirical
results, is that learning a small neural network to perform canonicalization is
better than using predefined heuristics. Our experiments show that learning the
canonicalization function is competitive with existing techniques for learning
equivariant functions across many tasks, including image classification,
$N$-body dynamics prediction, point cloud classification and part segmentation,
while being faster across the board.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topical: Learning Repository Embeddings from Source Code using Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agathe Lherondelle, Varun Babbar, Yash Satsangi, Fran Silavong, Shaltiel Eloul, Sean Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning on source code (MLOnCode) promises to transform how software
is delivered. By mining the context and relationship between software
artefacts, MLOnCode augments the software developers capabilities with code
auto-generation, code recommendation, code auto-tagging and other data-driven
enhancements. For many of these tasks a script level representation of code is
sufficient, however, in many cases a repository level representation that takes
into account various dependencies and repository structure is imperative, for
example, auto-tagging repositories with topics or auto-documentation of
repository code etc. Existing methods for computing repository level
representations suffer from (a) reliance on natural language documentation of
code (for example, README files) (b) naive aggregation of method/script-level
representation, for example, by concatenation or averaging. This paper
introduces Topical a deep neural network to generate repository level
embeddings of publicly available GitHub code repositories directly from source
code. Topical incorporates an attention mechanism that projects the source
code, the full dependency graph and the script level textual information into a
dense repository-level representation. To compute the repository-level
representations, Topical is trained to predict the topics associated with a
repository, on a dataset of publicly available GitHub repositories that were
crawled along with their ground truth topic tags. Our experiments show that the
embeddings computed by Topical are able to outperform multiple baselines,
including baselines that naively combine the method-level representations
through averaging or concatenation at the task of repository auto-tagging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FOCUS: Object-Centric World Models for Robotics Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the world in terms of objects and the possible interplays with
them is an important cognition ability, especially in robotics manipulation,
where many tasks require robot-object interactions. However, learning such a
structured world model, which specifically captures entities and relationships,
remains a challenging and underexplored problem. To address this, we propose
FOCUS, a model-based agent that learns an object-centric world model. Thanks to
a novel exploration bonus that stems from the object-centric representation,
FOCUS can be deployed on robotics manipulation tasks to explore object
interactions more easily. Evaluating our approach on manipulation tasks across
different settings, we show that object-centric world models allow the agent to
solve tasks more efficiently and enable consistent exploration of robot-object
interactions. Using a Franka Emika robot arm, we also showcase how FOCUS could
be adopted in real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRAPHSHAP: Explaining Identity-Aware Graph Classifiers Through the
  Language of Motifs <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Perotti, Paolo Bajardi, Francesco Bonchi, André Panisson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most methods for explaining black-box classifiers (e.g. on tabular data,
images, or time series) rely on measuring the impact that removing/perturbing
features has on the model output. This forces the explanation language to match
the classifier's feature space. However, when dealing with graph data, in which
the basic features correspond to the edges describing the graph structure, this
matching between features space and explanation language might not be
appropriate. Decoupling the feature space (edges) from a desired high-level
explanation language (such as motifs) is thus a major challenge towards
developing actionable explanations for graph classification tasks. In this
paper we introduce GRAPHSHAP, a Shapley-based approach able to provide
motif-based explanations for identity-aware graph classifiers, assuming no
knowledge whatsoever about the model or its training data: the only requirement
is that the classifier can be queried as a black-box at will. For the sake of
computational efficiency we explore a progressive approximation strategy and
show how a simple kernel can efficiently approximate explanation scores, thus
allowing GRAPHSHAP to scale on scenarios with a large explanation space (i.e.
large number of motifs). We showcase GRAPHSHAP on a real-world brain-network
dataset consisting of patients affected by Autism Spectrum Disorder and a
control group. Our experiments highlight how the classification provided by a
black-box model can be effectively explained by few connectomics patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Joint Conference on Neural Networks 2023
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Innovation Paradox: Concept Space Expansion with Diminishing
  Originality and the Promise of Creative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serhad Sarica, Jianxi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Innovation, typically spurred by reusing, recombining, and synthesizing
existing concepts, is expected to result in an exponential growth of the
concept space over time. However, our statistical analysis of TechNet, which is
a comprehensive technology semantic network encompassing over four million
concepts derived from patent texts, reveals a linear rather than exponential
expansion of the overall technological concept space. Moreover, there is a
notable decline in the originality of newly created concepts. These trends can
be attributed to the constraints of human cognitive abilities to innovate
beyond an ever-growing space of prior art, among other factors. Integrating
creative artificial intelligence into the innovation process holds the
potential to overcome these limitations and alter the observed trends in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Design Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-efficient NLLB-200: Language-specific Expert Pruning of a
  Massively Multilingual Machine Translation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09811v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09811v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeskendir Koishekenov, Alexandre Berard, Vassilina Nikoulina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently released NLLB-200 is a set of multilingual Neural Machine
Translation models that cover 202 languages. The largest model is based on a
Mixture of Experts architecture and achieves SoTA results across many language
pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just
for inference. In this work, we propose a pruning method that enables the
removal of up to 80% of experts without further finetuning and with a
negligible loss in translation quality, which makes it feasible to run the
model on a single 32GB GPU. Further analysis suggests that our pruning metrics
can identify language-specific experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline Prioritized Experience Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yue, Bingyi Kang, Xiao Ma, Gao Huang, Shiji Song, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) is challenged by the distributional shift
problem. To address this problem, existing works mainly focus on designing
sophisticated policy constraints between the learned policy and the behavior
policy. However, these constraints are applied equally to well-performing and
inferior actions through uniform sampling, which might negatively affect the
learned policy. To alleviate this issue, we propose Offline Prioritized
Experience Replay (OPER), featuring a class of priority functions designed to
prioritize highly-rewarding transitions, making them more frequently visited
during training. Through theoretical analysis, we show that this class of
priority functions induce an improved behavior policy, and when constrained to
this improved policy, a policy-constrained offline RL algorithm is likely to
yield a better solution. We develop two practical strategies to obtain priority
weights by estimating advantages based on a fitted value network (OPER-A) or
utilizing trajectory returns (OPER-R) for quick computation. OPER is a
plug-and-play component for offline RL algorithms. As case studies, we evaluate
OPER on five different algorithms, including BC, TD3+BC, Onestep RL, CQL, and
IQL. Extensive experiments demonstrate that both OPER-A and OPER-R
significantly improve the performance for all baseline methods. Codes and
priority weights are availiable at https://github.com/sail-sg/OPER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Adversarial Training via Reweighting Optimization Trajectory <span class="chip">ECML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14275v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14275v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjin Huang, Shiwei Liu, Tianlong Chen, Meng Fang, Li Shen, Vlaod Menkovski, Lu Yin, Yulong Pei, Mykola Pechenizkiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the fact that adversarial training has become the de facto method for
improving the robustness of deep neural networks, it is well-known that vanilla
adversarial training suffers from daunting robust overfitting, resulting in
unsatisfactory robust generalization. A number of approaches have been proposed
to address these drawbacks such as extra regularization, adversarial weights
perturbation, and training with more data over the last few years. However, the
robust generalization improvement is yet far from satisfactory. In this paper,
we approach this challenge with a brand new perspective -- refining historical
optimization trajectories. We propose a new method named \textbf{Weighted
Optimization Trajectories (WOT)} that leverages the optimization trajectories
of adversarial training in time. We have conducted extensive experiments to
demonstrate the effectiveness of WOT under various state-of-the-art adversarial
attacks. Our results show that WOT integrates seamlessly with the existing
adversarial training methods and consistently overcomes the robust overfitting
issue, resulting in better adversarial robustness. For example, WOT boosts the
robust accuracy of AT-PGD under AA-$L_{\infty}$ attack by 1.53\% $\sim$ 6.11\%
and meanwhile increases the clean accuracy by 0.55\%$\sim$5.47\% across SVHN,
CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Modality Imbalance In Multimodal Pedestrian Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arindam Das, Sudip Das, Ganesh Sistu, Jonathan Horgan, Ujjwal Bhattacharya, Edward Jones, Martin Glavin, Ciarán Eising
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning, particularly for pedestrian detection, has recently
received emphasis due to its capability to function equally well in several
critical autonomous driving scenarios such as low-light, night-time, and
adverse weather conditions. However, in most cases, the training distribution
largely emphasizes the contribution of one specific input that makes the
network biased towards one modality. Hence, the generalization of such models
becomes a significant problem where the non-dominant input modality during
training could be contributing more to the course of inference. Here, we
introduce a novel training setup with regularizer in the multimodal
architecture to resolve the problem of this disparity between the modalities.
Specifically, our regularizer term helps to make the feature fusion method more
robust by considering both the feature extractors equivalently important during
the training to extract the multimodal distribution which is referred to as
removing the imbalance problem. Furthermore, our decoupling concept of output
stream helps the detection task by sharing the spatial sensitive information
mutually. Extensive experiments of the proposed method on KAIST and UTokyo
datasets shows improvement of the respective state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figure, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Optimal Transport for Domain Adaptation on SPD Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05745v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05745v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Ju, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been significant interest in solving the domain
adaptation (DA) problem on symmetric positive definite (SPD) manifolds within
the machine learning community. This interest stems from the fact that complex
neurophysiological data generated by medical equipment, such as
electroencephalograms, magnetoencephalograms, and diffusion tensor imaging,
often exhibit a shift in data distribution across different domains. These data
representations, represented by signal covariance matrices, possess properties
of symmetry and positive definiteness. However, directly applying previous
experiences and solutions to the DA problem poses challenges due to the
manipulation complexities of covariance matrices.To address this, our research
introduces a category of deep learning-based transfer learning approaches
called deep optimal transport. This category utilizes optimal transport theory
and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we
present a comprehensive categorization of existing geometric methods to tackle
these problems effectively. This categorization provides practical solutions
for specific DA problems, including handling discrepancies in marginal and
conditional distributions between the source and target domains on the SPD
manifold. To evaluate the effectiveness, we conduct experiments on three
publicly available highly non-stationary cross-session brain-computer interface
scenarios. Moreover, we provide visualization results on the SPD cone to offer
further insights into the framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, and 4 tables; This work has been submitted to
  the IEEE for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Code-Switched Text Synthesis in Unseen Language Pairs <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        I-Hung Hsu, Avik Ray, Shubham Garg, Nanyun Peng, Jing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing efforts on text synthesis for code-switching mostly require training
on code-switched texts in the target language pairs, limiting the deployment of
the models to cases lacking code-switched data. In this work, we study the
problem of synthesizing code-switched texts for language pairs absent from the
training data. We introduce GLOSS, a model built on top of a pre-trained
multilingual machine translation model (PMMTM) with an additional
code-switching module. This module, either an adapter or extra prefixes, learns
code-switching patterns from code-switched data during training, while the
primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only
adjusting the code-switching module prevents our model from overfitting to the
constrained training data for code-switching. Hence, GLOSS exhibits the ability
to generalize and synthesize code-switched texts across a broader spectrum of
language pairs. Additionally, we develop a self-training algorithm on target
language pairs further to enhance the reliability of GLOSS. Automatic
evaluations on four language pairs show that GLOSS achieves at least 55%
relative BLEU and METEOR scores improvements compared to strong baselines.
Human evaluations on two language pairs further validate the success of GLOSS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by ACL2023 as a Finding paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Interleaving Semantics of the Timed Concurrent Language for
  Argumentation to Model Debates and Dialogue Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Bistarelli, Maria Chiara Meo, Carlo Taticchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time is a crucial factor in modelling dynamic behaviours of intelligent
agents: activities have a determined temporal duration in a real-world
environment, and previous actions influence agents' behaviour. In this paper,
we propose a language for modelling concurrent interaction between agents that
also allows the specification of temporal intervals in which particular actions
occur. Such a language exploits a timed version of Abstract Argumentation
Frameworks to realise a shared memory used by the agents to communicate and
reason on the acceptability of their beliefs with respect to a given time
interval. An interleaving model on a single processor is used for basic
computation steps, with maximum parallelism for time elapsing. Following this
approach, only one of the enabled agents is executed at each moment. To
demonstrate the capabilities of language, we also show how it can be used to
model interactions such as debates and dialogue games taking place between
intelligent agents. Lastly, we present an implementation of the language that
can be accessed via a web interface. Under consideration in Theory and Practice
of Logic Programming (TPLP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Summarizing Strategy Card Game AI Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Kowalski, Radosław Miernik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper concludes five years of AI competitions based on Legends of Code
and Magic (LOCM), a small Collectible Card Game (CCG), designed with the goal
of supporting research and algorithm development. The game was used in a number
of events, including Community Contests on the CodinGame platform, and Strategy
Card Game AI Competition at the IEEE Congress on Evolutionary Computation and
IEEE Conference on Games. LOCM has been used in a number of publications
related to areas such as game tree search algorithms, neural networks,
evaluation functions, and CCG deckbuilding. We present the rules of the game,
the history of organized competitions, and a listing of the participant and
their approaches, as well as some general advice on organizing AI competitions
for the research community. Although the COG 2022 edition was announced to be
the last one, the game remains available and can be played using an online
leaderboard arena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Conference on Games 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Memetic Algorithm with Reinforcement Learning for Sociotechnical
  Production Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10936v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10936v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Grumbach, Nour Eldin Alaa Badr, Pascal Reusch, Sebastian Trojahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The following interdisciplinary article presents a memetic algorithm with
applying deep reinforcement learning (DRL) for solving practically oriented
dual resource constrained flexible job shop scheduling problems (DRC-FJSSP).
From research projects in industry, we recognize the need to consider flexible
machines, flexible human workers, worker capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-dependent setup times and
(partially) automated tasks in human-machine-collaboration. In recent years,
there has been extensive research on metaheuristics and DRL techniques but
focused on simple scheduling environments. However, there are few approaches
combining metaheuristics and DRL to generate schedules more reliably and
efficiently. In this paper, we first formulate a DRC-FJSSP to map complex
industry requirements beyond traditional job shop models. Then we propose a
scheduling framework integrating a discrete event simulation (DES) for schedule
evaluation, considering parallel computing and multicriteria optimization.
Here, a memetic algorithm is enriched with DRL to improve sequencing and
assignment decisions. Through numerical experiments with real-world production
data, we confirm that the framework generates feasible schedules efficiently
and reliably for a balanced optimization of makespan (MS) and total tardiness
(TT). Utilizing DRL instead of random metaheuristic operations leads to better
results in fewer algorithm iterations and outperforms traditional approaches in
such complex environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted by IEEE Access on June 30, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Suffering Toasters -- A New Self-Awareness Test for AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ira Wolfson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A widely accepted definition of intelligence in the context of Artificial
Intelligence (AI) still eludes us. Due to our exceedingly rapid development of
AI paradigms, architectures, and tools, the prospect of naturally arising AI
consciousness seems more likely than ever. In this paper, we claim that all
current intelligence tests are insufficient to point to the existence or lack
of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas
in the philosophy of science, psychology, and other areas of research to
provide a clearer definition of the problems of artificial intelligence,
self-awareness, and agency. We furthermore propose a new heuristic approach to
test for artificial self-awareness and outline a possible implementation.
Finally, we discuss some of the questions that arise from this new heuristic,
be they philosophical or implementation-oriented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 double-column pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $E(2)$-Equivariant Vision <span class="highlight-title">Transformer</span> <span class="chip">UAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjun Xu, Kaifan Yang, Ke Liu, Fengxiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformer (ViT) has achieved remarkable performance in computer
vision. However, positional encoding in ViT makes it substantially difficult to
learn the intrinsic equivariance in data. Initial attempts have been made on
designing equivariant ViT but are proved defective in some cases in this paper.
To address this issue, we design a Group Equivariant Vision Transformer
(GE-ViT) via a novel, effective positional encoding operator. We prove that
GE-ViT meets all the theoretical requirements of an equivariant neural network.
Comprehensive experiments are conducted on standard benchmark datasets,
demonstrating that GE-ViT significantly outperforms non-equivariant
self-attention networks. The code is available at
https://github.com/ZJUCDSYangKaifan/GEVit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to UAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSelect: Customized Selection of Parameters for Fine-Tuning during
  Personalized Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishub Tamirisa, John Won, Chengjun Lu, Ron Arel, Andy Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in federated learning (FL) seek to increase client-level
performance by fine-tuning client parameters on local data or personalizing
architectures for the local task. Existing methods for such personalization
either prune a global model or fine-tune a global model on a local client
distribution. However, these existing methods either personalize at the expense
of retaining important global knowledge, or predetermine network layers for
fine-tuning, resulting in suboptimal storage of global knowledge within client
models. Enlightened by the lottery ticket hypothesis, we first introduce a
hypothesis for finding optimal client subnetworks to locally fine-tune while
leaving the rest of the parameters frozen. We then propose a novel FL
framework, FedSelect, using this procedure that directly personalizes both
client subnetwork structure and parameters, via the simultaneous discovery of
optimal parameters for personalization and the rest of parameters for global
aggregation during training. We show that this method achieves promising
results on CIFAR-10.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward-Respecting Subtasks for Model-Based Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03466v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03466v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard S. Sutton, Marlos C. Machado, G. Zacharias Holland, David Szepesvari, Finbarr Timbers, Brian Tanner, Adam White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve the ambitious goals of artificial intelligence, reinforcement
learning must include planning with a model of the world that is abstract in
state and time. Deep learning has made progress with state abstraction, but
temporal abstraction has rarely been used, despite extensively developed theory
based on the options framework. One reason for this is that the space of
possible options is immense, and the methods previously proposed for option
discovery do not take into account how the option models will be used in
planning. Options are typically discovered by posing subsidiary tasks, such as
reaching a bottleneck state or maximizing the cumulative sum of a sensory
signal other than reward. Each subtask is solved to produce an option, and then
a model of the option is learned and made available to the planning process. In
most previous work, the subtasks ignore the reward on the original problem,
whereas we propose subtasks that use the original reward plus a bonus based on
a feature of the state at the time the option terminates. We show that option
models obtained from such reward-respecting subtasks are much more likely to be
useful in planning than eigenoptions, shortest path options based on bottleneck
states, or reward-respecting options generated by the option-critic. Reward
respecting subtasks strongly constrain the space of options and thereby also
provide a partial solution to the problem of option discovery. Finally, we show
how values, policies, options, and models can all be learned online and
off-policy using standard algorithms and general value functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ F2A2: Flexible Fully-decentralized Approximate Actor-critic for
  Cooperative Multi-agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.11145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.11145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Bo Jin, Xiangfeng Wang, Junchi Yan, Hongyuan Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional centralized multi-agent reinforcement learning (MARL) algorithms
are sometimes unpractical in complicated applications, due to non-interactivity
between agents, curse of dimensionality and computation complexity. Hence,
several decentralized MARL algorithms are motivated. However, existing
decentralized methods only handle the fully cooperative setting where massive
information needs to be transmitted in training. The block coordinate gradient
descent scheme they used for successive independent actor and critic steps can
simplify the calculation, but it causes serious bias. In this paper, we propose
a flexible fully decentralized actor-critic MARL framework, which can combine
most of actor-critic methods, and handle large-scale general cooperative
multi-agent setting. A primal-dual hybrid gradient descent type algorithm
framework is designed to learn individual agents separately for
decentralization. From the perspective of each agent, policy improvement and
value evaluation are jointly optimized, which can stabilize multi-agent policy
learning. Furthermore, our framework can achieve scalability and stability for
large-scale environment and reduce information transmission, by the parameter
sharing mechanism and a novel modeling-other-agents methods based on
theory-of-mind and online supervised learning. Sufficient experiments in
cooperative Multi-agent Particle Environment and StarCraft II show that our
decentralized MARL instantiation algorithms perform competitively against
conventional centralized and decentralized methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>75 pages, 10 figures, JMLR camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10886v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10886v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Yuan, Bo Li, Xin Jin, Wenjun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and
adaptively provides high-quality intrinsic rewards to enhance exploration in
reinforcement learning (RL). More specifically, AIRS selects shaping function
from a predefined set based on the estimated task return in real-time,
providing reliable exploration incentives and alleviating the biased objective
problem. Moreover, we develop an intrinsic reward toolkit to provide efficient
and reliable implementations of diverse intrinsic reward approaches. We test
AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite.
Extensive simulation demonstrates that AIRS can outperform the benchmarking
schemes and achieve superior performance with simple architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SocNavGym: A Reinforcement Learning Gym for Social Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kapoor, Sushant Swamy, Luis Manso, Pilar Bachiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is essential for autonomous robots to be socially compliant while
navigating in human-populated environments. Machine Learning and, especially,
Deep Reinforcement Learning have recently gained considerable traction in the
field of Social Navigation. This can be partially attributed to the resulting
policies not being bound by human limitations in terms of code complexity or
the number of variables that are handled. Unfortunately, the lack of safety
guarantees and the large data requirements by DRL algorithms make learning in
the real world unfeasible. To bridge this gap, simulation environments are
frequently used. We propose SocNavGym, an advanced simulation environment for
social navigation that can generate a wide variety of social navigation
scenarios and facilitates the development of intelligent social agents.
SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly
configured to generate different types of social navigation scenarios. It can
also be configured to work with different hand-crafted and data-driven social
reward signals and to yield a variety of evaluation metrics to benchmark
agents' performance. Further, we also provide a case study where a Dueling-DQN
agent is trained to learn social-navigation policies using SocNavGym. The
results provides evidence that SocNavGym can be used to train an agent from
scratch to navigate in simple as well as complex social scenarios. Our
experiments also show that the agents trained using the data-driven reward
function displays more advanced social compliance in comparison to the
heuristic-based reward function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE RO-MAN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Elastic Decision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02484v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02484v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueh-Hua Wu, Xiaolong Wang, Masashi Hamaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Elastic Decision Transformer (EDT), a significant
advancement over the existing Decision Transformer (DT) and its variants.
Although DT purports to generate an optimal trajectory, empirical evidence
suggests it struggles with trajectory stitching, a process involving the
generation of an optimal or near-optimal trajectory from the best parts of a
set of sub-optimal trajectories. The proposed EDT differentiates itself by
facilitating trajectory stitching during action inference at test time,
achieved by adjusting the history length maintained in DT. Further, the EDT
optimizes the trajectory by retaining a longer history when the previous
trajectory is optimal and a shorter one when it is sub-optimal, enabling it to
"stitch" with a more optimal trajectory. Extensive experimentation demonstrates
EDT's ability to bridge the performance gap between DT-based and Q
Learning-based approaches. In particular, the EDT outperforms Q Learning-based
methods in a multi-task regime on the D4RL locomotion benchmark and Atari
games. Videos are available at: https://kristery.github.io/edt/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://kristery.github.io/edt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Fourier Filter Bank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Wu, Yuhe Jin, Kwang Moo Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defend Data Poisoning Attacks on Voice Authentication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Li, Cameron Baird, Dan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advances in deep learning, speaker verification has achieved very
high accuracy and is gaining popularity as a type of biometric authentication
option in many scenes of our daily life, especially the growing market of web
services. Compared to traditional passwords, "vocal passwords" are much more
convenient as they relieve people from memorizing different passwords. However,
new machine learning attacks are putting these voice authentication systems at
risk. Without a strong security guarantee, attackers could access legitimate
users' web accounts by fooling the deep neural network (DNN) based voice
recognition models. In this paper, we demonstrate an easy-to-implement data
poisoning attack to the voice authentication system, which can hardly be
captured by existing defense mechanisms. Thus, we propose a more robust defense
method, called Guardian, which is a convolutional neural network-based
discriminator. The Guardian discriminator integrates a series of novel
techniques including bias reduction, input augmentation, and ensemble learning.
Our approach is able to distinguish about 95% of attacked accounts from normal
accounts, which is much more effective than existing approaches with only 60%
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-Rate-Free Learning by D-Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07733v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07733v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Defazio, Konstantin Mishchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  D-Adaptation is an approach to automatically setting the learning rate which
asymptotically achieves the optimal rate of convergence for minimizing convex
Lipschitz functions, with no back-tracking or line searches, and no additional
function value or gradient evaluations per step. Our approach is the first
hyper-parameter free method for this class without additional multiplicative
log factors in the convergence rate. We present extensive experiments for SGD
and Adam variants of our method, where the method automatically matches
hand-tuned learning rates across more than a dozen diverse machine learning
problems, including large-scale vision and language problems.
  An open-source implementation is available.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-06T00:00:00Z">2023-07-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong
  General Audio Event Taggers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gong, Sameer Khurana, Leonid Karlinsky, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on Whisper, a recent automatic speech recognition
model trained with a massive 680k hour labeled speech corpus recorded in
diverse conditions. We first show an interesting finding that while Whisper is
very robust against real-world background sounds (e.g., music), its audio
representation is actually not noise-invariant, but is instead highly
correlated to non-speech sounds, indicating that Whisper recognizes speech
conditioned on the noise type. With this finding, we build a unified audio
tagging and speech recognition model Whisper-AT by freezing the backbone of
Whisper, and training a lightweight audio tagging model on top of it. With <1%
extra computational cost, Whisper-AT can recognize audio events, in addition to
spoken text, in a single forward pass.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2023. Code at
  https://github.com/yuangongnd/whisper-at</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track Mix Generation on Music Streaming Services using <span class="highlight-title">Transformer</span>s <span class="chip">RecSys 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Bendada, Théo Bontempelli, Mathieu Morlon, Benjamin Chapus, Thibault Cador, Thomas Bouabça, Guillaume Salha-Galvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Track Mix, a personalized playlist generation system
released in 2022 on the music streaming service Deezer. Track Mix automatically
generates "mix" playlists inspired by initial music tracks, allowing users to
discover music similar to their favorite content. To generate these mixes, we
consider a Transformer model trained on millions of track sequences from user
playlists. In light of the growing popularity of Transformers in recent years,
we analyze the advantages, drawbacks, and technical challenges of using such a
model for mix generation on the service, compared to a more traditional
collaborative filtering approach. Since its release, Track Mix has been
generating playlists for millions of users daily, enhancing their music
discovery experience on Deezer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RecSys 2023 - Industry track with oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning for the Efficient Detection of COVID-19 from
  Smartphone Audio Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Giovanni Campana, Franca Delmastro, Elena Pagani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disease detection from smartphone data represents an open research challenge
in mobile health (m-health) systems. COVID-19 and its respiratory symptoms are
an important case study in this area and their early detection is a potential
real instrument to counteract the pandemic situation. The efficacy of this
solution mainly depends on the performances of AI algorithms applied to the
collected data and their possible implementation directly on the users' mobile
devices. Considering these issues, and the limited amount of available data, in
this paper we present the experimental evaluation of 3 different deep learning
models, compared also with hand-crafted features, and of two main approaches of
transfer learning in the considered scenario: both feature extraction and
fine-tuning. Specifically, we considered VGGish, YAMNET, and
L\textsuperscript{3}-Net (including 12 different configurations) evaluated
through user-independent experiments on 4 different datasets (13,447 samples in
total). Results clearly show the advantages of L\textsuperscript{3}-Net in all
the experimental settings as it overcomes the other solutions by 12.3\% in
terms of Precision-Recall AUC as features extractor, and by 10\% when the model
is fine-tuned. Moreover, we note that to fine-tune only the fully-connected
layers of the pre-trained models generally leads to worse performances, with an
average drop of 6.6\% with respect to feature extraction. %highlighting the
need for further investigations. Finally, we evaluate the memory footprints of
the different models for their possible applications on commercial mobile
devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation
  and Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guinan Li, Jiajun Deng, Mengzhe Geng, Zengrui Jin, Tianzi Wang, Shujie Hu, Mingyu Cui, Helen Meng, Xunying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate recognition of cocktail party speech containing overlapping
speakers, noise and reverberation remains a highly challenging task to date.
Motivated by the invariance of visual modality to acoustic signal corruption,
an audio-visual multi-channel speech separation, dereverberation and
recognition approach featuring a full incorporation of visual information into
all system components is proposed in this paper. The efficacy of the video
input is consistently demonstrated in mask-based MVDR speech separation,
DNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end and
Conformer ASR back-end. Audio-visual integrated front-end architectures
performing speech separation and dereverberation in a pipelined or joint
fashion via mask-based WPD are investigated. The error cost mismatch between
the speech enhancement front-end and ASR back-end components is minimized by
end-to-end jointly fine-tuning using either the ASR cost function alone, or its
interpolation with the speech enhancement loss. Experiments were conducted on
the mixture overlapped and reverberant speech data constructed using simulation
or replay of the Oxford LRS2 dataset. The proposed audio-visual multi-channel
speech separation, dereverberation and recognition systems consistently
outperformed the comparable audio-only baseline by 9.1% and 6.2% absolute
(41.7% and 36.0% relative) word error rate (WER) reductions. Consistent speech
enhancement improvements were also obtained on PESQ, STOI and SRMR scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Relationship Between Speech Features Changes When You Get Depressed:
  Feature Correlations for Improving Speed and Performance of Depression
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuxiang Tao, Wei Ma, Xuri Ge, Anna Esposito, Alessandro Vinciarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work shows that depression changes the correlation between features
extracted from speech. Furthermore, it shows that using such an insight can
improve the training speed and performance of depression detectors based on
SVMs and LSTMs. The experiments were performed over the Androids Corpus, a
publicly available dataset involving 112 speakers, including 58 people
diagnosed with depression by professional psychiatrists. The results show that
the models used in the experiments improve in terms of training speed and
performance when fed with feature correlation matrices rather than with feature
vectors. The relative reduction of the error rate ranges between 23.1% and
26.6% depending on the model. The probable explanation is that feature
correlation matrices appear to be more variable in the case of depressed
speakers. Correspondingly, such a phenomenon can be thought of as a depression
marker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating raw waveforms with deep learning frameworks for speech
  emotion recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeynep Hilal Kilimci, Ulku Bayraktar, Ayhan Kucukmanisa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition is a challenging task in speech processing field.
For this reason, feature extraction process has a crucial importance to
demonstrate and process the speech signals. In this work, we represent a model,
which feeds raw audio files directly into the deep neural networks without any
feature extraction stage for the recognition of emotions utilizing six
different data sets, EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. To
demonstrate the contribution of proposed model, the performance of traditional
feature extraction techniques namely, mel-scale spectogram, mel-frequency
cepstral coefficients, are blended with machine learning algorithms, ensemble
learning methods, deep and hybrid deep learning techniques. Support vector
machine, decision tree, naive Bayes, random forests models are evaluated as
machine learning algorithms while majority voting and stacking methods are
assessed as ensemble learning techniques. Moreover, convolutional neural
networks, long short-term memory networks, and hybrid CNN- LSTM model are
evaluated as deep learning techniques and compared with machine learning and
ensemble learning methods. To demonstrate the effectiveness of proposed model,
the comparison with state-of-the-art studies are carried out. Based on the
experiment results, CNN model excels existent approaches with 95.86% of
accuracy for TESS+RAVDESS data set using raw audio files, thence determining
the new state-of-the-art. The proposed model performs 90.34% of accuracy for
EMO-DB with CNN model, 90.42% of accuracy for RAVDESS with CNN model, 99.48% of
accuracy for TESS with LSTM model, 69.72% of accuracy for CREMA with CNN model,
85.76% of accuracy for SAVEE with CNN model in speaker-independent audio
categorization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSARSR: Deep Stacked Auto-encoders Enhanced Robust Speaker Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Wang, Chunyan Zeng, Surong Duan, Hongjie Ouyang, Hongmin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker recognition is a biometric modality that utilizes the speaker's
speech segments to recognize the identity, determining whether the test speaker
belongs to one of the enrolled speakers. In order to improve the robustness of
the i-vector framework on cross-channel conditions and explore the nova method
for applying deep learning to speaker recognition, the Stacked Auto-encoders
are used to get the abstract extraction of the i-vector instead of applying
PLDA. After pre-processing and feature extraction, the speaker and
channel-independent speeches are employed for UBM training. The UBM is then
used to extract the i-vector of the enrollment and test speech. Unlike the
traditional i-vector framework, which uses linear discriminant analysis (LDA)
to reduce dimension and increase the discrimination between speaker subspaces,
this research use stacked auto-encoders to reconstruct the i-vector with lower
dimension and different classifiers can be chosen to achieve final
classification. The experimental results show that the proposed method achieves
better performance than the state-of-the-art method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-Device Constrained <span class="highlight-title">Self-Supervised</span> Speech Representation Learning for
  Keyword Spotting via Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gene-Ping Yang, Yue Gu, Qingming Tang, Dongsu Du, Yuzong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large self-supervised models are effective feature extractors, but their
application is challenging under on-device budget constraints and biased
dataset collection, especially in keyword spotting. To address this, we
proposed a knowledge distillation-based self-supervised speech representation
learning (S3RL) architecture for on-device keyword spotting. Our approach used
a teacher-student framework to transfer knowledge from a larger, more complex
model to a smaller, light-weight model using dual-view cross-correlation
distillation and the teacher's codebook as learning objectives. We evaluated
our model's performance on an Alexa keyword spotting detection task using a
16.6k-hour in-house dataset. Our technique showed exceptional performance in
normal and noisy conditions, demonstrating the efficacy of knowledge
distillation methods in constructing self-supervised models for keyword
spotting tasks while working within on-device resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gammatonegram Representation for End-to-End Dysarthric Speech Processing
  Tasks: Speech Recognition, Speaker Identification, and Intelligibility
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Farhadipour, Hadi Veisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dysarthria is a disability that causes a disturbance in the human speech
system and reduces the quality and intelligibility of a person's speech.
Because of this effect, the normal speech processing systems can not work
properly on impaired speech. This disability is usually associated with
physical disabilities. Therefore, designing a system that can perform some
tasks by receiving voice commands in the smart home can be a significant
achievement. In this work, we introduce gammatonegram as an effective method to
represent audio files with discriminative details, which is used as input for
the convolutional neural network. On the other word, we convert each speech
file into an image and propose image recognition system to classify speech in
different scenarios. Proposed CNN is based on the transfer learning method on
the pre-trained Alexnet. In this research, the efficiency of the proposed
system for speech recognition, speaker identification, and intelligibility
assessment is evaluated. According to the results on the UA dataset, the
proposed speech recognition system achieved 91.29% accuracy in
speaker-dependent mode, the speaker identification system acquired 87.74%
accuracy in text-dependent mode, and the intelligibility assessment system
achieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network
speech recognition system that works fully automatically. This system is
located in a cascade arrangement with the two-class intelligibility assessment
system, and the output of this system activates each one of the speech
recognition networks. This architecture achieves an accuracy of 92.3% WRR. The
source code of this paper is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Comparison of <span class="highlight-title">Pre-train</span>ed Models for Speech-to-Text in
  Turkish: Whisper-Small and Wav2Vec2-XLS-R-300M 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oyku Berfin Mercan, Sercan Cepni, Davut Emre Tasar, Sukru Ozan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, the performances of the Whisper-Small and Wav2Vec2-XLS-R-300M
models which are two pre-trained multilingual models for speech to text were
examined for the Turkish language. Mozilla Common Voice version 11.0 which is
prepared in Turkish language and is an open-source data set, was used in the
study. The multilingual models, Whisper- Small and Wav2Vec2-XLS-R-300M were
fine-tuned with this data set which contains a small amount of data. The speech
to text performance of the two models was compared. WER values are calculated
as 0.28 and 0.16 for the Wav2Vec2-XLS- R-300M and the Whisper-Small models
respectively. In addition, the performances of the models were examined with
the test data prepared with call center records that were not included in the
training and validation dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, in Turkish language, Pre-Print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reef Elegy: An Auditory Display of Hawaii's 2019 Coral Bleaching Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Kalonaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes an auditory display of Hawaii's 2019 coral bleaching
data via means of spatial audio and parameter mapping methods. Selected data
fields spanning 78 days are mapped to sound surrogates of coral reefs' natural
soundscapes, which are progressively altered in their constituent elements as
the corresponding coral locations undergo bleaching. For some of these
elements, this process outlines a trajectory from a dense to a sparser, reduced
soundscape, while for others it translates moving away from harmonic tones and
towards complex spectra. This experiment is accompanied by a short evaluation
study to contextualize it in an established aesthetic perspective space and to
probe its potential for public engagement in the discourse around climate
change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in: Proceedings of the 28th International Conference on
  Auditory Display (ICAD 2023) NOTE: This version (v2) replaces Figure 2, which
  was incorrectly rendered. Do not use or cite the previous version (v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> representations in speech-based depression detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wu, Chao Zhang, Philip C. Woodland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes handling training data sparsity in speech-based automatic
depression detection (SDD) using foundation models pre-trained with
self-supervised learning (SSL). An analysis of SSL representations derived from
different layers of pre-trained foundation models is first presented for SDD,
which provides insight to suitable indicator for depression detection.
Knowledge transfer is then performed from automatic speech recognition (ASR)
and emotion recognition to SDD by fine-tuning the foundation models. Results
show that the uses of oracle and ASR transcriptions yield similar SDD
performance when the hidden representations of the ASR model is incorporated
along with the ASR textual information. By integrating representations from
multiple foundation models, state-of-the-art SDD results based on real ASR were
achieved on the DAIC-WOZ dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Singing Voice Conversion Challenge 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Chin Huang, Lester Phillip Violeta, Songxiang Liu, Jiatong Shi, Tomoki Toda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the latest iteration of the voice conversion challenge (VCC)
series, a bi-annual scientific event aiming to compare and understand different
voice conversion (VC) systems based on a common dataset. This year we shifted
our focus to singing voice conversion (SVC), thus named the challenge the
Singing Voice Conversion Challenge (SVCC). A new database was constructed for
two tasks, namely in-domain and cross-domain SVC. The challenge was run for two
months, and in total we received 26 submissions, including 2 baselines. Through
a large-scale crowd-sourced listening test, we observed that for both tasks,
although human-level naturalness was achieved by the top system, no team was
able to obtain a similarity score as high as the target speakers. Also, as
expected, cross-domain SVC is harder than in-domain SVC, especially in the
similarity aspect. We also investigated whether existing objective measurements
were able to predict perceptual performance, and found that only few of them
could reach a significant correlation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align With Purpose: Optimize Desired Properties in CTC Models with a
  General Plug-and-Play Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connectionist Temporal Classification (CTC) is a widely used criterion for
training supervised sequence-to-sequence (seq2seq) models. It enables learning
the relations between input and output sequences, termed alignments, by
marginalizing over perfect alignments (that yield the ground truth), at the
expense of imperfect alignments. This binary differentiation of perfect and
imperfect alignments falls short of capturing other essential alignment
properties that hold significance in other real-world applications. Here we
propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play
framework}$ for enhancing a desired property in models trained with the CTC
criterion. We do that by complementing the CTC with an additional loss term
that prioritizes alignments according to a desired property. Our method does
not require any intervention in the CTC loss function, enables easy
optimization of a variety of properties, and allows differentiation between
both perfect and imperfect alignments. We apply our framework in the domain of
Automatic Speech Recognition (ASR) and show its generality in terms of property
selection, architectural choice, and scale of training dataset (up to 280,000
hours). To demonstrate the effectiveness of our framework, we apply it to two
unrelated properties: emission time and word error rate (WER). For the former,
we report an improvement of up to 570ms in latency optimization with a minor
reduction in WER, and for the latter, we report a relative improvement of 4.5%
WER over the baseline models. To the best of our knowledge, these applications
have never been demonstrated to work on a scale of data as large as ours.
Notably, our method can be implemented using only a few lines of code, and can
be extended to other alignment-free loss functions and to domains other than
ASR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Microphone Signals Alone Sufficient for Self-Positioning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faxian Cao, Yongqiang Cheng, Adil Mehmood Khan, Zhijing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where asynchronous environments pose challenges to traditional
self-positioning methods, we propose a new transformation to the existing
paradigm. Traditionally, time of arrival (TOA) measurements require both
microphone and source signals, limiting their applicability in environments
with unknown emission time of human voices or sources and unknown recording
start time of independent microphones. To address this issue, our research
pioneers a mapping function capable of transforming both TOA and time
difference of arrival (TDOA) formulas, demonstrating, for the first time, that
they can be identical to one another. This implies that microphone signals
alone are sufficient for self-positioning without the need for source signal
waveforms, a groundbreaking advancement in the field that carries the potential
to revolutionize self-positioning techniques, expanding their applicability in
challenging environments. Supported by a robust mathematical proof and
compelling experimental results, this research represents a timely and
significant contribution to the current discourse in signal, and audio
processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1 figure, including 3 sub-figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04596v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04596v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Polák, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, Juan Pino, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by
the broadening interests of the spoken language translation community.
ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2)
simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech
translation (S2ST) -- each task is supported with a wide variety of approaches,
differentiating ESPnet-ST-v2 from other open source spoken language translation
toolkits. This toolkit offers state-of-the-art architectures such as
transducers, hybrid CTC/attention, multi-decoders with searchable
intermediates, time-synchronous blockwise CTC/attention, Translatotron models,
and direct discrete unit models. In this paper, we describe the overall design,
example models for each task, and performance benchmarking behind ESPnet-ST-v2,
which is publicly available at https://github.com/espnet/espnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023; System Demonstration</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong
  General Audio Event Taggers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gong, Sameer Khurana, Leonid Karlinsky, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on Whisper, a recent automatic speech recognition
model trained with a massive 680k hour labeled speech corpus recorded in
diverse conditions. We first show an interesting finding that while Whisper is
very robust against real-world background sounds (e.g., music), its audio
representation is actually not noise-invariant, but is instead highly
correlated to non-speech sounds, indicating that Whisper recognizes speech
conditioned on the noise type. With this finding, we build a unified audio
tagging and speech recognition model Whisper-AT by freezing the backbone of
Whisper, and training a lightweight audio tagging model on top of it. With <1%
extra computational cost, Whisper-AT can recognize audio events, in addition to
spoken text, in a single forward pass.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2023. Code at
  https://github.com/yuangongnd/whisper-at</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering implicit pitch contours from formants in whispered speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Pérez Zarazaga, Zofia Malisz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whispered speech is characterised by a noise-like excitation that results in
the lack of fundamental frequency. Considering that prosodic phenomena such as
intonation are perceived through f0 variation, the perception of whispered
prosody is relatively difficult. At the same time, studies have shown that
speakers do attempt to produce intonation when whispering and that prosodic
variability is being transmitted, suggesting that intonation "survives" in
whispered formant structure. In this paper, we aim to estimate the way in which
formant contours correlate with an "implicit" pitch contour in whisper, using a
machine learning model. We propose a two-step method: using a parallel corpus,
we first transform the whispered formants into their phonated equivalents using
a denoising autoencoder. We then analyse the formant contours to predict
phonated pitch contour variation. We observe that our method is effective in
establishing a relationship between whispered and phonated formants and in
uncovering implicit pitch contours in whisper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 2 tables, Accepted at ICPhS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label-Synchronous Neural Transducer for End-to-End ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqi Deng, Philip C. Woodland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural transducers provide a natural approach to streaming ASR. However, they
augment output sequences with blank tokens which leads to challenges for domain
adaptation using text data. This paper proposes a label-synchronous neural
transducer (LS-Transducer), which extracts a label-level encoder representation
before combining it with the prediction network output. Hence blank tokens are
no longer needed and the prediction network can be easily adapted using text
data. An Auto-regressive Integrate-and-Fire (AIF) mechanism is proposed to
generate the label-level encoder representation while retaining the streaming
property. In addition, a streaming joint decoding method is designed to improve
ASR accuracy. Experiments show that compared to standard neural transducers,
the proposed LS-Transducer gave a 10% relative WER reduction (WERR) for
intra-domain Librispeech-100h data, as well as 17% and 19% relative WERRs on
cross-domain TED-LIUM 2 and AESRC2020 data with an adapted prediction network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track Mix Generation on Music Streaming Services using <span class="highlight-title">Transformer</span>s <span class="chip">RecSys 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Bendada, Théo Bontempelli, Mathieu Morlon, Benjamin Chapus, Thibault Cador, Thomas Bouabça, Guillaume Salha-Galvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Track Mix, a personalized playlist generation system
released in 2022 on the music streaming service Deezer. Track Mix automatically
generates "mix" playlists inspired by initial music tracks, allowing users to
discover music similar to their favorite content. To generate these mixes, we
consider a Transformer model trained on millions of track sequences from user
playlists. In light of the growing popularity of Transformers in recent years,
we analyze the advantages, drawbacks, and technical challenges of using such a
model for mix generation on the service, compared to a more traditional
collaborative filtering approach. Since its release, Track Mix has been
generating playlists for millions of users daily, enhancing their music
discovery experience on Deezer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RecSys 2023 - Industry track with oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning for the Efficient Detection of COVID-19 from
  Smartphone Audio Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Giovanni Campana, Franca Delmastro, Elena Pagani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disease detection from smartphone data represents an open research challenge
in mobile health (m-health) systems. COVID-19 and its respiratory symptoms are
an important case study in this area and their early detection is a potential
real instrument to counteract the pandemic situation. The efficacy of this
solution mainly depends on the performances of AI algorithms applied to the
collected data and their possible implementation directly on the users' mobile
devices. Considering these issues, and the limited amount of available data, in
this paper we present the experimental evaluation of 3 different deep learning
models, compared also with hand-crafted features, and of two main approaches of
transfer learning in the considered scenario: both feature extraction and
fine-tuning. Specifically, we considered VGGish, YAMNET, and
L\textsuperscript{3}-Net (including 12 different configurations) evaluated
through user-independent experiments on 4 different datasets (13,447 samples in
total). Results clearly show the advantages of L\textsuperscript{3}-Net in all
the experimental settings as it overcomes the other solutions by 12.3\% in
terms of Precision-Recall AUC as features extractor, and by 10\% when the model
is fine-tuned. Moreover, we note that to fine-tune only the fully-connected
layers of the pre-trained models generally leads to worse performances, with an
average drop of 6.6\% with respect to feature extraction. %highlighting the
need for further investigations. Finally, we evaluate the memory footprints of
the different models for their possible applications on commercial mobile
devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation
  and Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guinan Li, Jiajun Deng, Mengzhe Geng, Zengrui Jin, Tianzi Wang, Shujie Hu, Mingyu Cui, Helen Meng, Xunying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate recognition of cocktail party speech containing overlapping
speakers, noise and reverberation remains a highly challenging task to date.
Motivated by the invariance of visual modality to acoustic signal corruption,
an audio-visual multi-channel speech separation, dereverberation and
recognition approach featuring a full incorporation of visual information into
all system components is proposed in this paper. The efficacy of the video
input is consistently demonstrated in mask-based MVDR speech separation,
DNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end and
Conformer ASR back-end. Audio-visual integrated front-end architectures
performing speech separation and dereverberation in a pipelined or joint
fashion via mask-based WPD are investigated. The error cost mismatch between
the speech enhancement front-end and ASR back-end components is minimized by
end-to-end jointly fine-tuning using either the ASR cost function alone, or its
interpolation with the speech enhancement loss. Experiments were conducted on
the mixture overlapped and reverberant speech data constructed using simulation
or replay of the Oxford LRS2 dataset. The proposed audio-visual multi-channel
speech separation, dereverberation and recognition systems consistently
outperformed the comparable audio-only baseline by 9.1% and 6.2% absolute
(41.7% and 36.0% relative) word error rate (WER) reductions. Consistent speech
enhancement improvements were also obtained on PESQ, STOI and SRMR scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Relationship Between Speech Features Changes When You Get Depressed:
  Feature Correlations for Improving Speed and Performance of Depression
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuxiang Tao, Wei Ma, Xuri Ge, Anna Esposito, Alessandro Vinciarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work shows that depression changes the correlation between features
extracted from speech. Furthermore, it shows that using such an insight can
improve the training speed and performance of depression detectors based on
SVMs and LSTMs. The experiments were performed over the Androids Corpus, a
publicly available dataset involving 112 speakers, including 58 people
diagnosed with depression by professional psychiatrists. The results show that
the models used in the experiments improve in terms of training speed and
performance when fed with feature correlation matrices rather than with feature
vectors. The relative reduction of the error rate ranges between 23.1% and
26.6% depending on the model. The probable explanation is that feature
correlation matrices appear to be more variable in the case of depressed
speakers. Correspondingly, such a phenomenon can be thought of as a depression
marker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating raw waveforms with deep learning frameworks for speech
  emotion recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeynep Hilal Kilimci, Ulku Bayraktar, Ayhan Kucukmanisa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition is a challenging task in speech processing field.
For this reason, feature extraction process has a crucial importance to
demonstrate and process the speech signals. In this work, we represent a model,
which feeds raw audio files directly into the deep neural networks without any
feature extraction stage for the recognition of emotions utilizing six
different data sets, EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. To
demonstrate the contribution of proposed model, the performance of traditional
feature extraction techniques namely, mel-scale spectogram, mel-frequency
cepstral coefficients, are blended with machine learning algorithms, ensemble
learning methods, deep and hybrid deep learning techniques. Support vector
machine, decision tree, naive Bayes, random forests models are evaluated as
machine learning algorithms while majority voting and stacking methods are
assessed as ensemble learning techniques. Moreover, convolutional neural
networks, long short-term memory networks, and hybrid CNN- LSTM model are
evaluated as deep learning techniques and compared with machine learning and
ensemble learning methods. To demonstrate the effectiveness of proposed model,
the comparison with state-of-the-art studies are carried out. Based on the
experiment results, CNN model excels existent approaches with 95.86% of
accuracy for TESS+RAVDESS data set using raw audio files, thence determining
the new state-of-the-art. The proposed model performs 90.34% of accuracy for
EMO-DB with CNN model, 90.42% of accuracy for RAVDESS with CNN model, 99.48% of
accuracy for TESS with LSTM model, 69.72% of accuracy for CREMA with CNN model,
85.76% of accuracy for SAVEE with CNN model in speaker-independent audio
categorization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSARSR: Deep Stacked Auto-encoders Enhanced Robust Speaker Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Wang, Chunyan Zeng, Surong Duan, Hongjie Ouyang, Hongmin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker recognition is a biometric modality that utilizes the speaker's
speech segments to recognize the identity, determining whether the test speaker
belongs to one of the enrolled speakers. In order to improve the robustness of
the i-vector framework on cross-channel conditions and explore the nova method
for applying deep learning to speaker recognition, the Stacked Auto-encoders
are used to get the abstract extraction of the i-vector instead of applying
PLDA. After pre-processing and feature extraction, the speaker and
channel-independent speeches are employed for UBM training. The UBM is then
used to extract the i-vector of the enrollment and test speech. Unlike the
traditional i-vector framework, which uses linear discriminant analysis (LDA)
to reduce dimension and increase the discrimination between speaker subspaces,
this research use stacked auto-encoders to reconstruct the i-vector with lower
dimension and different classifiers can be chosen to achieve final
classification. The experimental results show that the proposed method achieves
better performance than the state-of-the-art method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-Device Constrained <span class="highlight-title">Self-Supervised</span> Speech Representation Learning for
  Keyword Spotting via Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gene-Ping Yang, Yue Gu, Qingming Tang, Dongsu Du, Yuzong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large self-supervised models are effective feature extractors, but their
application is challenging under on-device budget constraints and biased
dataset collection, especially in keyword spotting. To address this, we
proposed a knowledge distillation-based self-supervised speech representation
learning (S3RL) architecture for on-device keyword spotting. Our approach used
a teacher-student framework to transfer knowledge from a larger, more complex
model to a smaller, light-weight model using dual-view cross-correlation
distillation and the teacher's codebook as learning objectives. We evaluated
our model's performance on an Alexa keyword spotting detection task using a
16.6k-hour in-house dataset. Our technique showed exceptional performance in
normal and noisy conditions, demonstrating the efficacy of knowledge
distillation methods in constructing self-supervised models for keyword
spotting tasks while working within on-device resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gammatonegram Representation for End-to-End Dysarthric Speech Processing
  Tasks: Speech Recognition, Speaker Identification, and Intelligibility
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Farhadipour, Hadi Veisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dysarthria is a disability that causes a disturbance in the human speech
system and reduces the quality and intelligibility of a person's speech.
Because of this effect, the normal speech processing systems can not work
properly on impaired speech. This disability is usually associated with
physical disabilities. Therefore, designing a system that can perform some
tasks by receiving voice commands in the smart home can be a significant
achievement. In this work, we introduce gammatonegram as an effective method to
represent audio files with discriminative details, which is used as input for
the convolutional neural network. On the other word, we convert each speech
file into an image and propose image recognition system to classify speech in
different scenarios. Proposed CNN is based on the transfer learning method on
the pre-trained Alexnet. In this research, the efficiency of the proposed
system for speech recognition, speaker identification, and intelligibility
assessment is evaluated. According to the results on the UA dataset, the
proposed speech recognition system achieved 91.29% accuracy in
speaker-dependent mode, the speaker identification system acquired 87.74%
accuracy in text-dependent mode, and the intelligibility assessment system
achieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network
speech recognition system that works fully automatically. This system is
located in a cascade arrangement with the two-class intelligibility assessment
system, and the output of this system activates each one of the speech
recognition networks. This architecture achieves an accuracy of 92.3% WRR. The
source code of this paper is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Comparison of <span class="highlight-title">Pre-train</span>ed Models for Speech-to-Text in
  Turkish: Whisper-Small and Wav2Vec2-XLS-R-300M 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oyku Berfin Mercan, Sercan Cepni, Davut Emre Tasar, Sukru Ozan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, the performances of the Whisper-Small and Wav2Vec2-XLS-R-300M
models which are two pre-trained multilingual models for speech to text were
examined for the Turkish language. Mozilla Common Voice version 11.0 which is
prepared in Turkish language and is an open-source data set, was used in the
study. The multilingual models, Whisper- Small and Wav2Vec2-XLS-R-300M were
fine-tuned with this data set which contains a small amount of data. The speech
to text performance of the two models was compared. WER values are calculated
as 0.28 and 0.16 for the Wav2Vec2-XLS- R-300M and the Whisper-Small models
respectively. In addition, the performances of the models were examined with
the test data prepared with call center records that were not included in the
training and validation dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, in Turkish language, Pre-Print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Read, Look or Listen? What's Needed for Solving a Multimodal <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Netta Madvil, Yonatan Bitton, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of large-scale multimodal datasets presents unique challenges
in assessing dataset quality. We propose a two-step method to analyze
multimodal datasets, which leverages a small seed of human annotation to map
each multimodal instance to the modalities required to process it. Our method
sheds light on the importance of different modalities in datasets, as well as
the relationship between them. We apply our approach to TVQA, a video
question-answering dataset, and discover that most questions can be answered
using a single modality, without a substantial bias towards any specific
modality. Moreover, we find that more than 70% of the questions are solvable
using several different single-modality strategies, e.g., by either looking at
the video or listening to the audio, highlighting the limited integration of
multiple modalities in TVQA. We leverage our annotation and analyze the MERLOT
Reserve, finding that it struggles with image-based questions compared to text
and audio, but also with auditory speaker identification. Based on our
observations, we introduce a new test set that necessitates multiple
modalities, observing a dramatic drop in model performance. Our methodology
provides valuable insights into multimodal datasets and highlights the need for
the development of more robust models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Noise Control in The New Century: The Role and Prospect of Signal
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyuan Shi, Bhan Lam, Woon-Seng Gan, Jordan Cheer, Stephen J. Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since Paul Leug's 1933 patent application for a system for the active control
of sound, the field of active noise control (ANC) has not flourished until the
advent of digital signal processors forty years ago. Early theoretical
advancements in digital signal processing and processors laid the groundwork
for the phenomenal growth of the field, particularly over the past
quarter-century. The widespread commercial success of ANC in aircraft cabins,
automobile cabins, and headsets demonstrates the immeasurable public health and
economic benefits of ANC. This article continues where Elliott and Nelson's
1993 Signal Processing Magazine article and Elliott's 1997 50th anniversary
commentary on ANC left off, tracing the technical developments and applications
in ANC spurred by the seminal texts of Nelson and Elliott (1991), Kuo and
Morgan (1996), Hansen and Snyder (1996), and Elliott (2001) since the turn of
the century. This article focuses on technical developments pertaining to
real-world implementations, such as improving algorithmic convergence, reducing
system latency, and extending control to non-stationary and/or broadband noise,
as well as the commercial transition challenges from analog to digital ANC
systems. Finally, open issues and the future of ANC in the era of artificial
intelligence are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to inter.noise 2023, Chiba, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Version Control of Speaker Recognition Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.12069v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.12069v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Wang, Ignacio Lopez Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses one of the most challenging practical engineering
problems in speaker recognition systems - the version control of models and
user profiles. A typical speaker recognition system consists of two stages: the
enrollment stage, where a profile is generated from user-provided enrollment
audio; and the runtime stage, where the voice identity of the runtime audio is
compared against the stored profiles. As technology advances, the speaker
recognition system needs to be updated for better performance. However, if the
stored user profiles are not updated accordingly, version mismatch will result
in meaningless recognition results. In this paper, we describe different
version control strategies for speaker recognition systems that had been
carefully studied at Google from years of engineering practice. These
strategies are categorized into three groups according to how they are deployed
in the production environment: device-side deployment, server-side deployment,
and hybrid deployment. To compare different strategies with quantitative
metrics under various network configurations, we present SpeakerVerSim, an
easily-extensible Python-based simulation framework for different server-side
deployment strategies of speaker recognition systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reef Elegy: An Auditory Display of Hawaii's 2019 Coral Bleaching Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Kalonaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes an auditory display of Hawaii's 2019 coral bleaching
data via means of spatial audio and parameter mapping methods. Selected data
fields spanning 78 days are mapped to sound surrogates of coral reefs' natural
soundscapes, which are progressively altered in their constituent elements as
the corresponding coral locations undergo bleaching. For some of these
elements, this process outlines a trajectory from a dense to a sparser, reduced
soundscape, while for others it translates moving away from harmonic tones and
towards complex spectra. This experiment is accompanied by a short evaluation
study to contextualize it in an established aesthetic perspective space and to
probe its potential for public engagement in the discourse around climate
change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in: Proceedings of the 28th International Conference on
  Auditory Display (ICAD 2023) NOTE: This version (v2) replaces Figure 2, which
  was incorrectly rendered. Do not use or cite the previous version (v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> representations in speech-based depression detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wu, Chao Zhang, Philip C. Woodland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes handling training data sparsity in speech-based automatic
depression detection (SDD) using foundation models pre-trained with
self-supervised learning (SSL). An analysis of SSL representations derived from
different layers of pre-trained foundation models is first presented for SDD,
which provides insight to suitable indicator for depression detection.
Knowledge transfer is then performed from automatic speech recognition (ASR)
and emotion recognition to SDD by fine-tuning the foundation models. Results
show that the uses of oracle and ASR transcriptions yield similar SDD
performance when the hidden representations of the ASR model is incorporated
along with the ASR textual information. By integrating representations from
multiple foundation models, state-of-the-art SDD results based on real ASR were
achieved on the DAIC-WOZ dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Singing Voice Conversion Challenge 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Chin Huang, Lester Phillip Violeta, Songxiang Liu, Jiatong Shi, Tomoki Toda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the latest iteration of the voice conversion challenge (VCC)
series, a bi-annual scientific event aiming to compare and understand different
voice conversion (VC) systems based on a common dataset. This year we shifted
our focus to singing voice conversion (SVC), thus named the challenge the
Singing Voice Conversion Challenge (SVCC). A new database was constructed for
two tasks, namely in-domain and cross-domain SVC. The challenge was run for two
months, and in total we received 26 submissions, including 2 baselines. Through
a large-scale crowd-sourced listening test, we observed that for both tasks,
although human-level naturalness was achieved by the top system, no team was
able to obtain a similarity score as high as the target speakers. Also, as
expected, cross-domain SVC is harder than in-domain SVC, especially in the
similarity aspect. We also investigated whether existing objective measurements
were able to predict perceptual performance, and found that only few of them
could reach a significant correlation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align With Purpose: Optimize Desired Properties in CTC Models with a
  General Plug-and-Play Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connectionist Temporal Classification (CTC) is a widely used criterion for
training supervised sequence-to-sequence (seq2seq) models. It enables learning
the relations between input and output sequences, termed alignments, by
marginalizing over perfect alignments (that yield the ground truth), at the
expense of imperfect alignments. This binary differentiation of perfect and
imperfect alignments falls short of capturing other essential alignment
properties that hold significance in other real-world applications. Here we
propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play
framework}$ for enhancing a desired property in models trained with the CTC
criterion. We do that by complementing the CTC with an additional loss term
that prioritizes alignments according to a desired property. Our method does
not require any intervention in the CTC loss function, enables easy
optimization of a variety of properties, and allows differentiation between
both perfect and imperfect alignments. We apply our framework in the domain of
Automatic Speech Recognition (ASR) and show its generality in terms of property
selection, architectural choice, and scale of training dataset (up to 280,000
hours). To demonstrate the effectiveness of our framework, we apply it to two
unrelated properties: emission time and word error rate (WER). For the former,
we report an improvement of up to 570ms in latency optimization with a minor
reduction in WER, and for the latter, we report a relative improvement of 4.5%
WER over the baseline models. To the best of our knowledge, these applications
have never been demonstrated to work on a scale of data as large as ours.
Notably, our method can be implemented using only a few lines of code, and can
be extended to other alignment-free loss functions and to domains other than
ASR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Microphone Signals Alone Sufficient for Self-Positioning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faxian Cao, Yongqiang Cheng, Adil Mehmood Khan, Zhijing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where asynchronous environments pose challenges to traditional
self-positioning methods, we propose a new transformation to the existing
paradigm. Traditionally, time of arrival (TOA) measurements require both
microphone and source signals, limiting their applicability in environments
with unknown emission time of human voices or sources and unknown recording
start time of independent microphones. To address this issue, our research
pioneers a mapping function capable of transforming both TOA and time
difference of arrival (TDOA) formulas, demonstrating, for the first time, that
they can be identical to one another. This implies that microphone signals
alone are sufficient for self-positioning without the need for source signal
waveforms, a groundbreaking advancement in the field that carries the potential
to revolutionize self-positioning techniques, expanding their applicability in
challenging environments. Supported by a robust mathematical proof and
compelling experimental results, this research represents a timely and
significant contribution to the current discourse in signal, and audio
processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1 figure, including 3 sub-figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04596v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04596v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Polák, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, Juan Pino, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by
the broadening interests of the spoken language translation community.
ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2)
simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech
translation (S2ST) -- each task is supported with a wide variety of approaches,
differentiating ESPnet-ST-v2 from other open source spoken language translation
toolkits. This toolkit offers state-of-the-art architectures such as
transducers, hybrid CTC/attention, multi-decoders with searchable
intermediates, time-synchronous blockwise CTC/attention, Translatotron models,
and direct discrete unit models. In this paper, we describe the overall design,
example models for each task, and performance benchmarking behind ESPnet-ST-v2,
which is publicly available at https://github.com/espnet/espnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023; System Demonstration</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">27</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Power-Aperture Resource Allocation for a MPAR with Communications
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Augusto Aubry, Antonio De Maio, Luca Pallotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multifunction phased array radars (MPARs) exploit the intrinsic flexibility
of their active electronically steered array (ESA) to perform, at the same
time, a multitude of operations, such as search, tracking, fire control,
classification, and communications. This paper aims at addressing the MPAR
resource allocation so as to satisfy the quality of service (QoS) demanded by
both line of sight (LOS) and non line of sight (NLOS) search operations along
with communications tasks. To this end, the ranges at which the cumulative
detection probability and the channel capacity per bandwidth reach a desired
value are introduced as task quality metrics for the search and communication
functions, respectively. Then, to quantify the satisfaction level of each task,
for each of them a bespoke utility function is defined to map the associated
quality metric into the corresponding perceived utility. Hence, assigning
different priority weights to each task, the resource allocation problem, in
terms of radar power aperture (PAP) specification, is formulated as a
constrained optimization problem whose solution optimizes the global radar QoS.
Several simulations are conducted in scenarios of practical interest to prove
the effectiveness of the approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Knowledge-Data Driven Channel Semantic Acquisition and
  Beamforming for Cell-Free Massive MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Gao, Shicong Liu, Yu Su, Zhongxiang Li, Dezhi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on advancing outdoor wireless systems to better support
ubiquitous extended reality (XR) applications, and close the gap with current
indoor wireless transmission capabilities. We propose a hybrid knowledge-data
driven method for channel semantic acquisition and multi-user beamforming in
cell-free massive multiple-input multiple-output (MIMO) systems. Specifically,
we firstly propose a data-driven multiple layer perceptron (MLP)-Mixer-based
auto-encoder for channel semantic acquisition, where the pilot signals, CSI
quantizer for channel semantic embedding, and CSI reconstruction for channel
semantic extraction are jointly optimized in an end-to-end manner. Moreover,
based on the acquired channel semantic, we further propose a knowledge-driven
deep-unfolding multi-user beamformer, which is capable of achieving good
spectral efficiency with robustness to imperfect CSI in outdoor XR scenarios.
By unfolding conventional successive over-relaxation (SOR)-based linear
beamforming scheme with deep learning, the proposed beamforming scheme is
capable of adaptively learning the optimal parameters to accelerate convergence
and improve the robustness to imperfect CSI. The proposed deep unfolding
beamforming scheme can be used for access points (APs) with fully-digital array
and APs with hybrid analog-digital array structure. Simulation results
demonstrate the effectiveness of our proposed scheme in improving the accuracy
of channel acquisition, as well as reducing complexity in both CSI acquisition
and beamformer design. The proposed beamforming method achieves approximately
96% of the converged spectrum efficiency performance after only three
iterations in downlink transmission, demonstrating its efficacy and potential
to improve outdoor XR applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid End-to-End Spatio-Temporal Attention Neural Network with
  Graph-Smooth Signals for EEG Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadi Sartipi, Mastaneh Torkamani-Azar, Mujdat Cetin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, physiological data such as electroencephalography (EEG) signals
have attracted significant attention in affective computing. In this context,
the main goal is to design an automated model that can assess emotional states.
Lately, deep neural networks have shown promising performance in emotion
recognition tasks. However, designing a deep architecture that can extract
practical information from raw data is still a challenge. Here, we introduce a
deep neural network that acquires interpretable physiological representations
by a hybrid structure of spatio-temporal encoding and recurrent attention
network blocks. Furthermore, a preprocessing step is applied to the raw data
using graph signal processing tools to perform graph smoothing in the spatial
domain. We demonstrate that our proposed architecture exceeds state-of-the-art
results for emotion classification on the publicly available DEAP dataset. To
explore the generality of the learned model, we also evaluate the performance
of our architecture towards transfer learning (TL) by transferring the model
parameters from a specific source to other target domains. Using DEAP as the
source dataset, we demonstrate the effectiveness of our model in performing
cross-modality TL and improving emotion classification accuracy on DREAMER and
the Emotional English Word (EEWD) datasets, which involve EEG-based emotion
classification tasks with different stimuli.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Analysis and Approximate Message Passing Detection of
  Orthogonal Time Sequency Multiplexing Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeping Sui, Shefeng Yan, Hongming Zhang, Sumei Sun, Yonghong Zeng, Lie-Liang Yang, Lajos Hanzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In orthogonal time sequency multiplexing (OTSM) modulation, the information
symbols are conveyed in the delay-sequency domain upon exploiting the inverse
Walsh Hadamard transform (IWHT). It has been shown that OTSM is capable of
attaining a bit error ratio (BER) similar to that of orthogonal time-frequency
space (OTFS) modulation at a lower complexity, since the saving of
multiplication operations in the IWHT. Hence we provide its BER performance
analysis and characterize its detection complexity. We commence by deriving its
generalized input-output relationship and its unconditional pairwise error
probability (UPEP). Then, its BER upper bound is derived in closed form under
both ideal and imperfect channel estimation conditions, which is shown to be
tight at moderate to high signal-to-noise ratios (SNRs). Moreover, a novel
approximate message passing (AMP) aided OTSM detection framework is proposed.
Specifically, to circumvent the high residual BER of the conventional AMP
detector, we proposed a vector AMP-based expectation-maximization (VAMP-EM)
detector for performing joint data detection and noise variance estimation. The
variance auto-tuning algorithm based on the EM algorithm is designed for the
VAMP-EM detector to further improve the convergence performance. The simulation
results illustrate that the VAMP-EM detector is capable of striking an
attractive BER vs. complexity trade-off than the state-of-the-art schemes as
well as providing a better convergence. Finally, we propose AMP and VAMP-EM
turbo receivers for low-density parity-check (LDPC)-coded OTSM systems. It is
demonstrated that our proposed VAMP-EM turbo receiver is capable of providing
both BER and convergence performance improvements over the conventional AMP
solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV Swarms for Joint Data Ferrying and Dynamic Cell Coverage via Optimal
  Transport Descent and Quadratic Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Cui, Lars Baumgärtner, Burak Yilmaz, Mengguang Li, Christian Fabian, Benjamin Becker, Lin Xiang, Maximilian Bauer, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both data ferrying with disruption-tolerant networking (DTN) and mobile
cellular base stations constitute important techniques for UAV-aided
communication in situations of crises where standard communication
infrastructure is unavailable. For optimal use of a limited number of UAVs, we
propose providing both DTN and a cellular base station on each UAV. Here, DTN
is used for large amounts of low-priority data, while capacity-constrained cell
coverage remains reserved for emergency calls or command and control. We
optimize cell coverage via a novel optimal transport-based formulation using
alternating minimization, while for data ferrying we periodically deliver data
between dynamic clusters by solving quadratic assignment problems. In our
evaluation, we consider different scenarios with varying mobility models and a
wide range of flight patterns. Overall, we tractably achieve optimal cell
coverage under quality-of-service costs with DTN-based data ferrying, enabling
large-scale deployment of UAV swarms for crisis communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE LCN 2023 as full paper, pre-final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-time Human Pose Estimation Approach for Optimal Sensor Placement
  in Sensor-based Human Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orhan Konak, Alexander Wischmann, Robin van de Water, Bert Arnrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sensor-based Human Activity Recognition facilitates unobtrusive monitoring of
human movements. However, determining the most effective sensor placement for
optimal classification performance remains challenging. This paper introduces a
novel methodology to resolve this issue, using real-time 2D pose estimations
derived from video recordings of target activities. The derived skeleton data
provides a unique strategy for identifying the optimal sensor location. We
validate our approach through a feasibility study, applying inertial sensors to
monitor 13 different activities across ten subjects. Our findings indicate that
the vision-based method for sensor placement offers comparable results to the
conventional deep learning approach, demonstrating its efficacy. This research
significantly advances the field of Human Activity Recognition by providing a
lightweight, on-device solution for determining the optimal sensor placement,
thereby enhancing data anonymization and supporting a multimodal classification
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta Federated Reinforcement Learning for Distributed Resource
  Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Ji, Zhijin Qin, Xiaoming Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cellular networks, resource allocation is usually performed in a
centralized way, which brings huge computation complexity to the base station
(BS) and high transmission overhead. This paper explores a distributed resource
allocation method that aims to maximize energy efficiency (EE) while ensuring
the quality of service (QoS) for users. Specifically, in order to address
wireless channel conditions, we propose a robust meta federated reinforcement
learning (\textit{MFRL}) framework that allows local users to optimize transmit
power and assign channels using locally trained neural network models, so as to
offload computational burden from the cloud server to the local users, reducing
transmission overhead associated with local channel state information. The BS
performs the meta learning procedure to initialize a general global model,
enabling rapid adaptation to different environments with improved EE
performance. The federated learning technique, based on decentralized
reinforcement learning, promotes collaboration and mutual benefits among users.
Analysis and numerical results demonstrate that the proposed \textit{MFRL}
framework accelerates the reinforcement learning process, decreases
transmission overhead, and offloads computation, while outperforming the
conventional decentralized reinforcement learning algorithm in terms of
convergence speed and EE performance across various scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TWC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cell-Free XL-MIMO Meets Multi-Agent Reinforcement Learning:
  Architectures, Challenges, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilong Liu, Jiayi Zhang, Ziheng Liu, Hongyang Du, Zhe Wang, Dusit Niyato, Mohsen Guizani, Bo Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cell-free massive multiple-input multiple-output (mMIMO) and extremely
large-scale MIMO (XL-MIMO) are regarded as promising innovations for the
forthcoming generation of wireless communication systems. Their significant
advantages in augmenting the number of degrees of freedom have garnered
considerable interest. In this article, we first review the essential
opportunities and challenges induced by XL-MIMO systems. We then propose the
enhanced paradigm of cell-free XL-MIMO, which incorporates multi-agent
reinforcement learning (MARL) to provide a distributed strategy for tackling
the problem of high-dimension signal processing and costly energy consumption.
Based on the unique near-field characteristics, we propose two categories of
the low-complexity design, i.e., antenna selection and power control, to adapt
to different cell-free XL-MIMO scenarios and achieve the maximum data rate. For
inspiration, several critical future research directions pertaining to green
cell-free XL-MIMO systems are presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Singular-value-based Marker for the Detection of Atrial Fibrillation
  Using High-resolution Electrograms and Multi-lead ECG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanie Moghaddasi, Richard C. Hendriks, Borbala Hunyadi, Paul Knops, Mathijs S van Schie, Natasja M. S. de Groot, Alle-Jan van der Veen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The severity of atrial fibrillation (AF) can be assessed from intra-operative
epicardial measurements (high-resolution electrograms), using metrics such as
conduction block (CB) and continuous conduction delay and block (cCDCB). These
features capture differences in conduction velocity and wavefront propagation.
However, they do not clearly differentiate patients with various degrees of AF
while they are in sinus rhythm, and complementary features are needed. In this
work, we focus on the morphology of the action potentials, and derive features
to detect variations in the atrial potential waveforms. Methods: We show that
the spatial variation of atrial potential morphology during a single beat may
be described by changes in the singular values of the epicardial measurement
matrix. The method is non-parametric and requires little preprocessing. A
corresponding singular value map points at areas subject to fractionation and
block. Further, we developed an experiment where we simultaneously measure
electrograms (EGMs) and a multi-lead ECG. Results: The captured data showed
that the normalized singular values of the heartbeats during AF are higher than
during SR, and that this difference is more pronounced for the (non-invasive)
ECG data than for the EGM data, if the electrodes are positioned at favorable
locations. Conclusion: Overall, the singular value-based features are a useful
indicator to detect and evaluate AF. Significance: The proposed method might be
beneficial for identifying electropathological regions in the tissue without
estimating the local activation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Spatial-Wideband Effects in Millimeter-Wave Cell-Free Massive
  MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyoung Ahn, Soohyeong Kim, Yongseok Kwon, Joohan Park, Jiseung Youn, Sunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the spatial-wideband effects in cell-free
massive MIMO (CF-mMIMO) systems in mmWave bands. The utilization of mmWave
frequencies brings challenges such as signal attenuation and the need for
denser networks like ultra-dense networks (UDN) to maintain communication
performance. CF-mMIMO is introduced as a solution, where distributed access
points (APs) transmit signals to a central processing unit (CPU) for joint
processing. CF-mMIMO offers advantages in reducing non-line-of-sight (NLOS)
conditions and overcoming signal blockage. We investigate the synchronization
problem in CF-mMIMO due to time delays between APs. It proposes a minimum
cyclic prefix length to mitigate inter-symbol interference (ISI) in OFDM
systems. Furthermore, the spatial correlations of channel responses are
analyzed in the frequency-phase domain. The impact of these correlations on
system performance is examined. The findings contribute to improving the
performance of CF-mMIMO systems and enhancing the effective utilization of
mmWave communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Empowered Autonomous Edge AI for Connected
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Shen, Jiawei Shao, Xinjie Zhang, Zehong Lin, Hao Pan, Dongsheng Li, Jun Zhang, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of wireless networks gravitates towards connected intelligence,
a concept that envisions seamless interconnectivity among humans, objects, and
intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a
promising solution to achieve connected intelligence by delivering
high-quality, low-latency, and privacy-preserving AI services at the network
edge. In this article, we introduce an autonomous edge AI system that
automatically organizes, adapts, and optimizes itself to meet users' diverse
requirements. The system employs a cloud-edge-client hierarchical architecture,
where the large language model, i.e., Generative Pretrained Transformer (GPT),
resides in the cloud, and other AI models are co-deployed on devices and edge
servers. By leveraging the powerful abilities of GPT in language understanding,
planning, and code generation, we present a versatile framework that
efficiently coordinates edge AI models to cater to users' personal demands
while automatically generating code to train new models via edge federated
learning. Experimental results demonstrate the system's remarkable ability to
accurately comprehend user demands, efficiently execute AI models with minimal
cost, and effectively create high-performance AI models through federated
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Magazine paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multi-time Scale User Admission and Resource Allocation for
  Semantic Extraction in MEC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanpeng Zheng, Tiankui Zhang, Jonathan Loo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the semantic extraction task-oriented dynamic
multi-time scale user admission and resourceallocation in mobile edge computing
(MEC) systems. Amid prevalence artifi cial intelligence applications in various
industries,the offloading of semantic extraction tasks which are mainlycomposed
of convolutional neural networks of computer vision isa great challenge for
communication bandwidth and computing capacity allocation in MEC systems.
Considering the stochasticnature of the semantic extraction tasks, we formulate
a stochastic optimization problem by modeling it as the dynamic arrival of
tasks in the temporal domain. We jointly optimize the system revenue and cost
which are represented as user admission in the long term and resource
allocation in the short term respectively. To handle the proposed stochastic
optimization problem, we decompose it into short-time-scale subproblems and a
long-time-scale subproblem by using the Lyapunov optimization technique. After
that, the short-time-scale optimization variables of resource allocation,
including user association, bandwidth allocation, and computing capacity
allocation are obtained in closed form. The user admission optimization on
long-time scales is solved by a heuristic iteration method. Then, the
multi-time scale user admission and resource allocation algorithm is proposed
for dynamic semantic extraction task computing in MEC systems. Simulation
results demonstrate that, compared with the benchmarks, the proposed algorithm
improves the performance of user admission and resource allocation efficiently
and achieves a flexible trade-off between system revenue and cost at multi-time
scales and considering semantic extraction tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing Offloading and Semantic Compression for Intelligent Computing
  Tasks in MEC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanpeng Zheng, Tiankui Zhang, Rong Huang, Yapeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the intelligent computing task-oriented computing
offloading and semantic compression in mobile edge computing (MEC) systems.
With the popularity of intelligent applications in various industries,
terminals increasingly need to offload intelligent computing tasks with complex
demands to MEC servers for computing, which is a great challenge for bandwidth
and computing capacity allocation in MEC systems. Considering the accuracy
requirement of intelligent computing tasks, we formulate an optimization
problem of computing offloading and semantic compression. We jointly optimize
the system utility which are represented as computing accuracy and task delay
respectively to acquire the optimized system utility. To solve the proposed
optimization problem, we decompose it into computing capacity allocation
subproblem and compression offloading subproblem and obtain solutions through
convex optimization and successive convex approximation. After that, the
offloading decisions, computing capacity and compressed ratio are obtained in
closed forms. We design the computing offloading and semantic compression
algorithm for intelligent computing tasks in MEC systems then. Simulation
results represent that our algorithm converges quickly and acquires better
performance and resource utilization efficiency through the trend with total
number of users and computing capacity compared with benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALPCAH: Sample-wise Heteroscedastic PCA with Tail Singular Value
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Salazar Cavazos, Jeffrey A. Fessler, Laura Balzano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal component analysis (PCA) is a key tool in the field of data
dimensionality reduction that is useful for various data science problems.
However, many applications involve heterogeneous data that varies in quality
due to noise characteristics associated with different sources of the data.
Methods that deal with this mixed dataset are known as heteroscedastic methods.
Current methods like HePPCAT make Gaussian assumptions of the basis
coefficients that may not hold in practice. Other methods such as Weighted PCA
(WPCA) assume the noise variances are known, which may be difficult to know in
practice. This paper develops a PCA method that can estimate the sample-wise
noise variances and use this information in the model to improve the estimate
of the subspace basis associated with the low-rank structure of the data. This
is done without distributional assumptions of the low-rank component and
without assuming the noise variances are known. Simulations show the
effectiveness of accounting for such heteroscedasticity in the data, the
benefits of using such a method with all of the data versus retaining only good
data, and comparisons are made against other PCA methods established in the
literature like PCA, Robust PCA (RPCA), and HePPCAT. Code available at
https://github.com/javiersc1/ALPCAH
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in the Fourteenth
  International Conference on Sampling Theory and Applications, accessible via
  IEEE XPlore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Massive MIMO with Cauchy Noise: Channel Estimation, Achievable Rate and
  Data Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziya Gulgun, Erik G. Larsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider massive multiple-input multiple-output (MIMO) systems in the
presence of Cauchy noise. First, we focus on the channel estimation problem. In
the standard massive MIMO setup, the users transmit orthonormal pilots during
the training phase and the received signal at the base station is projected
onto each pilot. This processing is optimum when the noise is Gaussian. We show
that this processing is not optimal when the noise is Cauchy and as a remedy
propose a channel estimation technique that operates on the raw received
signal. Second, we derive uplink-downlink achievable rates in the presence of
Cauchy noise for perfect and imperfect channel state information. Finally, we
derive log-likelihood ratio expressions for soft bit detection for both uplink
and downlink, and simulate coded bit-error-rate curves. In addition to this, we
derive and compare the symbol detectors in the presence of both Gaussian and
Cauchy noises. An important observation is that the detector constructed for
Cauchy noise performs well with both Gaussian and Cauchy noises; on the other
hand, the detector for Gaussian noise works poorly in the presence of Cauchy
noise. That is, the Cauchy detector is robust against heavy-tailed noise,
whereas the Gaussian detector is not.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the IEEE Transactions on Wireless Communications, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Touch, press and stroke: a soft capacitive sensor skin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirza S. Sarwar, Ryusuke Ishizaki, Kieran Morton, Claire Preston, Tan Nguyen, Xu Fan, Bertille Dupont, Leanna Hogarth, Takahide Yoshiike, Shahriar Mirabbasi, John D. W. Madden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft sensors that can discriminate shear and normal force could help provide
machines the fine control desirable for safe and effective physical
interactions with people. A capacitive sensor is made for this purpose,
composed of patterned elastomer and containing both fixed and sliding pillars
that allow the sensor to deform and buckle, much like skin itself. The sensor
differentiates between simultaneously applied pressure and shear. In addition,
finger proximity is detectable up to 15 mm, with a pressure and shear
sensitivity of 1 kPa and a displacement resolution of 50 $\mu$m. The operation
is demonstrated on a simple gripper holding a cup. The combination of features
and the straightforward fabrication method make this sensor a candidate for
implementation as a sensing skin for humanoid robotics applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, submitted to Scientific Reports Nature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoder-Decoder Networks for <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pretrain</span>ing and Downstream
  Signal Bandwidth Regression on Digital Antenna Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajib Bhattacharjea, Nathan West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents the first applications of self-supervised learning applied
to data from digital antenna arrays. Encoder-decoder networks are pretrained on
digital array data to perform a self-supervised noisy-reconstruction task
called channel in-painting, in which the network infers the contents of array
data that has been masked with zeros. The self-supervised step requires no
human-labeled data. The encoder architecture and weights from pretraining are
then transferred to a new network with a task-specific decoder, and the new
network is trained on a small volume of labeled data. We show that pretraining
on the unlabeled data allows the new network to perform the task of bandwidth
regression on the digital array data better than an equivalent network that is
trained on the same labeled data from random initialization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Computing Offloading and Resource Allocation for Classification
  Intelligent Tasks in MEC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanpeng Zheng, Tiankui Zhang, Jonathan Loo, Yapeng Wang, Arumugam Nallanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile edge computing (MEC) enables low-latency and high-bandwidth
applications by bringing computation and data storage closer to end-users.
Intelligent computing is an important application of MEC, where computing
resources are used to solve intelligent task-related problems based on task
requirements. However, efficiently offloading computing and allocating
resources for intelligent tasks in MEC systems is a challenging problem due to
complex interactions between task requirements and MEC resources. To address
this challenge, we investigate joint computing offloading and resource
allocation for intelligent tasks in MEC systems. Our goal is to optimize system
utility by jointly considering computing accuracy and task delay to achieve
maximum system performance. We focus on classification intelligence tasks and
formulate an optimization problem that considers both the accuracy requirements
of tasks and the parallel computing capabilities of MEC systems. To solve the
optimization problem, we decompose it into three subproblems: subcarrier
allocation, computing capacity allocation, and compression offloading. We use
convex optimization and successive convex approximation to derive closed-form
expressions for the subcarrier allocation, offloading decisions, computing
capacity, and compressed ratio. Based on our solutions, we design an efficient
computing offloading and resource allocation algorithm for intelligent tasks in
MEC systems. Our simulation results demonstrate that our proposed algorithm
significantly improves the performance of intelligent tasks in MEC systems and
achieves a flexible trade-off between system revenue and cost considering
intelligent tasks compared with the benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2307.02747</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Kernels for Interpretable and Efficient PPG Signal Quality
  Assessment and Artifact Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sully F. Chen, Zhicheng Guo, Cheng Ding, Xiao Hu, Cynthia Rudin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoplethysmography (PPG) provides a low-cost, non-invasive method to
continuously monitor various cardiovascular parameters. PPG signals are
generated by wearable devices and frequently contain large artifacts caused by
external factors, such as motion of the human subject. In order to ensure
robust and accurate extraction of physiological parameters, corrupted areas of
the signal need to be identified and handled appropriately. Previous
methodology relied either on handcrafted feature detectors or signal metrics
which yield sub-optimal performance, or relied on machine learning techniques
such as deep neural networks (DNN) which lack interpretability and are
computationally and memory intensive. In this work, we present a novel method
to learn a small set of interpretable convolutional kernels that has
performance similar to -- and often better than -- the state-of-the-art DNN
approach with several orders of magnitude fewer parameters. This work allows
for efficient, robust, and interpretable signal quality assessment and artifact
segmentation on low-power devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive
  signals and human language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuwa Xi, Sendong Zhao, Haochun Wang, Chi Liu, Bing Qin, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our
understanding of the human language system, paving the way for building
versatile Brain-Computer Interface. However, existing studies largely focus on
decoding individual word-level fMRI volumes from a restricted vocabulary, which
is far too idealized for real-world application. In this paper, we propose
fMRI2text, the first openvocabulary task aiming to bridge fMRI time series and
human language. Furthermore, to explore the potential of this new task, we
present a baseline solution, UniCoRN: the Unified Cognitive Signal
ReconstructioN for Brain Decoding. By reconstructing both individual time
points and time series, UniCoRN establishes a robust encoder for cognitive
signals (fMRI & EEG). Leveraging a pre-trained language model as decoder,
UniCoRN proves its efficacy in decoding coherent text from fMRI series across
various split settings. Our model achieves a 34.77% BLEU score on fMRI2text,
and a 37.04% BLEU when generalized to EEGto-text decoding, thereby surpassing
the former baseline. Experimental results indicate the feasibility of decoding
consecutive fMRI volumes, and the effectiveness of decoding different cognitive
signals using a unified structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the 61st Annual Meeting of the Association for Computational
  Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pattern and Polarization Diversity Multi-Sector Annular Antenna for IoT
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abel Zandamela, Nicola Marchetti, Max J. Ammann, Adam Narbudowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a small pattern and polarization diversity multi-sector
annular antenna with electrical size and profile of ${ka=1.2}$ and
${0.018\lambda}$, respectively. The antenna is planar and comprises annular
sectors that are fed using different ports to enable digital beamforming
techniques, with efficiency and gain of up to 78% and 4.62 dBi, respectively.
The cavity mode analysis is used to describe the design concept and the antenna
diversity. The proposed method can produce different polarization states (e.g.
linearly and circularly polarized patterns), and pattern diversity
characteristics covering the elevation plane. Owing to its small electrical
size, low-profile and diversity properties, the solution shows good promise to
enable advanced radio applications like wireless physical layer security in
many emerging and size-constrained Internet of Things (IoT) devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Antennas and Propagation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Noise Control in The New Century: The Role and Prospect of Signal
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyuan Shi, Bhan Lam, Woon-Seng Gan, Jordan Cheer, Stephen J. Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since Paul Leug's 1933 patent application for a system for the active control
of sound, the field of active noise control (ANC) has not flourished until the
advent of digital signal processors forty years ago. Early theoretical
advancements in digital signal processing and processors laid the groundwork
for the phenomenal growth of the field, particularly over the past
quarter-century. The widespread commercial success of ANC in aircraft cabins,
automobile cabins, and headsets demonstrates the immeasurable public health and
economic benefits of ANC. This article continues where Elliott and Nelson's
1993 Signal Processing Magazine article and Elliott's 1997 50th anniversary
commentary on ANC left off, tracing the technical developments and applications
in ANC spurred by the seminal texts of Nelson and Elliott (1991), Kuo and
Morgan (1996), Hansen and Snyder (1996), and Elliott (2001) since the turn of
the century. This article focuses on technical developments pertaining to
real-world implementations, such as improving algorithmic convergence, reducing
system latency, and extending control to non-stationary and/or broadband noise,
as well as the commercial transition challenges from analog to digital ANC
systems. Finally, open issues and the future of ANC in the era of artificial
intelligence are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to inter.noise 2023, Chiba, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Saving Precoder Design for Narrowband and Wideband Massive MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Peschiera, François Rottenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study massive multiple-input multiple-output (MIMO)
precoders optimizing power consumption while achieving the users' rate
requirements. We first characterize analytically the solutions for narrowband
and wideband systems minimizing the power amplifiers (PAs) consumption in low
system load, where the per-antenna power constraints are not binding. After, we
focus on the asymptotic wideband regime. The power consumed by the whole base
station (BS) and the high-load scenario are then also investigated. We obtain
simple solutions, and the optimal strategy in the asymptotic case reduces to
finding the optimal number of active antennas, relying on known precoders among
the active antennas. Numerical results show that large savings in power
consumption are achievable in the narrowband system by employing antenna
selection, while all antennas need to be activated in the wideband system when
considering only the PAs consumption, and this implies lower savings. When
considering the overall BS power consumption and a large number of subcarriers,
we show that significant savings are achievable in the low-load regime by using
a subset of the BS antennas. While optimization based on transmit power pushes
to activate all antennas, optimization based on consumed power activates a
number of antennas proportional to the load.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Transactions on Green Communications and Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Prediction of Gas Flow Dynamics in Diesel Engines using a Deep
  Neural Operator Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Kumar, Somdatta Goswami, Daniel J. Smith, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a data-driven deep neural operator framework to approximate
multiple output states for a diesel engine and generate real-time predictions
with reasonable accuracy. As emission norms become more stringent, the need for
fast and accurate models that enable analysis of system behavior have become an
essential requirement for system development. The fast transient processes
involved in the operation of a combustion engine make it difficult to develop
accurate physics-based models for such systems. As an alternative to physics
based models, we develop an operator-based regression model (DeepONet) to learn
the relevant output states for a mean-value gas flow engine model using the
engine operating conditions as input variables. We have adopted a mean-value
model as a benchmark for comparison, simulated using Simulink. The developed
approach necessitates using the initial conditions of the output states to
predict the accurate sequence over the temporal domain. To this end, a
sequence-to-sequence approach is embedded into the proposed framework. The
accuracy of the model is evaluated by comparing the prediction output to ground
truth generated from Simulink model. The maximum $\mathcal L_2$ relative error
observed was approximately $6.5\%$. The sensitivity of the DeepONet model is
evaluated under simulated noise conditions and the model shows relatively low
sensitivity to noise. The uncertainty in model prediction is further assessed
by using a mean ensemble approach. The worst-case error at the $(\mu +
2\sigma)$ boundary was found to be $12\%$. The proposed framework provides the
ability to predict output states in real-time and enables data-driven learning
of complex input-output operator mapping. As a result, this model can be
applied during initial development stages, where accurate models may not be
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated manuscript title to better reflect this work and field of
  study</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single-Input Polarization-Sensitive Optical Coherence Tomography Through
  a Catheter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09517v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09517v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgia L. Jones, Qiaozhou Xiong, Xinyu Liu, Brett E. Bouma, Martin Villiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intravascular polarimetry with catheter-based polarization-sensitive optical
coherence tomography (PS-OCT) complements the high-resolution structural
tomograms of OCT with morphological contrast available through polarimetry. Its
clinical translation has been complicated by the need for modification of
conventional OCT hardware to enable polarimetric measurements. Here, we present
a signal processing method to reconstruct polarization properties of tissue
from measurements with a single input polarization state, bypassing the need
for modulation or multiplexing of input states. Our method relies on a
polarization symmetry intrinsic to round-trip measurements and uses the
residual spectral variation of the polarization states incident on the tissue
to avoid measurement ambiguities. We demonstrate depth-resolved birefringence
and optic axis orientation maps reconstructed from in-vivo data of human
coronary arteries. We validate our method through comparison with conventional
dual-input state measurements and find a mean cumulative retardance error of
13.2deg without observable bias. The 95% limit of agreement between
depth-resolved birefringence is 2.80 x 10^(-4), which is less than the
agreement between two repeat pullbacks of conventional PS-OCT (3.14 x 10^(-4)),
indicating that the two methods can be used interchangeably. The hardware
simplification arising from using a single input state may be decisive in
realizing the potential of polarimetric measurements for assessing coronary
atherosclerosis in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-invariant Prefix Coding for LQG Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00588v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00588v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Travis C. Cuvelier, Takashi Tanaka, Robert W. Heath Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by control with communication constraints, in this work we develop
a time-invariant data compression architecture for linear-quadratic-Gaussian
(LQG) control with minimum bitrate prefix-free feedback. For any fixed control
performance, the approach we propose nearly achieves known directed information
(DI) lower bounds on the time-average expected codeword length. We refine the
analysis of a classical achievability approach, which required quantized plant
measurements to be encoded via a time-varying lossless source code. We prove
that the sequence of random variables describing the quantizations has a
limiting distribution and that the quantizations may be encoded with a fixed
source code optimized for this distribution without added time-asymptotic
redundancy. Our result follows from analyzing the long-term stochastic behavior
of the system, and permits us to additionally guarantee that the time-average
codeword length (as opposed to expected length) is almost surely within a few
bits of the minimum DI. To our knowledge, this time-invariant achievability
result is the first in the literature.
  The originally published version of the supplementary material included a
proof that contained an error that turned out to be inconsequential. This
updated preprint corrects this error, which originally appeared under Lemma
A.7.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version as accepted to the IEEE Journal on Selected Areas in
  Information Theory (Special Issue on Modern Compression), modulo an
  additional correction to the proof of Lemma A.7. Official version:
  https://ieeexplore.ieee.org/document/10002900. 14 page main paper, 4 pages
  appendix, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The feasibility of Q-band millimeter wave on hand-gesture recognition
  for indoor FTTR scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Hu, Zhaoyang Xia, Yanbo Zhao, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalization for different scenarios and dif-ferent users is an urgent
problem for millimeter wave gesture recognition for indoor fiber-to-the-room
(FTTR) scenario. In order to solve this problem and verify the feasibility of
FTTR Q-band millimeter wave in gesture recognition, we build a real-time
millimeter wave gesture recognition system. The moving hand-gestures are
represented as a variety of time-variant spec-trum features, such as
micro-Doppler feature, and then the feature learning and classification is
realized by using a convo-lution neural network (CNN). The experimental results
show that the millimeter wave gesture recognition system can achieve the
generalized gesture recognition for 2 scenarios and 4 users.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Configuration and Management of WiFi Direct Groups for
  Real Opportunistic Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Arnaboldi, Mattia Giovanni Campana, Franca Delmastro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wi-Fi Direct is a promising technology for the support of device-to-device
communications (D2D) on commercial mobile devices. However, the standard
as-it-is is not sufficient to support the real deployment of networking
solutions entirely based on D2D such as opportunistic networks. In fact, WiFi
Direct presents some characteristics that could limit the autonomous creation
of D2D connections among users' personal devices. Specifically, the standard
explicitly requires the user's authorization to establish a connection between
two or more devices, and it provides a limited support for inter-group
communication. In some cases, this might lead to the creation of isolated
groups of nodes which cannot communicate among each other. In this paper, we
propose a novel middleware-layer protocol for the efficient configuration and
management of WiFi Direct groups (WiFi Direct Group Manager, WFD-GM) to enable
autonomous connections and inter-group communication. This enables
opportunistic networks in real conditions (e.g., variable mobility and network
size). WFD-GM defines a context function that takes into account heterogeneous
parameters for the creation of the best group configuration in a specific time
window, including an index of nodes' stability and power levels. We evaluate
the protocol performances by simulating three reference scenarios including
different mobility models, geographical areas and number of nodes. Simulations
are also supported by experimental results related to the evaluation in a real
testbed of the involved context parameters. We compare WFD-GM with the
state-of-the-art solutions and we show that it performs significantly better
than a Baseline approach in scenarios with medium/low mobility, and it is
comparable with it in case of high mobility, without introducing additional
overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the IEEE 14th International Conference on Mobile Ad Hoc
  and Sensor Systems (MASS), 2017</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV Swarms for Joint Data Ferrying and Dynamic Cell Coverage via Optimal
  Transport Descent and Quadratic Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Cui, Lars Baumgärtner, Burak Yilmaz, Mengguang Li, Christian Fabian, Benjamin Becker, Lin Xiang, Maximilian Bauer, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both data ferrying with disruption-tolerant networking (DTN) and mobile
cellular base stations constitute important techniques for UAV-aided
communication in situations of crises where standard communication
infrastructure is unavailable. For optimal use of a limited number of UAVs, we
propose providing both DTN and a cellular base station on each UAV. Here, DTN
is used for large amounts of low-priority data, while capacity-constrained cell
coverage remains reserved for emergency calls or command and control. We
optimize cell coverage via a novel optimal transport-based formulation using
alternating minimization, while for data ferrying we periodically deliver data
between dynamic clusters by solving quadratic assignment problems. In our
evaluation, we consider different scenarios with varying mobility models and a
wide range of flight patterns. Overall, we tractably achieve optimal cell
coverage under quality-of-service costs with DTN-based data ferrying, enabling
large-scale deployment of UAV swarms for crisis communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE LCN 2023 as full paper, pre-final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Deployment and Resource Allocation for Robotic Aerial Base
  Station Enabled OFDM Integrated Sensing and Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Liao, Vasilis Friderikos, Halim Yanikomeroglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The envisioned robotic aerial base station (RABS) concept is expected to
bring further flexibility to integrated sensing and communication (ISAC)
systems. In this letter, characterizing the spatial traffic distribution on a
grid-based model, the RABS-assisted ISAC system is formulated as a robust
optimization problem to maximize the minimum satisfaction rate (SR) under a
cardinality constrained uncertainty set. The problem is reformulated as a
mixed-integer linear programming (MILP) and solved approximately by the
iterative linear programming rounding algorithm. Numerical investigations show
that the minimum SR can be improved by 28.61% on average compared to fixed
small cells.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Spatial-Wideband Effects in Millimeter-Wave Cell-Free Massive
  MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyoung Ahn, Soohyeong Kim, Yongseok Kwon, Joohan Park, Jiseung Youn, Sunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the spatial-wideband effects in cell-free
massive MIMO (CF-mMIMO) systems in mmWave bands. The utilization of mmWave
frequencies brings challenges such as signal attenuation and the need for
denser networks like ultra-dense networks (UDN) to maintain communication
performance. CF-mMIMO is introduced as a solution, where distributed access
points (APs) transmit signals to a central processing unit (CPU) for joint
processing. CF-mMIMO offers advantages in reducing non-line-of-sight (NLOS)
conditions and overcoming signal blockage. We investigate the synchronization
problem in CF-mMIMO due to time delays between APs. It proposes a minimum
cyclic prefix length to mitigate inter-symbol interference (ISI) in OFDM
systems. Furthermore, the spatial correlations of channel responses are
analyzed in the frequency-phase domain. The impact of these correlations on
system performance is examined. The findings contribute to improving the
performance of CF-mMIMO systems and enhancing the effective utilization of
mmWave communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Empowered Autonomous Edge AI for Connected
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Shen, Jiawei Shao, Xinjie Zhang, Zehong Lin, Hao Pan, Dongsheng Li, Jun Zhang, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of wireless networks gravitates towards connected intelligence,
a concept that envisions seamless interconnectivity among humans, objects, and
intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a
promising solution to achieve connected intelligence by delivering
high-quality, low-latency, and privacy-preserving AI services at the network
edge. In this article, we introduce an autonomous edge AI system that
automatically organizes, adapts, and optimizes itself to meet users' diverse
requirements. The system employs a cloud-edge-client hierarchical architecture,
where the large language model, i.e., Generative Pretrained Transformer (GPT),
resides in the cloud, and other AI models are co-deployed on devices and edge
servers. By leveraging the powerful abilities of GPT in language understanding,
planning, and code generation, we present a versatile framework that
efficiently coordinates edge AI models to cater to users' personal demands
while automatically generating code to train new models via edge federated
learning. Experimental results demonstrate the system's remarkable ability to
accurately comprehend user demands, efficiently execute AI models with minimal
cost, and effectively create high-performance AI models through federated
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Magazine paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-driven Intelligent Control and Orchestration in O-RAN Via
  Hierarchical Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Arafat Habib, Hao Zhou, Pedro Enrique Iturria-Rivera, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  rApps and xApps need to be controlled and orchestrated well in the open radio
access network (O-RAN) so that they can deliver a guaranteed network
performance in a complex multi-vendor environment. This paper proposes a novel
intent-driven intelligent control and orchestration scheme based on
hierarchical reinforcement learning (HRL). The proposed scheme can orchestrate
multiple rApps or xApps according to the operator's intent of optimizing
certain key performance indicators (KPIs), such as throughput, energy
efficiency, and latency. Specifically, we propose a bi-level architecture with
a meta-controller and a controller. The meta-controller provides the target
performance in terms of KPIs, while the controller performs xApp orchestration
at the lower level. Our simulation results show that the proposed HRL-based
intent-driven xApp orchestration mechanism achieves 7.5% and 21.4% increase in
average system throughput with respect to two baselines, i.e., a single xApp
baseline and a non-machine learning-based algorithm, respectively. Similarly,
17.3% and 37.9% increase in energy efficiency are observed in comparison to the
same baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE MASS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multi-time Scale User Admission and Resource Allocation for
  Semantic Extraction in MEC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanpeng Zheng, Tiankui Zhang, Jonathan Loo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the semantic extraction task-oriented dynamic
multi-time scale user admission and resourceallocation in mobile edge computing
(MEC) systems. Amid prevalence artifi cial intelligence applications in various
industries,the offloading of semantic extraction tasks which are mainlycomposed
of convolutional neural networks of computer vision isa great challenge for
communication bandwidth and computing capacity allocation in MEC systems.
Considering the stochasticnature of the semantic extraction tasks, we formulate
a stochastic optimization problem by modeling it as the dynamic arrival of
tasks in the temporal domain. We jointly optimize the system revenue and cost
which are represented as user admission in the long term and resource
allocation in the short term respectively. To handle the proposed stochastic
optimization problem, we decompose it into short-time-scale subproblems and a
long-time-scale subproblem by using the Lyapunov optimization technique. After
that, the short-time-scale optimization variables of resource allocation,
including user association, bandwidth allocation, and computing capacity
allocation are obtained in closed form. The user admission optimization on
long-time scales is solved by a heuristic iteration method. Then, the
multi-time scale user admission and resource allocation algorithm is proposed
for dynamic semantic extraction task computing in MEC systems. Simulation
results demonstrate that, compared with the benchmarks, the proposed algorithm
improves the performance of user admission and resource allocation efficiently
and achieves a flexible trade-off between system revenue and cost at multi-time
scales and considering semantic extraction tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing Offloading and Semantic Compression for Intelligent Computing
  Tasks in MEC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanpeng Zheng, Tiankui Zhang, Rong Huang, Yapeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the intelligent computing task-oriented computing
offloading and semantic compression in mobile edge computing (MEC) systems.
With the popularity of intelligent applications in various industries,
terminals increasingly need to offload intelligent computing tasks with complex
demands to MEC servers for computing, which is a great challenge for bandwidth
and computing capacity allocation in MEC systems. Considering the accuracy
requirement of intelligent computing tasks, we formulate an optimization
problem of computing offloading and semantic compression. We jointly optimize
the system utility which are represented as computing accuracy and task delay
respectively to acquire the optimized system utility. To solve the proposed
optimization problem, we decompose it into computing capacity allocation
subproblem and compression offloading subproblem and obtain solutions through
convex optimization and successive convex approximation. After that, the
offloading decisions, computing capacity and compressed ratio are obtained in
closed forms. We design the computing offloading and semantic compression
algorithm for intelligent computing tasks in MEC systems then. Simulation
results represent that our algorithm converges quickly and acquires better
performance and resource utilization efficiency through the trend with total
number of users and computing capacity compared with benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Logical Way to Negotiate Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Glenn Bruns, Mauricio Cortes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Service providers commonly provide only a fixed catalog of services to their
clients. Both clients and service providers can benefit from service
negotiation, in which a client makes a query for a specific service, and the
provider counters with an offer. The query could include parameters that
control the performance, reliability, and function of the service. However, a
problem with service negotiation is that it can be expensive for a service
provider to support.
  In this paper we define a formal negotiation policy language that enables
automated service negotiation. In the model supported by the language, service
providers can recursively obtain the services they need from sub-providers. The
queries made by clients, and the offers returned from service providers, are
expressed in quantifier-free first-order logic. Quantifier elimination is used
to transform constraints between providers and sub-providers. The pattern of
interaction between clients and service providers is defined in process
algebra. We show a correctness property of our language: if sub-providers
respond positively to queries, then so does the provider itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Computing Offloading and Resource Allocation for Classification
  Intelligent Tasks in MEC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanpeng Zheng, Tiankui Zhang, Jonathan Loo, Yapeng Wang, Arumugam Nallanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile edge computing (MEC) enables low-latency and high-bandwidth
applications by bringing computation and data storage closer to end-users.
Intelligent computing is an important application of MEC, where computing
resources are used to solve intelligent task-related problems based on task
requirements. However, efficiently offloading computing and allocating
resources for intelligent tasks in MEC systems is a challenging problem due to
complex interactions between task requirements and MEC resources. To address
this challenge, we investigate joint computing offloading and resource
allocation for intelligent tasks in MEC systems. Our goal is to optimize system
utility by jointly considering computing accuracy and task delay to achieve
maximum system performance. We focus on classification intelligence tasks and
formulate an optimization problem that considers both the accuracy requirements
of tasks and the parallel computing capabilities of MEC systems. To solve the
optimization problem, we decompose it into three subproblems: subcarrier
allocation, computing capacity allocation, and compression offloading. We use
convex optimization and successive convex approximation to derive closed-form
expressions for the subcarrier allocation, offloading decisions, computing
capacity, and compressed ratio. Based on our solutions, we design an efficient
computing offloading and resource allocation algorithm for intelligent tasks in
MEC systems. Our simulation results demonstrate that our proposed algorithm
significantly improves the performance of intelligent tasks in MEC systems and
achieves a flexible trade-off between system revenue and cost considering
intelligent tasks compared with the benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2307.02747</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Version Control of Speaker Recognition Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.12069v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.12069v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Wang, Ignacio Lopez Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses one of the most challenging practical engineering
problems in speaker recognition systems - the version control of models and
user profiles. A typical speaker recognition system consists of two stages: the
enrollment stage, where a profile is generated from user-provided enrollment
audio; and the runtime stage, where the voice identity of the runtime audio is
compared against the stored profiles. As technology advances, the speaker
recognition system needs to be updated for better performance. However, if the
stored user profiles are not updated accordingly, version mismatch will result
in meaningless recognition results. In this paper, we describe different
version control strategies for speaker recognition systems that had been
carefully studied at Google from years of engineering practice. These
strategies are categorized into three groups according to how they are deployed
in the production environment: device-side deployment, server-side deployment,
and hybrid deployment. To compare different strategies with quantitative
metrics under various network configurations, we present SpeakerVerSim, an
easily-extensible Python-based simulation framework for different server-side
deployment strategies of speaker recognition systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AGChain: A Blockchain-based Gateway for Trustworthy App Delegation from
  Mobile App Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.06454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.06454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Chen, Xiao Yi, Daoyuan Wu, Jianliang Xu, Yingjiu Li, Debin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of smartphones has led to the growth of mobile app markets,
creating a need for enhanced transparency, global access, and secure
downloading. This paper introduces AGChain, a blockchain-based gateway that
enables trustworthy app delegation within existing markets. AGChain ensures
that markets can continue providing services while users benefit from
permanent, distributed, and secure app delegation. During its development, we
address two key challenges: significantly reducing smart contract gas costs and
enabling fully distributed IPFS-based file storage. Additionally, we tackle
three system issues related to security and sustainability. We have implemented
a prototype of AGChain on Ethereum and Polygon blockchains, achieving effective
security and decentralization with a minimal gas cost of around 0.002 USD per
app upload (no cost for app download). The system also exhibits reasonable
performance with an average overhead of 12%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a technical report submitted to the Special Issue of the
  Elsevier Journal of Systems Architecture (JSA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Explainable AI for 6G O-RAN: Architecture, Use Cases,
  Challenges and Research Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bouziane Brik, Hatim Chergui, Lanfranco Zanzi, Francesco Devoti, Adlen Ksentini, Muhammad Shuaib Siddiqui, Xavier Costa-Pérez, Christos Verikoukis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent O-RAN specifications promote the evolution of RAN architecture by
function disaggregation, adoption of open interfaces, and instantiation of a
hierarchical closed-loop control architecture managed by RAN Intelligent
Controllers (RICs) entities. This paves the road to novel data-driven network
management approaches based on programmable logic. Aided by Artificial
Intelligence (AI) and Machine Learning (ML), novel solutions targeting
traditionally unsolved RAN management issues can be devised. Nevertheless, the
adoption of such smart and autonomous systems is limited by the current
inability of human operators to understand the decision process of such AI/ML
solutions, affecting their trust in such novel tools. eXplainable AI (XAI) aims
at solving this issue, enabling human users to better understand and
effectively manage the emerging generation of artificially intelligent schemes,
reducing the human-to-machine barrier. In this survey, we provide a summary of
the XAI methods and metrics before studying their deployment over the O-RAN
Alliance RAN architecture along with its main building blocks. We then present
various use-cases and discuss the automation of XAI pipelines for O-RAN as well
as the underlying security aspects. We also review some projects/standards that
tackle this area. Finally, we identify different challenges and research
directions that may arise from the heavy adoption of AI/ML decision entities in
this context, focusing on how XAI can help to interpret, understand, and
improve trust in O-RAN operational networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BrickPal: Augmented Reality-based Assembly Instructions for Brick Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Shi, Xiaofeng Zhang, Ran zhang, Zhou Yang, Xiao Tang, Hongni Ye, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The assembly instruction is a mandatory component of Lego-like brick sets.The
conventional production of assembly instructions requires a considerable amount
of manual fine-tuning, which is intractable for casual users and customized
brick sets.Moreover, the traditional paper-based instructions lack
expressiveness and interactivity.To tackle the two problems above, we present
BrickPal, an augmented reality-based system, which visualizes assembly
instructions in an augmented reality head-mounted display. It utilizes Natural
Language Processing (NLP) techniques to generate plausible assembly sequences,
and provide real-time guidance in the AR headset.Our user study demonstrates
BrickPal's effectiveness at assisting users in brick assembly compared to
traditional assembly methods. Additionally, the NLP algorithm-generated
assembly sequences achieve the same usability with manually adapted sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,7 figures. Project URL: https://origami.dance/brickpal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge
  Base Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Yao, Yuanyong Chen, Xin Lv, Shulin Cao, Amy Xin, Jifan Yu, Hailong Jin, Jianjun Xu, Peng Zhang, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Visual Knowledge oriented Programming platform (VisKoP), a
knowledge base question answering (KBQA) system that integrates human into the
loop to edit and debug the knowledge base (KB) queries. VisKoP not only
provides a neural program induction module, which converts natural language
questions into knowledge oriented program language (KoPL), but also maps KoPL
programs into graphical elements. KoPL programs can be edited with simple
graphical operators, such as dragging to add knowledge operators and slot
filling to designate operator arguments. Moreover, VisKoP provides
auto-completion for its knowledge base schema and users can easily debug the
KoPL program by checking its intermediate results. To facilitate the practical
KBQA on a million-entity-level KB, we design a highly efficient KoPL execution
engine for the back-end. Experiment results show that VisKoP is highly
efficient and user interaction can fix a large portion of wrong KoPL programs
to acquire the correct answer. The VisKoP online demo
https://demoviskop.xlore.cn (Stable release of this paper) and
https://viskop.xlore.cn (Beta release with new features), highly efficient KoPL
engine https://pypi.org/project/kopl-engine, and screencast video
https://youtu.be/zAbJtxFPTXo are now publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shadow operator: Effective dynamic load change operation training in air
  separation processes based on industrial nonlinear MPC and Bloom's taxonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghui Yang, Zhijiang Shao, Rui Wang, Zuhua Xu, Lidan Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel human-machine interactive training method for dynamic load change
operation in air separation processes (ASPs) is proposed. A shadow operator
(SO) is developed in this method to train ASP operators through industrial
model predictive control (IMPC) and Bloom's taxonomy. First, a nonlinear
two-layer IMPC machine algorithm is developed for dynamic load change
operation. The IMPC uses a linear parameter varying prediction model and an
iterative multi-step linearization algorithm to compute accurate control
decisions. Second, a hierarchical human-machine cooperation model is
established to improve the effectiveness of operation training. The model is
inspired by an educational psychology framework (Bloom's taxonomy) and assists
ASP operators in enhancing their dynamic operational skills. Finally, five
dynamic training modes of the SO are designed based on the IMPC algorithm and
the human-machine cooperation model. The practical application results
demonstrate that the SO improves the effectiveness of skill acquisition for
novice operators and the safety of dynamic operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In Time and Space: Towards Usable Adaptive Control for Assistive Robotic
  Arms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Pascher, Kirill Kronhardt, Felix Ferdinand Goldau, Udo Frese, Jens Gerken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic solutions, in particular robotic arms, are becoming more frequently
deployed for close collaboration with humans, for example in manufacturing or
domestic care environments. These robotic arms require the user to control
several Degrees-of-Freedom (DoFs) to perform tasks, primarily involving
grasping and manipulating objects. Standard input devices predominantly have
two DoFs, requiring time-consuming and cognitively demanding mode switches to
select individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) have
shown to decrease the necessary number of mode switches but were up to now not
able to significantly reduce the perceived workload. Users still bear the
mental workload of incorporating abstract mode switching into their workflow.
We address this by providing feed-forward multimodal feedback using updated
recommendations of ADMC, allowing users to visually compare the current and the
suggested mapping in real-time. We contrast the effectiveness of two new
approaches that a) continuously recommend updated DoF combinations or b) use
discrete thresholds between current robot movements and new recommendations.
Both are compared in a Virtual Reality (VR) in-person study against a classic
control method. Significant results for lowered task completion time, fewer
mode switches, and reduced perceived workload conclusively establish that in
combination with feedforward, ADMC methods can indeed outperform classic mode
switching. A lack of apparent quantitative differences between Continuous and
Threshold reveals the importance of user-centered customization options.
Including these implications in the development process will improve usability,
which is essential for successfully implementing robotic technologies with high
user acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Emotional Dilemma: Influence of a Human-like Robot on Trust and
  Cooperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Becker, Diana Rueda, Felix Beese, Brenda Scarleth Gutierrez Torres, Myriem Lafdili, Kyra Ahrens, Di Fu, Erik Strahl, Tom Weber, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing anthropomorphic robot behavioral design could affect trust and
cooperation positively. However, studies have shown contradicting results and
suggest a task-dependent relationship between robots that display emotions and
trust. Therefore, this study analyzes the effect of robots that display
human-like emotions on trust, cooperation, and participants' emotions. In the
between-group study, participants play the coin entrustment game with an
emotional and a non-emotional robot. The results show that the robot that
displays emotions induces more anxiety than the neutral robot. Accordingly, the
participants trust the emotional robot less and are less likely to cooperate.
Furthermore, the perceived intelligence of a robot increases trust, while a
desire to outcompete the robot can reduce trust and cooperation. Thus, the
design of robots expressing emotions should be task dependent to avoid adverse
effects that reduce trust and cooperation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2023 32nd IEEE International Conference on Robot and
  Human Interactive Communication (RO-MAN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trends in Machine Learning and Electroencephalogram (EEG): A <span class="highlight-title">Review</span> for
  Undergraduate Researchers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Koome Murungi, Michael Vinh Pham, Xufeng Dai, Xiaodong Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic literature review on Brain-Computer
Interfaces (BCIs) in the context of Machine Learning. Our focus is on
Electroencephalography (EEG) research, highlighting the latest trends as of
2023. The objective is to provide undergraduate researchers with an accessible
overview of the BCI field, covering tasks, algorithms, and datasets. By
synthesizing recent findings, our aim is to offer a fundamental understanding
of BCI research, identifying promising avenues for future investigations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure, HCI International 2023 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image
  Enhancement for Gastrointestinal Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet M. Thai, Anh T. Vo, Hao K. Tieu, Linh N. P. Bui, Thien T. B. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, artificial intelligence has played an important role in
medicine and disease diagnosis, with many applications to be mentioned, one of
which is Medical Visual Question Answering (MedVQA). By combining computer
vision and natural language processing, MedVQA systems can assist experts in
extracting relevant information from medical image based on a given question
and providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023
challenge carried out visual question answering task in the gastrointestinal
domain, which includes gastroscopy and colonoscopy images. Our team approached
Task 1 of the challenge by proposing a multimodal learning method with image
enhancement to improve the VQA performance on gastrointestinal images. The
multimodal architecture is set up with BERT encoder and different pre-trained
vision models based on convolutional neural network (CNN) and Transformer
architecture for features extraction from question and endoscopy image. The
result of this study highlights the dominance of Transformer-based vision
models over the CNNs and demonstrates the effectiveness of the image
enhancement process, with six out of the eight vision models achieving better
F1-Score. Our best method, which takes advantages of BERT+BEiT fusion and image
enhancement, achieves up to 87.25% accuracy and 91.85% F1-Score on the
development test set, while also producing good result on the private test set
with accuracy of 82.01%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ImageCLEF2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Computer Interface (BCI) based on Electroencephalographic (EEG)
  patterns due to new cognitive tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahmeeth Sayed Sakkaff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New mental tasks were investigated for suitability in Brain-Computer
Interface (BCI). Electroencephalography (EEG) signals were collected and
analyzed to identify these mental tasks. MS Windows-based software was
developed for investigating and classifying recorded EEG data with unnecessary
frequencies filtered out with Bandpass filtering. To identify the best feature
vector construction method for a given mental task, feature vectors were
constructed using Bandpower, Principal Component Analysis, and Downsampling
separately. These feature vectors were then classified with Linear Discriminant
Analysis, Linear Support Vector Machines, Critical Distance Classifiers,
Nearest Neighbor Classifiers, and their Non-Linear counterparts to find the
best-performing classifier. For comparison purposes, performances of already
well-known mental tasks in the BCI community were computed along with that of
new mental tasks introduced in this thesis. In the preliminary studies, it was
found that the most promising new mental task which a BCI system could identify
is the imagination of hitting a given square with an imaginary arrow from above
(or below) and right, (or left) to the screen. The group of these mental tasks
was named as 'Hit Series' (HS). A detailed investigation of HS was carried out
and compared with the performance of Motor Imagery (MI) events which are the
most heavily used mental tasks in EEG-based BCI systems. One subject achieved
the maximum average performance for HS, 100 pct in the binary classifications
while 99 pct in overall combined performance. The best average performances of
the other two subjects for the same mental tasks were 93 pct and 87pct with the
overall performance of 89 pct and 78 pct. Performances of the same three
subjects for mental tasks in MI were relatively poor. The average performances
were 92, 78, and 92 pct while overall performances were 87, 69, and 88 pct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:1404.1100 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeLiNet: Sentiment enriched Lightweight Network for Emotion Recognition
  in Images <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuneer Khargonkar, Shwetank Choudhary, Sumit Kumar, Barath Raj KR
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a sentiment-enriched lightweight network SeLiNet
and an end-to-end on-device pipeline for contextual emotion recognition in
images. SeLiNet model consists of body feature extractor, image aesthetics
feature extractor, and learning-based fusion network which jointly estimates
discrete emotion and human sentiments tasks. On the EMOTIC dataset, the
proposed approach achieves an Average Precision (AP) score of 27.17 in
comparison to the baseline AP score of 27.38 while reducing the model size by
>85%. In addition, we report an on-device AP score of 26.42 with reduction in
model size by >93% when compared to the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper submitted in ISCAS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionDB: A Large-scale <span class="highlight-title">Prompt</span> Gallery <span class="highlight-title">Dataset</span> for Text-to-Image
  Generative Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14896v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14896v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, Duen Horng Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent advancements in diffusion models, users can generate high-quality
images by writing text prompts in natural language. However, generating images
with desired details requires proper prompts, and it is often unclear how a
model reacts to different prompts or what the best prompts are. To help
researchers tackle these critical challenges, we introduce DiffusionDB, the
first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14
million images generated by Stable Diffusion, 1.8 million unique prompts, and
hyperparameters specified by real users. We analyze the syntactic and semantic
characteristics of prompts. We pinpoint specific hyperparameter values and
prompt styles that can lead to model errors and present evidence of potentially
harmful model usage, such as the generation of misinformation. The
unprecedented scale and diversity of this human-actuated dataset provide
exciting research opportunities in understanding the interplay between prompts
and generative models, detecting deepfakes, and designing human-AI interaction
tools to help users more easily use these models. DiffusionDB is publicly
available at: https://poloclub.github.io/diffusiondb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 (nominated for best paper, top 1.6% of
  submissions, oral presentation). 17 pages, 11 figures. The dataset is
  available at https://huggingface.co/datasets/poloclub/diffusiondb. The code
  is at https://github.com/poloclub/diffusiondb. The interactive visualization
  demo is at https://poloclub.github.io/diffusiondb/explorer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computer says "No": The Case Against Empathetic Conversational AI <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alba Curry, Amanda Cercas Curry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotions are an integral part of human cognition and they guide not only our
understanding of the world but also our actions within it. As such, whether we
soothe or flame an emotion is not inconsequential. Recent work in
conversational AI has focused on responding empathetically to users, validating
and soothing their emotions without a real basis. This AI-aided emotional
regulation can have negative consequences for users and society, tending
towards a one-noted happiness defined as only the absence of "negative"
emotions. We argue that we must carefully consider whether and how to respond
to users' emotions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of the ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Some Observations on Fact-Checking Work with Implications for
  Computational Support <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02224v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02224v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rob Procter, Miguel Arana-Catania, Yulan He, Maria Liakata, Arkaitz Zubiaga, Elena Kochkina, Runcong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media and user-generated content (UGC) have become increasingly
important features of journalistic work in a number of different ways. However,
the growth of misinformation means that news organisations have had devote more
and more resources to determining its veracity and to publishing corrections if
it is found to be misleading. In this work, we present the results of
interviews with eight members of fact-checking teams from two organisations.
Team members described their fact-checking processes and the challenges they
currently face in completing a fact-check in a robust and timely way. The
former reveals, inter alia, significant differences in fact-checking practices
and the role played by collaboration between team members. We conclude with a
discussion of the implications for the development and application of
computational tools, including where computational tool support is currently
lacking and the importance of being able to accommodate different fact-checking
practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. International AAAI Conference on Web and Social Media,
  Mediate 2023: News Media and Computational Journalism Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INDCOR white paper 4: Evaluation of Interactive Narrative Design For
  Complexity Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09817v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09817v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Roth, Breanne Pitt, Lāsma Šķestere, Jonathan Barbara, Agnes Karolina Bakk, Kirsty Dunlop, Maria del Mar Grandio, Miguel Barreda, Despoina Sampatakou, Michael Schlauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a strength of Interactive Digital Narratives (IDN) is its support for
multiperspectivity, this also poses a substantial challenge to its evaluation.
Moreover, evaluation has to assess the system's ability to represent a complex
reality as well as the user's understanding of that complex reality as a result
of the experience of interacting with the system. This is needed to measure an
IDN's efficiency and effectiveness in representing the chosen complex
phenomenon. We here present some empirical methods employed by INDCOR members
in their research including UX toolkits and scales. Particularly, we consider
the impact of IDN on transformative learning and its evaluation through
self-reporting and other alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2010.10135</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Feedback Literacy Matters for Learning Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00879v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00879v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Shan Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning analytics (LA) provides data-driven feedback that aims to improve
learning and inform action. For learners, LA-based feedback may scaffold
self-regulated learning skills, which are crucial to learning success. For
teachers, LA-based feedback may help the evaluation of teaching effects and the
need for interventions. However, the current development of LA has presented
problems related to the cognitive, social-affective, and structural dimensions
of feedback. In light of this, this position paper argues that attention needs
to shift from the design of LA as a feedback product to one that facilitates a
process in which both teachers and students play active roles in
meaning-making. To this end, implications for feedback literacy in the context
of LA are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted at the 2022 International Conference of the
  Learning Sciences (ICLS).
  https://repository.isls.org/bitstream/1/8799/1/ICLS2022_27-34.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't trust your eyes: on the (un)reliability of feature visualizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04719v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04719v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Geirhos, Roland S. Zimmermann, Blair Bilodeau, Wieland Brendel, Been Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to "explain" how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">74</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Push Past Green: Learning to Look Behind Plant Foliage by Moving It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agriculture applications (e.g., inspection, phenotyping, plucking
fruits) require manipulating the plant foliage to look behind the leaves and
the branches. Partial visibility, extreme clutter, thin structures, and unknown
geometry and dynamics for plants make such manipulation challenging. We tackle
these challenges through data-driven methods. We use self-supervision to train
SRPNet, a neural network that predicts what space is revealed on execution of a
candidate action on a given plant. We use SRPNet with the cross-entropy method
to predict actions that are effective at revealing space beneath plant foliage.
Furthermore, as SRPNet does not just predict how much space is revealed but
also where it is revealed, we can execute a sequence of actions that
incrementally reveal more and more space beneath the plant foliage. We
experiment with a synthetic (vines) and a real plant (Dracaena) on a physical
test-bed across 5 settings including 2 settings that test generalization to
novel plant configurations. Our experiments reveal the effectiveness of our
overall method, PPG, over a competitive hand-crafted exploration method, and
the effectiveness of SRPNet over a hand-crafted dynamics model and relevant
ablations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for project website with video, see
  https://sites.google.com/view/pushpastgreen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEO: Learning Efficient Orderings for Multiobjective Binary Decision
  Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Patel, Elias B. Khalil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approaches based on Binary decision diagrams (BDDs) have recently achieved
state-of-the-art results for multiobjective integer programming problems. The
variable ordering used in constructing BDDs can have a significant impact on
their size and on the quality of bounds derived from relaxed or restricted BDDs
for single-objective optimization problems. We first showcase a similar impact
of variable ordering on the Pareto frontier (PF) enumeration time for the
multiobjective knapsack problem, suggesting the need for deriving variable
ordering methods that improve the scalability of the multiobjective BDD
approach. To that end, we derive a novel parameter configuration space based on
variable scoring functions which are linear in a small set of interpretable and
easy-to-compute variable features. We show how the configuration space can be
efficiently explored using black-box optimization, circumventing the curse of
dimensionality (in the number of variables and objectives), and finding good
orderings that reduce the PF enumeration time. However, black-box optimization
approaches incur a computational overhead that outweighs the reduction in time
due to good variable ordering. To alleviate this issue, we propose LEO, a
supervised learning approach for finding efficient variable orderings that
reduce the enumeration time. Experiments on benchmark sets from the knapsack
problem with 3-7 objectives and up to 80 variables show that LEO is ~30-300%
and ~10-200% faster at PF enumeration than common ordering strategies and
algorithm configuration. Our code and instances are available at
https://github.com/khalil-research/leo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focused <span class="highlight-title">Transformer</span>: Contrastive Training for Context Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have an exceptional capability to incorporate new
information in a contextual manner. However, the full potential of such an
approach is often restrained due to a limitation in the effective context
length. One solution to this issue is to endow an attention layer with access
to an external memory, which comprises of (key, value) pairs. Yet, as the
number of documents increases, the proportion of relevant keys to irrelevant
ones decreases, leading the model to focus more on the irrelevant keys. We
identify a significant challenge, dubbed the distraction issue, where keys
linked to different semantic values might overlap, making them hard to
distinguish. To tackle this problem, we introduce the Focused Transformer
(FoT), a technique that employs a training process inspired by contrastive
learning. This novel approach enhances the structure of the (key, value) space,
enabling an extension of the context length. Our method allows for fine-tuning
pre-existing, large-scale models to lengthen their effective context. This is
demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The
resulting models, which we name LongLLaMA, exhibit advancements in tasks
requiring a long context. We further illustrate that our LongLLaMA models
adeptly manage a $256 k$ context length for passkey retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BrickPal: Augmented Reality-based Assembly Instructions for Brick Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Shi, Xiaofeng Zhang, Ran zhang, Zhou Yang, Xiao Tang, Hongni Ye, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The assembly instruction is a mandatory component of Lego-like brick sets.The
conventional production of assembly instructions requires a considerable amount
of manual fine-tuning, which is intractable for casual users and customized
brick sets.Moreover, the traditional paper-based instructions lack
expressiveness and interactivity.To tackle the two problems above, we present
BrickPal, an augmented reality-based system, which visualizes assembly
instructions in an augmented reality head-mounted display. It utilizes Natural
Language Processing (NLP) techniques to generate plausible assembly sequences,
and provide real-time guidance in the AR headset.Our user study demonstrates
BrickPal's effectiveness at assisting users in brick assembly compared to
traditional assembly methods. Additionally, the NLP algorithm-generated
assembly sequences achieve the same usability with manually adapted sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,7 figures. Project URL: https://origami.dance/brickpal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Large Vision-Language Model with Out-of-Distribution
  Generalizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, Zhuowen Tu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student's OOD
generalization: (1) by better imitating teacher's visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher's language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Our code will be released at
https://github.com/xuanlinli17/large_vlm_distillation_ood
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Multi-Agent Intention-Aware Communication for Optimal
  Multi-Order Execution in Finance <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Fang, Zhenggang Tang, Kan Ren, Weiqing Liu, Li Zhao, Jiang Bian, Dongsheng Li, Weinan Zhang, Yong Yu, Tie-Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Order execution is a fundamental task in quantitative finance, aiming at
finishing acquisition or liquidation for a number of trading orders of the
specific assets. Recent advance in model-free reinforcement learning (RL)
provides a data-driven solution to the order execution problem. However, the
existing works always optimize execution for an individual order, overlooking
the practice that multiple orders are specified to execute simultaneously,
resulting in suboptimality and bias. In this paper, we first present a
multi-agent RL (MARL) method for multi-order execution considering practical
constraints. Specifically, we treat every agent as an individual operator to
trade one specific order, while keeping communicating with each other and
collaborating for maximizing the overall profits. Nevertheless, the existing
MARL algorithms often incorporate communication among agents by exchanging only
the information of their partial observations, which is inefficient in
complicated financial market. To improve collaboration, we then propose a
learnable multi-round communication protocol, for the agents communicating the
intended actions with each other and refining accordingly. It is optimized
through a novel action value attribution method which is provably consistent
with the original learning objective yet more efficient. The experiments on the
data from two real-world markets have illustrated superior performance with
significantly better collaboration effectiveness achieved by our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in KDD 2023; The website is at
  https://seqml.github.io/marl4fin</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Domain Adaptation of Sentence Embeddings using Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Schopf, Dennis Schneider, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentence embeddings enable us to capture the semantic similarity of short
texts. Most sentence embedding models are trained for general semantic textual
similarity (STS) tasks. Therefore, to use sentence embeddings in a particular
domain, the model must be adapted to it in order to achieve good results.
Usually, this is done by fine-tuning the entire sentence embedding model for
the domain of interest. While this approach yields state-of-the-art results,
all of the model's weights are updated during fine-tuning, making this method
resource-intensive. Therefore, instead of fine-tuning entire sentence embedding
models for each target domain individually, we propose to train lightweight
adapters. These domain-specific adapters do not require fine-tuning all
underlying sentence embedding model parameters. Instead, we only train a small
number of additional parameters while keeping the weights of the underlying
sentence embedding model fixed. Training domain-specific adapters allows always
using the same base model and only exchanging the domain-specific adapters to
adapt sentence embeddings to a specific domain. We show that using adapters for
parameter-efficient domain adaptation of sentence embeddings yields competitive
performance within 1% of a domain-adapted, entirely fine-tuned sentence
embedding model while only training approximately 3.6% of the parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International Conference on Recent Advances in
  Natural Language Processing (RANLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepOnto: A Python Package for Ontology Engineering with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan He, Jiaoyan Chen, Hang Dong, Ian Horrocks, Carlo Allocca, Taehun Kim, Brahmananda Sapkota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying deep learning techniques, particularly language models (LMs), in
ontology engineering has raised widespread attention. However, deep learning
frameworks like PyTorch and Tensorflow are predominantly developed for Python
programming, while widely-used ontology APIs, such as the OWL API and Jena, are
primarily Java-based. To facilitate seamless integration of these frameworks
and APIs, we present Deeponto, a Python package designed for ontology
engineering. The package encompasses a core ontology processing module founded
on the widely-recognised and reliable OWL API, encapsulating its fundamental
features in a more "Pythonic" manner and extending its capabilities to include
other essential components including reasoning, verbalisation, normalisation,
projection, and more. Building on this module, Deeponto offers a suite of
tools, resources, and algorithms that support various ontology engineering
tasks, such as ontology alignment and completion, by harnessing deep learning
methodologies, primarily pre-trained LMs. In this paper, we also demonstrate
the practical utility of Deeponto through two use-cases: the Digital Health
Coaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment
Evaluation Initiative (OAEI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review at Semantic Web Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing Backpropagation for Gradient-Based Interpretability <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Du, Lucas Torroba Hennigen, Niklas Stoehr, Alexander Warstadt, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many popular feature-attribution methods for interpreting deep neural
networks rely on computing the gradients of a model's output with respect to
its inputs. While these methods can indicate which input features may be
important for the model's prediction, they reveal little about the inner
workings of the model itself. In this paper, we observe that the gradient
computation of a model is a special case of a more general formulation using
semirings. This observation allows us to generalize the backpropagation
algorithm to efficiently compute other interpretable statistics about the
gradient graph of a neural network, such as the highest-weighted path and
entropy. We implement this generalized algorithm, evaluate it on synthetic
datasets to better understand the statistics it computes, and apply it to study
BERT's behavior on the subject-verb number agreement task (SVA). With this
method, we (a) validate that the amount of gradient flow through a component of
a model reflects its importance to a prediction and (b) for SVA, identify which
pathways of the self-attention mechanism are most important.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long paper accepted at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Art Authentication with Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovica Schaerf, Carina Popovici, Eric Postma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Transformers, initially developed for language, have been
successfully applied to visual tasks. Vision Transformers have been shown to
push the state-of-the-art in a wide range of tasks, including image
classification, object detection, and semantic segmentation. While ample
research has shown promising results in art attribution and art authentication
tasks using Convolutional Neural Networks, this paper examines if the
superiority of Vision Transformers extends to art authentication, improving,
thus, the reliability of computer-based authentication of artworks. Using a
carefully compiled dataset of authentic paintings by Vincent van Gogh and two
contrast datasets, we compare the art authentication performances of Swin
Transformers with those of EfficientNet. Using a standard contrast set
containing imitations and proxies (works by painters with styles closely
related to van Gogh), we find that EfficientNet achieves the best performance
overall. With a contrast set that only consists of imitations, we find the Swin
Transformer to be superior to EfficientNet by achieving an authentication
accuracy of over 85%. These results lead us to conclude that Vision
Transformers represent a strong and promising contender in art authentication,
particularly in enhancing the computer-based ability to detect artistic
imitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Yu, Chiaki Hirayama, Chenning Yu, Sylvia Herbert, Sicun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are two major challenges for scaling up robot navigation around dynamic
obstacles: the complex interaction dynamics of the obstacles can be hard to
model analytically, and the complexity of planning and control grows
exponentially in the number of obstacles. Data-driven and learning-based
methods are thus particularly valuable in this context. However, data-driven
methods are sensitive to distribution drift, making it hard to train and
generalize learned models across different obstacle densities. We propose a
novel method for compositional learning of Sequential Neural Control Barrier
models (SNCBFs) to achieve scalability. Our approach exploits an important
observation: the spatial interaction patterns of multiple dynamic obstacles can
be decomposed and predicted through temporal sequences of states for each
obstacle. Through decomposition, we can generalize control policies trained
only with a small number of obstacles, to environments where the obstacle
density can be 100x higher. We demonstrate the benefits of the proposed methods
in improving dynamic collision avoidance in comparison with existing methods
including potential fields, end-to-end reinforcement learning, and
model-predictive control. We also perform hardware experiments and show the
practical effectiveness of the approach in the supplementary video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IROS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Optimization of Hand Pose Estimation using Anatomical
  Features and Iterative Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Jauch, Timo Leitritz, Marco F. Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual assembly workers face increasing complexity in their work.
Human-centered assistance systems could help, but object recognition as an
enabling technology hinders sophisticated human-centered design of these
systems. At the same time, activity recognition based on hand poses suffers
from poor pose estimation in complex usage scenarios, such as wearing gloves.
This paper presents a self-supervised pipeline for adapting hand pose
estimation to specific use cases with minimal human interaction. This enables
cheap and robust hand posebased activity recognition. The pipeline consists of
a general machine learning model for hand pose estimation trained on a
generalized dataset, spatial and temporal filtering to account for anatomical
constraints of the hand, and a retraining step to improve the model. Different
parameter combinations are evaluated on a publicly available and annotated
dataset. The best parameter and model combination is then applied to unlabelled
videos from a manual assembly scenario. The effectiveness of the pipeline is
demonstrated by training an activity recognition as a downstream task in the
manual assembly scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted at IEEE SMC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Privacy-Preserving Walk in the Latent Space of Generative Models for
  Medical Applications <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Pennisi, Federica Proietto Salanitri, Giovanni Bellitto, Simone Palazzo, Ulas Bagci, Concetto Spampinato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have demonstrated their ability to
generate synthetic samples that match a target distribution. However, from a
privacy perspective, using GANs as a proxy for data sharing is not a safe
solution, as they tend to embed near-duplicates of real samples in the latent
space. Recent works, inspired by k-anonymity principles, address this issue
through sample aggregation in the latent space, with the drawback of reducing
the dataset by a factor of k. Our work aims to mitigate this problem by
proposing a latent space navigation strategy able to generate diverse synthetic
samples that may support effective training of deep models, while addressing
privacy concerns in a principled way. Our approach leverages an auxiliary
identity classifier as a guide to non-linearly walk between points in the
latent space, minimizing the risk of collision with near-duplicates of real
samples. We empirically demonstrate that, given any random pair of points in
the latent space, our walking strategy is safer than linear interpolation. We
then test our path-finding strategy combined to k-same methods and demonstrate,
on two benchmarks for tuberculosis and diabetic retinopathy classification,
that training a model using samples generated by our approach mitigate drops in
performance, while keeping privacy preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Cultural Gap in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingshuai Liu, Longyue Wang, Chenyang Lyu, Yong Zhang, Jinsong Su, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One challenge in text-to-image (T2I) generation is the inadvertent reflection
of culture gaps present in the training data, which signifies the disparity in
generated image quality when the cultural elements of the input text are rarely
collected in the training set. Although various T2I models have shown
impressive but arbitrary examples, there is no benchmark to systematically
evaluate a T2I model's ability to generate cross-cultural images. To bridge the
gap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensive
evaluation criteria, which can assess how well-suited a model is to a target
culture. By analyzing the flawed images generated by the Stable Diffusion model
on the C3 benchmark, we find that the model often fails to generate certain
cultural objects. Accordingly, we propose a novel multi-modal metric that
considers object-text alignment to filter the fine-tuning data in the target
culture, which is used to fine-tune a T2I model to improve cross-cultural
generation. Experimental results show that our multi-modal metric provides
stronger data selection performance on the C3 benchmark than existing metrics,
in which the object-text alignment is crucial. We release the benchmark, data,
code, and generated images to facilitate future research on culturally diverse
T2I generation (https://github.com/longyuewangdcu/C3-Bench).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Bingshuai Liu and Longyue Wang. Work done while
  Bingshuai Liu and Chengyang Lyu were interning at Tencent AI Lab. Zhaopeng Tu
  is the corresponding author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Neuromorphic Architecture for Reinforcement Learning from Real-Valued
  Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergio F. Chevtchenko, Yeshwanth Bethi, Teresa B. Ludermir, Saeed Afshar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) provides a powerful framework for decision-making
in complex environments. However, implementing RL in hardware-efficient and
bio-inspired ways remains a challenge. This paper presents a novel Spiking
Neural Network (SNN) architecture for solving RL problems with real-valued
observations. The proposed model incorporates multi-layered event-based
clustering, with the addition of Temporal Difference (TD)-error modulation and
eligibility traces, building upon prior work. An ablation study confirms the
significant impact of these components on the proposed model's performance. A
tabular actor-critic algorithm with eligibility traces and a state-of-the-art
Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our
network consistently outperforms the tabular approach and successfully
discovers stable control policies on classic RL environments: mountain car,
cart-pole, and acrobot. The proposed model offers an appealing trade-off in
terms of computational and hardware implementation requirements. The model does
not require an external memory buffer nor a global error gradient computation,
and synaptic updates occur online, driven by local learning rules and a
broadcasted TD-error signal. Thus, this work contributes to the development of
more hardware-efficient RL solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In Time and Space: Towards Usable Adaptive Control for Assistive Robotic
  Arms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Pascher, Kirill Kronhardt, Felix Ferdinand Goldau, Udo Frese, Jens Gerken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic solutions, in particular robotic arms, are becoming more frequently
deployed for close collaboration with humans, for example in manufacturing or
domestic care environments. These robotic arms require the user to control
several Degrees-of-Freedom (DoFs) to perform tasks, primarily involving
grasping and manipulating objects. Standard input devices predominantly have
two DoFs, requiring time-consuming and cognitively demanding mode switches to
select individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) have
shown to decrease the necessary number of mode switches but were up to now not
able to significantly reduce the perceived workload. Users still bear the
mental workload of incorporating abstract mode switching into their workflow.
We address this by providing feed-forward multimodal feedback using updated
recommendations of ADMC, allowing users to visually compare the current and the
suggested mapping in real-time. We contrast the effectiveness of two new
approaches that a) continuously recommend updated DoF combinations or b) use
discrete thresholds between current robot movements and new recommendations.
Both are compared in a Virtual Reality (VR) in-person study against a classic
control method. Significant results for lowered task completion time, fewer
mode switches, and reduced perceived workload conclusively establish that in
combination with feedforward, ADMC methods can indeed outperform classic mode
switching. A lack of apparent quantitative differences between Continuous and
Threshold reveals the importance of user-centered customization options.
Including these implications in the development process will improve usability,
which is essential for successfully implementing robotic technologies with high
user acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEA: Improving Sentence Similarity Robustness to Typos Using Lexical
  Attention Bias <span class="chip">KDD'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Almagro, Emilio Almazán, Diego Ortego, David Jiménez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual noise, such as typos or abbreviations, is a well-known issue that
penalizes vanilla Transformers for most downstream tasks. We show that this is
also the case for sentence similarity, a fundamental task in multiple domains,
e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached
using cross-encoders, where the two sentences are concatenated in the input
allowing the model to exploit the inter-relations between them. Previous works
addressing the noise issue mainly rely on data augmentation strategies, showing
improved robustness when dealing with corrupted samples that are similar to the
ones used for training. However, all these methods still suffer from the token
distribution shift induced by typos. In this work, we propose to tackle textual
noise by equipping cross-encoders with a novel LExical-aware Attention module
(LEA) that incorporates lexical similarities between words in both sentences.
By using raw text similarities, our approach avoids the tokenization shift
problem obtaining improved robustness. We demonstrate that the attention bias
introduced by LEA helps cross-encoders to tackle complex scenarios with textual
noise, specially in domains with short-text descriptions and limited context.
Experiments using three popular Transformer encoders in five e-commerce
datasets for product matching show that LEA consistently boosts performance
under the presence of noise, while remaining competitive on the original
(clean) splits. We also evaluate our approach in two datasets for textual
entailment and paraphrasing showing that LEA is robust to typos in domains with
longer sentences and more natural context. Additionally, we thoroughly analyze
several design choices in our approach, providing insights about the impact of
the decisions made and fostering future research in cross-encoders dealing with
typos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD'23 conference (main research track). (*) These authors
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation
  and Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guinan Li, Jiajun Deng, Mengzhe Geng, Zengrui Jin, Tianzi Wang, Shujie Hu, Mingyu Cui, Helen Meng, Xunying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate recognition of cocktail party speech containing overlapping
speakers, noise and reverberation remains a highly challenging task to date.
Motivated by the invariance of visual modality to acoustic signal corruption,
an audio-visual multi-channel speech separation, dereverberation and
recognition approach featuring a full incorporation of visual information into
all system components is proposed in this paper. The efficacy of the video
input is consistently demonstrated in mask-based MVDR speech separation,
DNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end and
Conformer ASR back-end. Audio-visual integrated front-end architectures
performing speech separation and dereverberation in a pipelined or joint
fashion via mask-based WPD are investigated. The error cost mismatch between
the speech enhancement front-end and ASR back-end components is minimized by
end-to-end jointly fine-tuning using either the ASR cost function alone, or its
interpolation with the speech enhancement loss. Experiments were conducted on
the mixture overlapped and reverberant speech data constructed using simulation
or replay of the Oxford LRS2 dataset. The proposed audio-visual multi-channel
speech separation, dereverberation and recognition systems consistently
outperformed the comparable audio-only baseline by 9.1% and 6.2% absolute
(41.7% and 36.0% relative) word error rate (WER) reductions. Consistent speech
enhancement improvements were also obtained on PESQ, STOI and SRMR scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruta Binkyte, Daniele Gorla, Catuscia Palamidessi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of unfair discrimination between two groups and
propose a pre-processing method to achieve fairness. Corrective methods like
statistical parity usually lead to bad accuracy and do not really achieve
fairness in situations where there is a correlation between the sensitive
attribute S and the legitimate attribute E (explanatory variable) that should
determine the decision. To overcome these drawbacks, other notions of fairness
have been proposed, in particular, conditional statistical parity and equal
opportunity. However, E is often not directly observable in the data, i.e., it
is a latent variable. We may observe some other variable Z representing E, but
the problem is that Z may also be affected by S, hence Z itself can be biased.
To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an
approach based on a combination of Bayes inference and the
Expectation-Maximization method, to estimate the most likely value of E for a
given Z for each group. The decision can then be based directly on the
estimated E. We show, by experiments on synthetic and real data sets, that our
approach provides a good level of fairness as well as high accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrast Is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burak Kilic, Florix Bex, Albert Gatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we analyze data-scarce classification scenarios, where
available labeled legal data is small and imbalanced, potentially hurting the
quality of the results. We focused on two finetuning objectives; SetFit
(Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla
finetuning setup on a legal provision classification task. Additionally, we
compare the features that are extracted with LIME (Local Interpretable
Model-agnostic Explanations) to see which particular features contributed to
the model's classification decisions. The results show that a contrastive setup
with SetFit performed better than vanilla finetuning while using a fraction of
the training samples. LIME results show that the contrastive learning approach
helps boost both positive and negative features which are legally informative
and contribute to the classification results. Thus a model finetuned with a
contrastive objective seems to base its decisions more confidently on legally
informative features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages + bib, 12 figures, ACAIL2023/ASAIL2023 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a safe MLOps Process for the Continuous Development and Safety
  Assurance of ML-based Systems in the Railway Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Zeller, Thomas Waschulzik, Reiner Schmid, Claus Bahlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional automation technologies alone are not sufficient to enable
driverless operation of trains (called Grade of Automation (GoA) 4) on
non-restricted infrastructure. The required perception tasks are nowadays
realized using Machine Learning (ML) and thus need to be developed and deployed
reliably and efficiently. One important aspect to achieve this is to use an
MLOps process for tackling improved reproducibility, traceability,
collaboration, and continuous adaptation of a driverless operation to changing
conditions. MLOps mixes ML application development and operation (Ops) and
enables high frequency software releases and continuous innovation based on the
feedback from operations. In this paper, we outline a safe MLOps process for
the continuous development and safety assurance of ML-based systems in the
railway domain. It integrates system engineering, safety assurance, and the ML
life-cycle in a comprehensive workflow. We present the individual stages of the
process and their interactions. Moreover, we describe relevant challenges to
automate the different stages of the safe MLOps process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Xiao, Xiaolin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News summary generation is an important task in the field of intelligence
analysis, which can provide accurate and comprehensive information to help
people better understand and respond to complex real-world events. However,
traditional news summary generation methods face some challenges, which are
limited by the model itself and the amount of training data, as well as the
influence of text noise, making it difficult to generate reliable information
accurately. In this paper, we propose a new paradigm for news summary
generation using LLM with powerful natural language understanding and
generative capabilities. We use LLM to extract multiple structured event
patterns from the events contained in news paragraphs, evolve the event pattern
population with genetic algorithm, and select the most adaptive event pattern
to input into the LLM to generate news summaries. A News Summary Generator
(NSG) is designed to select and evolve the event pattern populations and
generate news summaries. The experimental results show that the news summary
generator is able to generate accurate and reliable news summaries with some
generalization ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating raw waveforms with deep learning frameworks for speech
  emotion recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeynep Hilal Kilimci, Ulku Bayraktar, Ayhan Kucukmanisa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition is a challenging task in speech processing field.
For this reason, feature extraction process has a crucial importance to
demonstrate and process the speech signals. In this work, we represent a model,
which feeds raw audio files directly into the deep neural networks without any
feature extraction stage for the recognition of emotions utilizing six
different data sets, EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. To
demonstrate the contribution of proposed model, the performance of traditional
feature extraction techniques namely, mel-scale spectogram, mel-frequency
cepstral coefficients, are blended with machine learning algorithms, ensemble
learning methods, deep and hybrid deep learning techniques. Support vector
machine, decision tree, naive Bayes, random forests models are evaluated as
machine learning algorithms while majority voting and stacking methods are
assessed as ensemble learning techniques. Moreover, convolutional neural
networks, long short-term memory networks, and hybrid CNN- LSTM model are
evaluated as deep learning techniques and compared with machine learning and
ensemble learning methods. To demonstrate the effectiveness of proposed model,
the comparison with state-of-the-art studies are carried out. Based on the
experiment results, CNN model excels existent approaches with 95.86% of
accuracy for TESS+RAVDESS data set using raw audio files, thence determining
the new state-of-the-art. The proposed model performs 90.34% of accuracy for
EMO-DB with CNN model, 90.42% of accuracy for RAVDESS with CNN model, 99.48% of
accuracy for TESS with LSTM model, 69.72% of accuracy for CREMA with CNN model,
85.76% of accuracy for SAVEE with CNN model in speaker-independent audio
categorization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Domain Adaptive Medical Image Segmentation through
  Consistency Regularized Disentangled Contrastive Learning <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hritam Basak, Zhaozheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although unsupervised domain adaptation (UDA) is a promising direction to
alleviate domain shift, they fall short of their supervised counterparts. In
this work, we investigate relatively less explored semi-supervised domain
adaptation (SSDA) for medical image segmentation, where access to a few labeled
target samples can improve the adaptation performance substantially.
Specifically, we propose a two-stage training process. First, an encoder is
pre-trained in a self-learning paradigm using a novel domain-content
disentangled contrastive learning (CL) along with a pixel-level feature
consistency constraint. The proposed CL enforces the encoder to learn
discriminative content-specific but domain-invariant semantics on a global
scale from the source and target images, whereas consistency regularization
enforces the mining of local pixel-level information by maintaining spatial
sensitivity. This pre-trained encoder, along with a decoder, is further
fine-tuned for the downstream task, (i.e. pixel-level segmentation) using a
semi-supervised setting. Furthermore, we experimentally validate that our
proposed method can easily be extended for UDA settings, adding to the
superiority of the proposed strategy. Upon evaluation on two domain adaptive
image segmentation tasks, our proposed method outperforms the SoTA methods,
both in SSDA and UDA settings. Code is available at
https://github.com/hritam-98/GFDA-disentangled
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by
  Eliminating Ideological Segregation in Knowledge-based Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyan Wang, Yuxuan Hu, Zihan Yuan, Chenting Jiang, Weihua Li, Shiqing Wu, Quan Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of personalized recommendation systems, the increasing concern
is the amplification of belief imbalance and user biases, a phenomenon
primarily attributed to the filter bubble. Addressing this critical issue, we
introduce an innovative intermediate agency (BHEISR) between users and existing
recommendation systems to attenuate the negative repercussions of the filter
bubble effect in extant recommendation systems. The main objective is to strike
a belief balance for users while minimizing the detrimental influence caused by
filter bubbles. The BHEISR model amalgamates principles from nudge theory while
upholding democratic and transparent principles. It harnesses user-specific
category information to stimulate curiosity, even in areas users might
initially deem uninteresting. By progressively stimulating interest in novel
categories, the model encourages users to broaden their belief horizons and
explore the information they typically overlook. Our model is time-sensitive
and operates on a user feedback loop. It utilizes the existing recommendation
algorithm of the model and incorporates user feedback from the prior time
frame. This approach endeavors to transcend the constraints of the filter
bubble, enrich recommendation diversity, and strike a belief balance among
users while also catering to user preferences and system-specific business
requirements. To validate the effectiveness and reliability of the BHEISR
model, we conducted a series of comprehensive experiments with real-world
datasets. These experiments compared the performance of the BHEISR model
against several baseline models using nearly 200 filter bubble-impacted users
as test subjects. Our experimental results conclusively illustrate the superior
performance of the BHEISR model in mitigating filter bubbles and balancing user
perspectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Should Data Science Education Do with Large Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinming Tu, James Zou, Weijie J. Su, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advances of large language models (LLMs), such as ChatGPT, are
revolutionizing data science and statistics. These state-of-the-art tools can
streamline complex processes. As a result, it reshapes the role of data
scientists. We argue that LLMs are transforming the responsibilities of data
scientists, shifting their focus from hands-on coding, data-wrangling and
conducting standard analyses to assessing and managing analyses performed by
these automated AIs. This evolution of roles is reminiscent of the transition
from a software engineer to a product manager. We illustrate this transition
with concrete data science case studies using LLMs in this paper. These
developments necessitate a meaningful evolution in data science education.
Pedagogy must now place greater emphasis on cultivating diverse skillsets among
students, such as LLM-informed creativity, critical thinking, AI-guided
programming. LLMs can also play a significant role in the classroom as
interactive teaching and learning tools, contributing to personalized
education. This paper discusses the opportunities, resources and open
challenges for each of these directions. As with any transformative technology,
integrating LLMs into education calls for careful consideration. While LLMs can
perform repetitive tasks efficiently, it's crucial to remember that their role
is to supplement human intelligence and creativity, not to replace it.
Therefore, the new era of data science education should balance the benefits of
LLMs while fostering complementary human expertise and innovations. In
conclusion, the rise of LLMs heralds a transformative period for data science
and its education. This paper seeks to shed light on the emerging trends,
potential opportunities, and challenges accompanying this paradigm shift,
hoping to spark further discourse and investigation into this exciting,
uncharted territory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Subgroup Separability in Group-Fair Medical Image
  Classification <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Jones, Mélanie Roschewitz, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate performance disparities in deep classifiers. We find that the
ability of classifiers to separate individuals into subgroups varies
substantially across medical imaging modalities and protected characteristics;
crucially, we show that this property is predictive of algorithmic bias.
Through theoretical analysis and extensive empirical evaluation, we find a
relationship between subgroup separability, subgroup disparities, and
performance degradation when models are trained on data with systematic bias
such as underdiagnosis. Our findings shed new light on the question of how
models become biased, providing important insights for the development of fair
medical imaging AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2023. Code available under
  https://github.com/biomedia-mira/subgroup-separability</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        TaeHo Yoon, Kibeom Myoung, Keon Lee, Jaewoong Cho, Albert No, Ernest K. Ryu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently shown remarkable success in high-quality image
generation. Sometimes, however, a pre-trained diffusion model exhibits partial
misalignment in the sense that the model can generate good images, but it
sometimes outputs undesirable images. If so, we simply need to prevent the
generation of the bad images, and we call this task censoring. In this work, we
present censored generation with a pre-trained diffusion model using a reward
model trained on minimal human feedback. We show that censoring can be
accomplished with extreme human feedback efficiency and that labels generated
with a mere few minutes of human feedback are sufficient. Code available at:
https://github.com/tetrzim/diffusion-human-feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRD: Peer Rank and Discussion Improve Large Language Model based
  Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruosen Li, Teerth Patel, Xinya Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, the quality of responses generated by different modern large
language models (LLMs) are hard to evaluate and compare automatically. Recent
studies suggest and predominantly use LLMs as a reference-free metric for
open-ended question answering. More specifically, they use the recognized
"strongest" LLM as the evaluator, which conducts pairwise comparisons of
candidate models' answers and provides a ranking score. However, this intuitive
method has multiple problems, such as bringing in self-enhancement (favoring
its own answers) and positional bias. We draw insights and lessons from the
educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based
evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that
takes into account each peer LLM's pairwise preferences of all answer pairs,
and outputs a final ranking of models; and (2) peer discussion (PD), where we
prompt two LLMs to discuss and try to reach a mutual agreement on preferences
of two answers. We conduct experiments on two benchmark datasets. We find that
our approaches achieve higher accuracy and align better with human judgments,
respectively. Interestingly, PR can induce a relatively accurate self-ranking
of models under the anonymous setting, where each model's name is unrevealed.
Our work provides space to explore evaluating models that are hard to compare
for humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graph <span class="highlight-title">Self-Supervised</span> Rationalization for Recommendation <span class="chip">KDD'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new self-supervised rationalization method,
called KGRec, for knowledge-aware recommender systems. To effectively identify
informative knowledge connections, we propose an attentive knowledge
rationalization mechanism that generates rational scores for knowledge
triplets. With these scores, KGRec integrates generative and contrastive
self-supervised tasks for recommendation through rational masking. To highlight
rationales in the knowledge graph, we design a novel generative task in the
form of masking-reconstructing. By masking important knowledge with high
rational scores, KGRec is trained to rebuild and highlight useful knowledge
connections that serve as rationales. To further rationalize the effect of
collaborative interactions on knowledge graph learning, we introduce a
contrastive learning task that aligns signals from knowledge and user-item
interaction views. To ensure noise-resistant contrasting, potential noisy edges
in both graphs judged by the rational scores are masked. Extensive experiments
on three real-world datasets demonstrate that KGRec outperforms
state-of-the-art methods. We also provide the implementation codes for our
approach at https://github.com/HKUDS/KGRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Reinforcement Learning with Imbalanced <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Jiang, Sijie Chen, Jielin Qiu, Haoran Xu, Wai Kin Chan, Zhao Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalent use of benchmarks in current offline reinforcement learning
(RL) research has led to a neglect of the imbalance of real-world dataset
distributions in the development of models. The real-world offline RL dataset
is often imbalanced over the state space due to the challenge of exploration or
safety considerations. In this paper, we specify properties of imbalanced
datasets in offline RL, where the state coverage follows a power law
distribution characterized by skewed policies. Theoretically and empirically,
we show that typically offline RL methods based on distributional constraints,
such as conservative Q-learning (CQL), are ineffective in extracting policies
under the imbalanced dataset. Inspired by natural intelligence, we propose a
novel offline RL method that utilizes the augmentation of CQL with a retrieval
process to recall past related experiences, effectively alleviating the
challenges posed by imbalanced datasets. We evaluate our method on several
tasks in the context of imbalanced datasets with varying levels of imbalance,
utilizing the variant of D4RL. Empirical results demonstrate the superiority of
our method over other baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RecallM: An Architecture for Temporal Context Understanding and Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Kynoch, Hugo Latapie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ideal long-term memory mechanism for Large Language Model (LLM) based
chatbots, would lay the foundation for continual learning, complex reasoning
and allow sequential and temporal dependencies to be learnt. Creating this type
of memory mechanism is an extremely challenging problem. In this paper we
explore different methods of achieving the effect of long-term memory. We
propose a new architecture focused on creating adaptable and updatable
long-term memory for AGI systems. We demonstrate through various experiments
the benefits of the RecallM architecture, particularly the improved temporal
understanding it provides.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures Our code is publicly available online at:
  https://github.com/cisco-open/DeepVision/tree/main/recallm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-grained Action Analysis: A Multi-modality and Multi-task <span class="highlight-title">Dataset</span> of
  Figure Skating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng-Lan Liu, Yu-Ning Ding, Si-Fan Zhang, Wen-Yue Chen, Ning Zhou, Hao Liu, Gui-Hong Lao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fine-grained action analysis of the existing action datasets is
challenged by insufficient action categories, low fine granularities, limited
modalities, and tasks. In this paper, we propose a Multi-modality and
Multi-task dataset of Figure Skating (MMFS) which was collected from the World
Figure Skating Championships. MMFS, which possesses action recognition and
action quality assessment, captures RGB, skeleton, and is collected the score
of actions from 11671 clips with 256 categories including spatial and temporal
labels. The key contributions of our dataset fall into three aspects as
follows. (1) Independently spatial and temporal categories are first proposed
to further explore fine-grained action recognition and quality assessment. (2)
MMFS first introduces the skeleton modality for complex fine-grained action
quality assessment. (3) Our multi-modality and multi-task dataset encourage
more action analysis models. To benchmark our dataset, we adopt RGB-based and
skeleton-based baseline methods for action recognition and action quality
assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Empowerment: Towards Tractable Empowerment-Based
  Skill-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Levy, Sreehari Rammohan, Alessandro Allievi, Scott Niekum, George Konidaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General purpose agents will require large repertoires of skills. Empowerment
-- the maximum mutual information between skills and the states -- provides a
pathway for learning large collections of distinct skills, but mutual
information is difficult to optimize. We introduce a new framework,
Hierarchical Empowerment, that makes computing empowerment more tractable by
integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning.
Our framework makes two specific contributions. First, we introduce a new
variational lower bound on mutual information that can be used to compute
empowerment over short horizons. Second, we introduce a hierarchical
architecture for computing empowerment over exponentially longer time scales.
We verify the contributions of the framework in a series of simulated robotics
tasks. In a popular ant navigation domain, our four level agents are able to
learn skills that cover a surface area over two orders of magnitude larger than
prior work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted
  Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengfeng Wang, Liukai Xu, Songyuan Liu, zhi Li, Yiming Chen, Weifeng He, Xueqing Li, Yanan Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accommodating all the weights on-chip for large-scale NNs remains a great
challenge for SRAM based computing-in-memory (SRAM-CIM) with limited on-chip
capacity. Previous non-volatile SRAM-CIM (nvSRAM-CIM) addresses this issue by
integrating high-density single-level ReRAMs on the top of high-efficiency
SRAM-CIM for weight storage to eliminate the off-chip memory access. However,
previous SL-nvSRAM-CIM suffers from poor scalability for an increased number of
SL-ReRAMs and limited computing efficiency. To overcome these challenges, this
work proposes an ultra-high-density three-level ReRAMs-assisted
computing-in-nonvolatile-SRAM (TL-nvSRAM-CIM) scheme for large NN models. The
clustered n-selector-n-ReRAM (cluster-nSnRs) is employed for reliable
weight-restore with eliminated DC power. Furthermore, a ternary SRAM-CIM
mechanism with differential computing scheme is proposed for energy-efficient
ternary MAC operations while preserving high NN accuracy. The proposed
TL-nvSRAM-CIM achieves 7.8x higher storage density, compared with the
state-of-art works. Moreover, TL-nvSRAM-CIM shows up to 2.9x and 1.9x enhanced
energy-efficiency, respectively, compared to the baseline designs of SRAM-CIM
and ReRAM-CIM, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Validation of the Practicability of Logical Assessment Formula for
  Evaluations with Inaccurate Ground-Truth Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongquan Yang, Hong Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical assessment formula (LAF) is a new theory proposed for evaluations
with inaccurate ground-truth labels (IAGTLs) to assess the predictive models
for various artificial intelligence applications. However, the practicability
of LAF for evaluations with IAGTLs has not yet been validated in real-world
practice. In this paper, to address this issue, we applied LAF to tumour
segmentation for breast cancer (TSfBC) in medical histopathology whole slide
image analysis (MHWSIA). Experimental results and analysis show the validity of
LAF for evaluations with IAGTLs in the case of TSfBC and reflect the potentials
of LAF applied to MHWSIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2110.11567</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assisting Clinical Decisions for Scarcely Available Treatment via
  Disentangled Latent Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Xue, Ahmed Sameh Said, Ziqi Xu, Hanyang Liu, Neel Shah, Hanqing Yang, Philip Payne, Chenyang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting
modality for COVID-19 patients who are refractory to conventional therapies.
However, the proper treatment decision has been the subject of significant
debate and it remains controversial about who benefits from this scarcely
available and technically complex treatment option. To support clinical
decisions, it is a critical need to predict the treatment need and the
potential treatment and no-treatment responses. Targeting this clinical
challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel
approach for individualized treatment analysis. TVAE is specifically designed
to address the modeling challenges like ECMO with strong treatment selection
bias and scarce treatment cases. TVAE conceptualizes the treatment decision as
a multi-scale problem. We model a patient's potential treatment assignment and
the factual and counterfactual outcomes as part of their intrinsic
characteristics that can be represented by a deep latent variable model. The
factual and counterfactual prediction errors are alleviated via a
reconstruction regularization scheme together with semi-supervision, and the
selection bias and the scarcity of treatment cases are mitigated by the
disentangled and distribution-matched latent space and the label-balancing
generative strategy. We evaluate TVAE on two real-world COVID-19 datasets: an
international dataset collected from 1651 hospitals across 63 countries, and a
institutional dataset collected from 15 hospitals. The results show that TVAE
outperforms state-of-the-art treatment effect models in predicting both the
propensity scores and factual outcomes on heterogeneous COVID-19 datasets.
Additional experiments also show TVAE outperforms the best existing models in
individual treatment effect estimation on the synthesized IHDP benchmark
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Invariance, Equivariance, Correlation and Convolution of Spherical
  Harmonic Representations for Scalar and Vectorial Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mathematical representations of data in the Spherical Harmonic (SH)
domain has recently regained increasing interest in the machine learning
community. This technical report gives an in-depth introduction to the
theoretical foundation and practical implementation of SH representations,
summarizing works on rotation invariant and equivariant features, as well as
convolutions and exact correlations of signals on spheres. In extension, these
methods are then generalized from scalar SH representations to Vectorial
Harmonics (VH), providing the same capabilities for 3d vector fields on spheres
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>106 pages, tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vulnerability of Attribution Methods Using Pre-Softmax Scores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Lerma, Mirtha Lucas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss a vulnerability involving a category of attribution methods used
to provide explanations for the outputs of convolutional neural networks
working as classifiers. It is known that this type of networks are vulnerable
to adversarial attacks, in which imperceptible perturbations of the input may
alter the outputs of the model. In contrast, here we focus on effects that
small modifications in the model may cause on the attribution method without
altering the model outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ It is not Sexually Suggestive, It is Educative. Separating Sex Education
  from Suggestive Content on TikTok Videos <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enfa George, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled
as sexually suggestive (from the annotator's point of view), sex-educational
content, or neither. Such a dataset is necessary to address the challenge of
distinguishing between sexually suggestive content and virtual sex education
videos on TikTok. Children's exposure to sexually suggestive videos has been
shown to have adversarial effects on their development. Meanwhile, virtual sex
education, especially on subjects that are more relevant to the LGBTQIA+
community, is very valuable. The platform's current system removes or penalizes
some of both types of videos, even though they serve different purposes. Our
dataset contains video URLs, and it is also audio transcribed. To validate its
importance, we explore two transformer-based models for classifying the videos.
Our preliminary results suggest that the task of distinguishing between these
types of videos is learnable but challenging. These experiments suggest that
this dataset is meaningful and invites further study on the subject.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL Findings 2023. 10 pages, 3 figures, 5 tables . Please
  refer to https://github.com/enfageorge/SexTok for dataset and related details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Language <span class="highlight-title">Transformer</span>s: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Fields, Casey Kennington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frontier AI Regulation: Managing Emerging Risks to Public Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Anderljung, Joslyn Barnhart, Jade Leung, Anton Korinek, Cullen O'Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, Kevin Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced AI models hold the promise of tremendous benefits for humanity, but
society needs to proactively manage the accompanying risks. In this paper, we
focus on what we term "frontier AI" models: highly capable foundation models
that could possess dangerous capabilities sufficient to pose severe risks to
public safety. Frontier AI models pose a distinct regulatory challenge:
dangerous capabilities can arise unexpectedly; it is difficult to robustly
prevent a deployed model from being misused; and, it is difficult to stop a
model's capabilities from proliferating broadly. To address these challenges,
at least three building blocks for the regulation of frontier models are
needed: (1) standard-setting processes to identify appropriate requirements for
frontier AI developers, (2) registration and reporting requirements to provide
regulators with visibility into frontier AI development processes, and (3)
mechanisms to ensure compliance with safety standards for the development and
deployment of frontier AI models. Industry self-regulation is an important
first step. However, wider societal discussions and government intervention
will be needed to create standards and to ensure compliance with them. We
consider several options to this end, including granting enforcement powers to
supervisory authorities and licensure regimes for frontier AI models. Finally,
we propose an initial set of safety standards. These include conducting
pre-deployment risk assessments; external scrutiny of model behavior; using
risk assessments to inform deployment decisions; and monitoring and responding
to new information about model capabilities and uses post-deployment. We hope
this discussion contributes to the broader conversation on how to balance
public safety risks and innovation benefits from advances at the frontier of AI
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Region-Wise Attentive Multi-View Representation Learning for Urban
  Region Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiliang Chan, Qianqian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban region embedding is an important and yet highly challenging issue due
to the complexity and constantly changing nature of urban data. To address the
challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER)
to capture multi-view dependencies and learn expressive representations of
urban regions without the constraints of rigid neighbourhood region conditions.
Our model focus on learn urban region representation from multi-source urban
data. First, we capture the multi-view correlations from mobility flow
patterns, POI semantics and check-in dynamics. Then, we adopt global graph
attention networks to learn similarity of any two vertices in graphs. To
comprehensively consider and share features of multiple views, a two-stage
fusion module is further proposed to learn weights with external attention to
fuse multi-view embeddings. Extensive experiments for two downstream tasks on
real-world datasets demonstrate that our model outperforms state-of-the-art
methods by up to 17\% improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure Guided Multi-modal <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span> for Knowledge Graph
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Liang, Sihang Zhou, Yue Liu, Lingyuan Meng, Meng Liu, Xinwang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal knowledge graphs (MKGs), which intuitively organize information in
various modalities, can benefit multiple practical downstream tasks, such as
recommendation systems, and visual question answering. However, most MKGs are
still far from complete, which motivates the flourishing of MKG reasoning
models. Recently, with the development of general artificial architectures, the
pretrained transformer models have drawn increasing attention, especially for
multimodal scenarios. However, the research of multimodal pretrained
transformer (MPT) for knowledge graph reasoning (KGR) is still at an early
stage. As the biggest difference between MKG and other multimodal data, the
rich structural information underlying the MKG still cannot be fully leveraged
in existing MPT models. Most of them only utilize the graph structure as a
retrieval map for matching images and texts connected with the same entity.
This manner hinders their reasoning performances. To this end, we propose the
graph Structure Guided Multimodal Pretrained Transformer for knowledge graph
reasoning, termed SGMPT. Specifically, the graph structure encoder is adopted
for structural feature encoding. Then, a structure-guided fusion module with
two different strategies, i.e., weighted summation and alignment constraint, is
first designed to inject the structural information into both the textual and
visual features. To the best of our knowledge, SGMPT is the first MPT model for
multimodal KGR, which mines the structural information underlying the knowledge
graph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that
our SGMPT outperforms existing state-of-the-art models, and prove the
effectiveness of the designed strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Knowledge-Data Driven Channel Semantic Acquisition and
  Beamforming for Cell-Free Massive MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Gao, Shicong Liu, Yu Su, Zhongxiang Li, Dezhi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on advancing outdoor wireless systems to better support
ubiquitous extended reality (XR) applications, and close the gap with current
indoor wireless transmission capabilities. We propose a hybrid knowledge-data
driven method for channel semantic acquisition and multi-user beamforming in
cell-free massive multiple-input multiple-output (MIMO) systems. Specifically,
we firstly propose a data-driven multiple layer perceptron (MLP)-Mixer-based
auto-encoder for channel semantic acquisition, where the pilot signals, CSI
quantizer for channel semantic embedding, and CSI reconstruction for channel
semantic extraction are jointly optimized in an end-to-end manner. Moreover,
based on the acquired channel semantic, we further propose a knowledge-driven
deep-unfolding multi-user beamformer, which is capable of achieving good
spectral efficiency with robustness to imperfect CSI in outdoor XR scenarios.
By unfolding conventional successive over-relaxation (SOR)-based linear
beamforming scheme with deep learning, the proposed beamforming scheme is
capable of adaptively learning the optimal parameters to accelerate convergence
and improve the robustness to imperfect CSI. The proposed deep unfolding
beamforming scheme can be used for access points (APs) with fully-digital array
and APs with hybrid analog-digital array structure. Simulation results
demonstrate the effectiveness of our proposed scheme in improving the accuracy
of channel acquisition, as well as reducing complexity in both CSI acquisition
and beamformer design. The proposed beamforming method achieves approximately
96% of the converged spectrum efficiency performance after only three
iterations in downlink transmission, demonstrating its efficacy and potential
to improve outdoor XR applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>s as Statisticians: Provable In-Context Learning with
  In-Context Algorithm Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural sequence models based on the transformer architecture have
demonstrated remarkable \emph{in-context learning} (ICL) abilities, where they
can perform new tasks when prompted with training and test examples, without
any parameter update to the model. This work first provides a comprehensive
statistical theory for transformers to perform ICL. Concretely, we show that
transformers can implement a broad class of standard machine learning
algorithms in context, such as least squares, ridge regression, Lasso, learning
generalized linear models, and gradient descent on two-layer neural networks,
with near-optimal predictive power on various in-context data distributions.
Using an efficient implementation of in-context gradient descent as the
underlying mechanism, our transformer constructions admit mild size bounds, and
can be learned with polynomially many pretraining sequences.
  Building on these ``base'' ICL algorithms, intriguingly, we show that
transformers can implement more complex ICL procedures involving
\emph{in-context algorithm selection}, akin to what a statistician can do in
real life -- A \emph{single} transformer can adaptively select different base
ICL algorithms -- or even perform qualitatively different tasks -- on different
input sequences, without any explicit prompting of the right algorithm or task.
We both establish this in theory by explicit constructions, and also observe
this phenomenon experimentally. In theory, we construct two general mechanisms
for algorithm selection with concrete examples: pre-ICL testing, and post-ICL
validation. As an example, we use the post-ICL validation mechanism to
construct a transformer that can perform nearly Bayes-optimal ICL on a
challenging task -- noisy linear models with mixed noise levels.
Experimentally, we demonstrate the strong in-context algorithm selection
capabilities of standard transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V2 releases code</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenges and Opportunities in Offline Reinforcement Learning from
  Visual Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04779v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04779v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Lu, Philip J. Ball, Tim G. J. Rudner, Jack Parker-Holder, Michael A. Osborne, Yee Whye Teh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning has shown great promise in leveraging large
pre-collected datasets for policy learning, allowing agents to forgo
often-expensive online data collection. However, offline reinforcement learning
from visual observations with continuous action spaces remains under-explored,
with a limited understanding of the key challenges in this complex domain. In
this paper, we establish simple baselines for continuous control in the visual
domain and introduce a suite of benchmarking tasks for offline reinforcement
learning from visual observations designed to better represent the data
distributions present in real-world offline RL problems and guided by a set of
desiderata for offline RL from visual observations, including robustness to
visual distractions and visually identifiable changes in dynamics. Using this
suite of benchmarking tasks, we show that simple modifications to two popular
vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2,
suffice to outperform existing offline RL methods and establish competitive
baselines for continuous control in the visual domain. We rigorously evaluate
these algorithms and perform an empirical evaluation of the differences between
state-of-the-art model-based and model-free offline RL methods for continuous
control from visual observations. All code and data used in this evaluation are
open-sourced to facilitate progress in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at TMLR, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capturing Emerging Complexity in Lenia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanyam Jain, Aarati Shrestha, Stefano Nichele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research project investigates Lenia, an artificial life platform that
simulates ecosystems of digital creatures. Lenia's ecosystem consists of
simple, artificial organisms that can move, consume, grow, and reproduce. The
platform is important as a tool for studying artificial life and evolution, as
it provides a scalable and flexible environment for creating a diverse range of
organisms with varying abilities and behaviors. Measuring complexity in Lenia
is a key aspect of the study, which identifies the metrics for measuring
long-term complex emerging behavior of rules, with the aim of evolving better
Lenia behaviors which are yet not discovered. The Genetic Algorithm uses
neighborhoods or kernels as genotype while keeping the rest of the parameters
of Lenia as fixed, for example growth function, to produce different behaviors
respective to the population and then measures fitness value to decide the
complexity of the resulting behavior. First, we use Variation over Time as a
fitness function where higher variance between the frames are rewarded. Second,
we use Auto-encoder based fitness where variation of the list of reconstruction
loss for the frames is rewarded. Third, we perform combined fitness where
higher variation of the pixel density of reconstructed frames is rewarded. All
three experiments are tweaked with pixel alive threshold and frames used.
Finally, after performing nine experiments of each fitness for 500 generations,
we pick configurations from all experiments such that there is a scope of
further evolution, and run it for 2500 generations. Results show that the
kernel's center of mass increases with a specific set of pixels and together
with borders the kernel try to achieve a Gaussian distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UTRNet: High-Resolution Urdu Text Recognition In Printed Documents <span class="chip">ICDAR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdur Rahman, Arjun Ghosh, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach to address the challenges of
printed Urdu text recognition using high-resolution, multi-scale semantic
feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model,
demonstrates state-of-the-art performance on benchmark datasets. To address the
limitations of previous works, which struggle to generalize to the intricacies
of the Urdu script and the lack of sufficient annotated real-world data, we
have introduced the UTRSet-Real, a large-scale annotated real-world dataset
comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000
lines closely resembling real-world and made corrections to the ground truth of
the existing IIITH dataset, making it a more reliable resource for future
research. We also provide UrduDoc, a benchmark dataset for Urdu text line
detection in scanned documents. Additionally, we have developed an online tool
for end-to-end Urdu OCR from printed documents by integrating UTRNet with a
text detection model. Our work not only addresses the current limitations of
Urdu OCR but also paves the way for future research in this area and
facilitates the continued advancement of Urdu OCR technology. The project page
with source code, datasets, annotations, trained models, and online tool is
available at abdur75648.github.io/UTRNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The 17th International Conference on Document Analysis
  and Recognition (ICDAR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning-Friendly Biomedical <span class="highlight-title">Dataset</span>s for Equivalence and
  Subsumption Ontology Matching <span class="chip">ISWC-2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.03447v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.03447v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan He, Jiaoyan Chen, Hang Dong, Ernesto Jiménez-Ruiz, Ali Hadian, Ian Horrocks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology Matching (OM) plays an important role in many domains such as
bioinformatics and the Semantic Web, and its research is becoming increasingly
popular, especially with the application of machine learning (ML) techniques.
Although the Ontology Alignment Evaluation Initiative (OAEI) represents an
impressive effort for the systematic evaluation of OM systems, it still suffers
from several limitations including limited evaluation of subsumption mappings,
suboptimal reference mappings, and limited support for the evaluation of
ML-based systems. To tackle these limitations, we introduce five new biomedical
OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes
both equivalence and subsumption matching; the quality of reference mappings is
ensured by human curation, ontology pruning, etc.; and a comprehensive
evaluation framework is proposed to measure OM performance from various
perspectives for both ML-based and non-ML-based OM systems. We report
evaluation results for OM systems of different types to demonstrate the usage
of these resources, all of which are publicly available as part of the new
BioML track at OAEI 2022.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper (Best Resource Paper Candidate) in the 21st
  International Semantic Web Conference (ISWC-2022); Bio-ML Dataset:
  https://doi.org/10.5281/zenodo.6510086</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look Beneath the Surface: Exploiting Fundamental Symmetry for
  Sample-Efficient Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Cheng, Xianyuan Zhan, Zhihao Wu, Wenjia Zhang, Shoucheng Song, Han Wang, Youfang Lin, Li Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) offers an appealing approach to
real-world tasks by learning policies from pre-collected datasets without
interacting with the environment. However, the performance of existing offline
RL algorithms heavily depends on the scale and state-action space coverage of
datasets. Real-world data collection is often expensive and uncontrollable,
leading to small and narrowly covered datasets and posing significant
challenges for practical deployments of offline RL. In this paper, we provide a
new insight that leveraging the fundamental symmetry of system dynamics can
substantially enhance offline RL performance under small datasets.
Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced
Dynamics Model (TDM), which establishes consistency between a pair of forward
and reverse latent dynamics. TDM provides both well-behaved representations for
small datasets and a new reliability measure for OOD samples based on
compliance with the T-symmetry. These can be readily used to construct a new
offline RL algorithm (TSRL) with less conservative policy constraints and a
reliable latent space data augmentation procedure. Based on extensive
experiments, we find TSRL achieves great performance on small benchmark
datasets with as few as 1% of the original samples, which significantly
outperforms the recent offline RL algorithms in terms of data efficiency and
generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Time Leap Challenge for SAT Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.02215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.02215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes K. Fichte, Markus Hecher, Stefan Szeider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We compare the impact of hardware advancement and algorithm advancement for
SAT solving over the last two decades. In particular, we compare 20-year-old
SAT-solvers on new computer hardware with modern SAT-solvers on 20-year-old
hardware. Our findings show that the progress on the algorithmic side has at
least as much impact as the progress on the hardware side.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors' version of a paper which is to appear in the proceedings of
  CP'2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionDB: A Large-scale <span class="highlight-title">Prompt</span> Gallery <span class="highlight-title">Dataset</span> for Text-to-Image
  Generative Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14896v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14896v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, Duen Horng Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent advancements in diffusion models, users can generate high-quality
images by writing text prompts in natural language. However, generating images
with desired details requires proper prompts, and it is often unclear how a
model reacts to different prompts or what the best prompts are. To help
researchers tackle these critical challenges, we introduce DiffusionDB, the
first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14
million images generated by Stable Diffusion, 1.8 million unique prompts, and
hyperparameters specified by real users. We analyze the syntactic and semantic
characteristics of prompts. We pinpoint specific hyperparameter values and
prompt styles that can lead to model errors and present evidence of potentially
harmful model usage, such as the generation of misinformation. The
unprecedented scale and diversity of this human-actuated dataset provide
exciting research opportunities in understanding the interplay between prompts
and generative models, detecting deepfakes, and designing human-AI interaction
tools to help users more easily use these models. DiffusionDB is publicly
available at: https://poloclub.github.io/diffusiondb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 (nominated for best paper, top 1.6% of
  submissions, oral presentation). 17 pages, 11 figures. The dataset is
  available at https://huggingface.co/datasets/poloclub/diffusiondb. The code
  is at https://github.com/poloclub/diffusiondb. The interactive visualization
  demo is at https://poloclub.github.io/diffusiondb/explorer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generation of Highlights from Research Papers Using Pointer-Generator
  Networks and Sci<span class="highlight-title">BERT</span> Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tohida Rehman, Debarshi Kumar Sanyal, Samiran Chattopadhyay, Plaban Kumar Bhowmick, Partha Pratim Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays many research articles are prefaced with research highlights to
summarize the main findings of the paper. Highlights not only help researchers
precisely and quickly identify the contributions of a paper, they also enhance
the discoverability of the article via search engines. We aim to automatically
construct research highlights given certain segments of a research paper. We
use a pointer-generator network with coverage mechanism and a contextual
embedding layer at the input that encodes the input tokens into SciBERT
embeddings. We test our model on a benchmark dataset, CSPubSum, and also
present MixSub, a new multi-disciplinary corpus of papers for automatic
research highlight generation. For both CSPubSum and MixSub, we have observed
that the proposed model achieves the best performance compared to related
variants and other models proposed in the literature. On the CSPubSum dataset,
our model achieves the best performance when the input is only the abstract of
a paper as opposed to other segments of the paper. It produces ROUGE-1, ROUGE-2
and ROUGE-L F1-scores of 38.26, 14.26 and 35.51, respectively, METEOR score of
32.62, and BERTScore F1 of 86.65 which outperform all other baselines. On the
new MixSub dataset, where only the abstract is the input, our proposed model
(when trained on the whole training corpus without distinguishing between the
subject categories) achieves ROUGE-1, ROUGE-2 and ROUGE-L F1-scores of 31.78,
9.76 and 29.3, respectively, METEOR score of 24.00, and BERTScore F1 of 85.25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages, 9 Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FederatedTrust: A Solution for Trustworthy Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Ning Xie, Gérôme Bovet, Gregorio Martínez Pérez, Burkhard Stiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of the Internet of Things (IoT) and Edge Computing has
presented challenges for centralized Machine and Deep Learning (ML/DL) methods
due to the presence of distributed data silos that hold sensitive information.
To address concerns regarding data privacy, collaborative and
privacy-preserving ML/DL techniques like Federated Learning (FL) have emerged.
However, ensuring data privacy and performance alone is insufficient since
there is a growing need to establish trust in model predictions. Existing
literature has proposed various approaches on trustworthy ML/DL (excluding data
privacy), identifying robustness, fairness, explainability, and accountability
as important pillars. Nevertheless, further research is required to identify
trustworthiness pillars and evaluation metrics specifically relevant to FL
models, as well as to develop solutions that can compute the trustworthiness
level of FL models. This work examines the existing requirements for evaluating
trustworthiness in FL and introduces a comprehensive taxonomy consisting of six
pillars (privacy, robustness, fairness, explainability, accountability, and
federation), along with over 30 metrics for computing the trustworthiness of FL
models. Subsequently, an algorithm named FederatedTrust is designed based on
the pillars and metrics identified in the taxonomy to compute the
trustworthiness score of FL models. A prototype of FederatedTrust is
implemented and integrated into the learning process of FederatedScope, a
well-established FL framework. Finally, five experiments are conducted using
different configurations of FederatedScope to demonstrate the utility of
FederatedTrust in computing the trustworthiness of FL models. Three experiments
employ the FEMNIST dataset, and two utilize the N-BaIoT dataset considering a
real-world IoT security use case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Neural Link Predictors for Complex Query Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Arakelyan, Pasquale Minervini, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering complex queries on incomplete knowledge graphs is a challenging
task where a model needs to answer complex logical queries in the presence of
missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022)
showed that neural link predictors could also be used for answering complex
queries: their Continuous Query Decomposition (CQD) method works by decomposing
complex queries into atomic sub-queries, answers them using neural link
predictors and aggregates their scores via t-norms for ranking the answers to
each complex query. However, CQD does not handle negations and only uses the
training signal from atomic training queries: neural link prediction scores are
not calibrated to interact together via fuzzy logic t-norms during complex
query answering. In this work, we propose to address this problem by training a
parameter-efficient score adaptation model to re-calibrate neural link
prediction scores: this new component is trained on complex queries by
back-propagating through the complex query-answering process. Our method,
CQD$^{A}$, produces significantly more accurate results than current
state-of-the-art methods, improving from $34.4$ to $35.1$ Mean Reciprocal Rank
values averaged across all datasets and query types while using $\leq 35\%$ of
the available training query types. We further show that CQD$^{A}$ is
data-efficient, achieving competitive results with only $1\%$ of the training
data, and robust in out-of-domain evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Matters: A New <span class="highlight-title">Dataset</span> and Empirical Study for Multimodal
  Hyperbole Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huixuan Zhang, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection
of hyperbole is an important part of understanding human expression. There have
been several studies on hyperbole detection, but most of which focus on text
modality only. However, with the development of social media, people can create
hyperbolic expressions with various modalities, including text, images, videos,
etc. In this paper, we focus on multimodal hyperbole detection. We create a
multimodal detection dataset\footnote{The dataset will be released to the
community.} from Weibo (a Chinese social media) and carry out some studies on
it. We treat the text and image from a piece of weibo as two modalities and
explore the role of text and image for hyperbole detection. Different
pre-trained multimodal encoders are also evaluated on this downstream task to
show their performance. Besides, since this dataset is constructed from five
different topics, we also evaluate the cross-domain performance of different
models. These studies can serve as a benchmark and point out the direction of
further study on multimodal hyperbole detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures. 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Open Vocabulary Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of visual scene understanding, deep neural networks have made
impressive advancements in various core tasks like segmentation, tracking, and
detection. However, most approaches operate on the close-set assumption,
meaning that the model can only identify pre-defined categories that are
present in the training set. Recently, open vocabulary settings were proposed
due to the rapid progress of vision language pre-training. These new approaches
seek to locate and recognize categories beyond the annotated label space. The
open vocabulary approach is more general, practical, and effective compared to
weakly supervised and zero-shot settings. This paper provides a thorough review
of open vocabulary learning, summarizing and analyzing recent developments in
the field. In particular, we begin by comparing it to related concepts such as
zero-shot learning, open-set recognition, and out-of-distribution detection.
Then, we review several closely related tasks in the case of segmentation and
detection, including long-tail problems, few-shot, and zero-shot settings. For
the method survey, we first present the basic knowledge of detection and
segmentation in close-set as the preliminary knowledge. Next, we examine
various scenarios in which open vocabulary learning is used, identifying common
design elements and core ideas. Then, we compare the recent detection and
segmentation approaches in commonly used datasets and benchmarks. Finally, we
conclude with insights, issues, and discussions regarding future research
directions. To our knowledge, this is the first comprehensive literature review
of open vocabulary learning. We keep tracing related works at
https://github.com/jianzongwu/Awesome-Open-Vocabulary.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/jianzongwu/Awesome-Open-Vocabulary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stacking of Hyperparameter Tuned Models for Tagging Coding Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sathya Krishnan TS, S. Lakshmana Pandian, P. Shunmugapriya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coding problems are problems that require a solution in the form of a
computer program. Coding problems are popular among students and professionals
as it enhances their skills and career opportunities. An AI system that would
help those who practice coding problems would be highly useful and there is a
huge potential for such a system. In this work, we propose a model which uses
stacking of hyperparameter tuned boosting models to achieve impressive metric
scores of 77.8% accuracy and 0.815 PR-AUC on the dataset that was scraped from
Codeforces and Leetcode. We open source the dataset and the models developed
for this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Error corrections have to be made for certain metrics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spotting Virus from Satellites: Modeling the Circulation of West Nile
  Virus Through Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Bonicelli, Angelo Porrello, Stefano Vincenzi, Carla Ippoliti, Federica Iapaolo, Annamaria Conte, Simone Calderara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The occurrence of West Nile Virus (WNV) represents one of the most common
mosquito-borne zoonosis viral infections. Its circulation is usually associated
with climatic and environmental conditions suitable for vector proliferation
and virus replication. On top of that, several statistical models have been
developed to shape and forecast WNV circulation: in particular, the recent
massive availability of Earth Observation (EO) data, coupled with the
continuous advances in the field of Artificial Intelligence, offer valuable
opportunities.
  In this paper, we seek to predict WNV circulation by feeding Deep Neural
Networks (DNNs) with satellite images, which have been extensively shown to
hold environmental and climatic features. Notably, while previous approaches
analyze each geographical site independently, we propose a spatial-aware
approach that considers also the characteristics of close sites. Specifically,
we build upon Graph Neural Networks (GNN) to aggregate features from
neighbouring places, and further extend these modules to consider multiple
relations, such as the difference in temperature and soil moisture between two
sites, as well as the geographical distance. Moreover, we inject time-related
information directly into the model to take into account the seasonality of
virus spread.
  We design an experimental setting that combines satellite images - from
Landsat and Sentinel missions - with ground truth observations of WNV
circulation in Italy. We show that our proposed Multi-Adjacency Graph Attention
Network (MAGAT) consistently leads to higher performance when paired with an
appropriate pre-training stage. Finally, we assess the importance of each
component of MAGAT in our ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures. Accepted at the IEEE Transactions On Geoscience
  And Remote Sensing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable AI: Does the Next Generation Require Quantum Computing? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aras Bacho, Holger Boche, Gitta Kutyniok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this survey, we aim to explore the fundamental question of whether the
next generation of artificial intelligence requires quantum computing.
Artificial intelligence is increasingly playing a crucial role in many aspects
of our daily lives and is central to the fourth industrial revolution. It is
therefore imperative that artificial intelligence is reliable and trustworthy.
However, there are still many issues with reliability of artificial
intelligence, such as privacy, responsibility, safety, and security, in areas
such as autonomous driving, healthcare, robotics, and others. These problems
can have various causes, including insufficient data, biases, and robustness
problems, as well as fundamental issues such as computability problems on
digital hardware. The cause of these computability problems is rooted in the
fact that digital hardware is based on the computing model of the Turing
machine, which is inherently discrete. Notably, our findings demonstrate that
digital hardware is inherently constrained in solving problems about
optimization, deep learning, or differential equations. Therefore, these
limitations carry substantial implications for the field of artificial
intelligence, in particular for machine learning. Furthermore, although it is
well known that the quantum computer shows a quantum advantage for certain
classes of problems, our findings establish that some of these limitations
persist when employing quantum computing models based on the quantum circuit or
the quantum Turing machine paradigm. In contrast, analog computing models, such
as the Blum-Shub-Smale machine, exhibit the potential to surmount these
limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-efficient NLLB-200: Language-specific Expert Pruning of a
  Massively Multilingual Machine Translation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeskendir Koishekenov, Vassilina Nikoulina, Alexandre Berard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to conventional bilingual translation systems, massively
multilingual machine translation is appealing because a single model can
translate into multiple languages and benefit from knowledge transfer for low
resource languages. On the other hand, massively multilingual models suffer
from the curse of multilinguality, unless scaling their size massively, which
increases their training and inference costs. Sparse Mixture-of-Experts models
are a way to drastically increase model capacity without the need for a
proportional amount of computing. The recently released NLLB-200 is an example
of such a model. It covers 202 languages but requires at least four 32GB GPUs
just for inference. In this work, we propose a pruning method that allows the
removal of up to 80\% of experts with a negligible loss in translation quality,
which makes it feasible to run the model on a single 32GB GPU. Further analysis
suggests that our pruning metrics allow to identify language-specific experts
and prune non-relevant experts for a given language pair.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to accurately locate and navigate to a specific object is a
crucial capability for embodied agents that operate in the real world and
interact with objects to complete tasks. Such object navigation tasks usually
require large-scale training in visual environments with labeled objects, which
generalizes poorly to novel objects in unknown environments. In this work, we
present a novel zero-shot object navigation method, Exploration with Soft
Commonsense constraints (ESC), that transfers commonsense knowledge in
pre-trained models to open-world object navigation without any navigation
experience nor any other training on the visual environments. First, ESC
leverages a pre-trained vision and language model for open-world prompt-based
grounding and a pre-trained commonsense language model for room and object
reasoning. Then ESC converts commonsense knowledge into navigation actions by
modeling it as soft logic predicates for efficient exploration. Extensive
experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method
improves significantly over baselines, and achieves new state-of-the-art
results for zero-shot object navigation (e.g., 288% relative Success Rate
improvement than CoW on MP3D).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unbiased Heterogeneous Scene Graph Generation with Relation-aware
  Message Passing Neural Network <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00443v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00443v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanghoon Yoon, Kibum Kim, Jinyoung Moon, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent scene graph generation (SGG) frameworks have focused on learning
complex relationships among multiple objects in an image. Thanks to the nature
of the message passing neural network (MPNN) that models high-order
interactions between objects and their neighboring objects, they are dominant
representation learning modules for SGG. However, existing MPNN-based
frameworks assume the scene graph as a homogeneous graph, which restricts the
context-awareness of visual relations between objects. That is, they overlook
the fact that the relations tend to be highly dependent on the objects with
which the relations are associated. In this paper, we propose an unbiased
heterogeneous scene graph generation (HetSGG) framework that captures
relation-aware context using message passing neural networks. We devise a novel
message passing layer, called relation-aware message passing neural network
(RMP), that aggregates the contextual information of an image considering the
predicate type between objects. Our extensive evaluations demonstrate that
HetSGG outperforms state-of-the-art methods, especially outperforming on tail
predicate classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages; AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Potential sources of <span class="highlight-title">dataset</span> bias complicate investigation of
  underdiagnosis by machine learning algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélanie Bernhardt, Charles Jones, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An increasing number of reports raise concerns about the risk that machine
learning algorithms could amplify health disparities due to biases embedded in
the training data. Seyyed-Kalantari et al. find that models trained on three
chest X-ray datasets yield disparities in false-positive rates (FPR) across
subgroups on the 'no-finding' label (indicating the absence of disease). The
models consistently yield higher FPR on subgroups known to be historically
underserved, and the study concludes that the models exhibit and potentially
even amplify systematic underdiagnosis. We argue that the experimental setup in
the study is insufficient to study algorithmic underdiagnosis. In the absence
of specific knowledge (or assumptions) about the extent and nature of the
dataset bias, it is difficult to investigate model bias. Importantly, their use
of test data exhibiting the same bias as the training data (due to random
splitting) severely complicates the interpretation of the reported disparities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as Matters Arising in Nature Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AGM Belief Revision, Semantically 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.13557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.13557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faiq Miftakhul Falakh, Sebastian Rudolph, Kai Sauerwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a generic, model-theoretic characterization of belief revision
operators implementing the paradigm of minimal change according to the seminal
work by Alchourr\'{o}n, G\"{a}rdenfors, and Makinson (AGM). Our
characterization applies to all Tarskian logics, that is, all logics with a
classical model-theoretic semantics, and hence a wide variety of formalisms
used in knowledge representation and beyond, including many for which a
model-theoretic characterization has hitherto been lacking. Our starting point
is the approach by Katsuno and Mendelzon (K&M), who provided such a
characterization for propositional logic over finite signatures. We generalize
K&M's approach to the setting of AGM-style revision over bases in arbitrary
Tarskian logics, where base may refer to one of the various ways of
representing an agent's beliefs (such as belief sets, arbitrary or finite sets
of sentences, or single sentences). Our first core result is a representation
theorem providing a two-way correspondence between AGM-style revision operators
and specific assignments: functions associating every base to a "preference"
relation over interpretations, which must be total but is - in contrast to
prior approaches - not always transitive. As our second core contribution, we
provide a characterization of all logics for which our result can be
strengthened to assignments producing transitive preference relations (as in
K&M's original work). Alongside these main contributions, we discuss diverse
variants of our findings as well as ramifications for other areas of belief
revision theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>71 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chinese Fine-Grained Financial Sentiment Analysis with Large Language
  Models <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinyu Lan, Yanru Wu, Wang Xu, Weiqiang Feng, Youhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity-level fine-grained sentiment analysis in the financial domain is a
crucial subtask of sentiment analysis and currently faces numerous challenges.
The primary challenge stems from the lack of high-quality and large-scale
annotated corpora specifically designed for financial text sentiment analysis,
which in turn limits the availability of data necessary for developing
effective text processing techniques. Recent advancements in large language
models (LLMs) have yielded remarkable performance in natural language
processing tasks, primarily centered around language pattern matching. In this
paper, we propose a novel and extensive Chinese fine-grained financial
sentiment analysis dataset, FinChina SA, for enterprise early warning. We
thoroughly evaluate and experiment with well-known existing open-source LLMs
using our dataset. We firmly believe that our dataset will serve as a valuable
resource to advance the exploration of real-world financial sentiment analysis
tasks, which should be the focus of future research. Our dataset and all code
to replicate the experimental results will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>FinLLM Symposium at IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denoise <span class="highlight-title">Pretrain</span>ing on Nonequilibrium Molecules for Accurate and
  Transferable Neural Potentials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Wang, Changwen Xu, Zijie Li, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in equivariant graph neural networks (GNNs) have made deep
learning amenable to developing fast surrogate models to expensive ab initio
quantum mechanics (QM) approaches for molecular potential predictions. However,
building accurate and transferable potential models using GNNs remains
challenging, as the data is greatly limited by the expensive computational
costs and level of theory of QM methods, especially for large and complex
molecular systems. In this work, we propose denoise pretraining on
nonequilibrium molecular conformations to achieve more accurate and
transferable GNN potential predictions. Specifically, atomic coordinates of
sampled nonequilibrium conformations are perturbed by random noises and GNNs
are pretrained to denoise the perturbed molecular conformations which recovers
the original coordinates. Rigorous experiments on multiple benchmarks reveal
that pretraining significantly improves the accuracy of neural potentials.
Furthermore, we show that the proposed pretraining approach is model-agnostic,
as it improves the performance of different invariant and equivariant GNNs.
Notably, our models pretrained on small molecules demonstrate remarkable
transferability, improving performance when fine-tuned on diverse molecular
systems, including different elements, charged molecules, biomolecules, and
larger systems. These results highlight the potential for leveraging denoise
pretraining approaches to build more generalizable neural potentials for
complex molecular systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Journal of Chemical Theory and Computation. 32 pages, 5
  figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Multi-aspect Mining of Complex Time-stamped Event Streams <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Nakamura, Yasuko Matsubara, Koki Kawabata, Yuhei Umeda, Yuichiro Wada, Yasushi Sakurai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a huge, online stream of time-evolving events with multiple attributes,
such as online shopping logs: (item, price, brand, time), and local mobility
activities: (pick-up and drop-off locations, time), how can we summarize large,
dynamic high-order tensor streams? How can we see any hidden patterns, rules,
and anomalies? Our answer is to focus on two types of patterns, i.e.,
''regimes'' and ''components'', for which we present CubeScope, an efficient
and effective method over high-order tensor streams. Specifically, it
identifies any sudden discontinuity and recognizes distinct dynamical patterns,
''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it also
performs multi-way summarization for all attributes (e.g., item, price, brand,
and time) and discovers hidden ''components'' representing latent groups (e.g.,
item/brand groups) and their relationship. Thanks to its concise but effective
summarization, CubeScope can also detect the sudden appearance of anomalies and
identify the types of anomalies that occur in practice. Our proposed method has
the following properties: (a) Effective: it captures dynamical multi-aspect
patterns, i.e., regimes and components, and statistically summarizes all the
events; (b) General: it is practical for successful application to data
compression, pattern discovery, and anomaly detection on various types of
tensor streams; (c) Scalable: our algorithm does not depend on the length of
the data stream and its dimensionality. Extensive experiments on real datasets
demonstrate that CubeScope finds meaningful patterns and anomalies correctly,
and consistently outperforms the state-of-the-art methods as regards accuracy
and execution speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deductive Additivity for Planning of Natural Language Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current natural language systems designed for multi-step claim validation
typically operate in two phases: retrieve a set of relevant premise statements
using heuristics (planning), then generate novel conclusions from those
statements using a large language model (deduction). The planning step often
requires expensive Transformer operations and does not scale to arbitrary
numbers of premise statements. In this paper, we investigate whether an
efficient planning heuristic is possible via embedding spaces compatible with
deductive reasoning. Specifically, we evaluate whether embedding spaces exhibit
a property we call deductive additivity: the sum of premise statement
embeddings should be close to embeddings of conclusions based on those
premises. We explore multiple sources of off-the-shelf dense embeddings in
addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We
study embedding models both intrinsically, evaluating whether the property of
deductive additivity holds, and extrinsically, using them to assist planning in
natural language proof generation. Lastly, we create a dataset, Single-Step
Reasoning Contrast (SSRC), to further probe performance on various reasoning
types. Our findings suggest that while standard embedding methods frequently
embed conclusions near the sums of their premises, they fall short of being
effective heuristics and lack the ability to model certain categories of
reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REAL: A Representative Error-Driven Approach for Active Learning <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Chen, Yong Wang, Lizi Liao, Yueguo Chen, Xiaoyong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a limited labeling budget, active learning (AL) aims to sample the most
informative instances from an unlabeled pool to acquire labels for subsequent
model training. To achieve this, AL typically measures the informativeness of
unlabeled instances based on uncertainty and diversity. However, it does not
consider erroneous instances with their neighborhood error density, which have
great potential to improve the model performance. To address this limitation,
we propose $REAL$, a novel approach to select data instances with
$\underline{R}$epresentative $\underline{E}$rrors for $\underline{A}$ctive
$\underline{L}$earning. It identifies minority predictions as \emph{pseudo
errors} within a cluster and allocates an adaptive sampling budget for the
cluster based on estimated error density. Extensive experiments on five text
classification datasets demonstrate that $REAL$ consistently outperforms all
best-performing baselines regarding accuracy and F1-macro scores across a wide
range of hyperparameter settings. Our analysis also shows that $REAL$ selects
the most representative pseudo errors that match the distribution of
ground-truth errors along the decision boundary. Our code is publicly available
at https://github.com/withchencheng/ECML_PKDD_23_Real.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECML/PKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't trust your eyes: on the (un)reliability of feature visualizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04719v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04719v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Geirhos, Roland S. Zimmermann, Blair Bilodeau, Wieland Brendel, Been Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to "explain" how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breadth-First Pipeline Parallelism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Lamy-Poirier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Breadth-First Pipeline Parallelism, a novel training schedule
which optimizes the combination of pipeline and data parallelism. Breadth-First
Pipeline Parallelism lowers training time, cost and memory usage by combining a
high GPU utilization with a small batch size per GPU, and by making use of
fully sharded data parallelism. Experimentally, we observed an increase of up
to 43% in training throughput for a 52 billion-parameter model using a small
batch size per GPU compared to Megatron-LM, which would reduce the training
time and cost by the same amount on a large GPU cluster.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-07-14T09:33:43.203285192Z">
            2023-07-14 09:33:43 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
